{
	"id": 138630434,
	"number": 14653,
	"state": "closed",
	"title": "proposal: drastically improve performances of sort.Sort with big slices ",
	"body": "Please answer these questions before submitting your issue. Thanks!\r\n\r\n1. What version of Go are you using (`go version`)?\r\ngo1.6 darwin/amd64\r\n\r\n2. What operating system and processor architecture are you using (`go env`)?\r\nOSX, x86_64\r\n\r\n3. What did you do?\r\nI'm trying to sort a text file with millions of lines of code.\r\n\r\n```go\r\nfunc main() {\r\n\r\n\tflag.Parse()\r\n\tif flag.NArg() != 2 {\r\n\t\tlog.Fatal(\"Syntax error: sort \u003csrcPath\u003e \u003cdstPath\u003e\")\r\n\t}\r\n\r\n\tsrcPath := flag.Arg(0)\r\n\tdstPath := flag.Arg(1)\r\n\tlog.Printf(\"Sorting [%v] -\u003e [%v]\", srcPath, dstPath)\r\n\r\n\tsrcBytes, err := ioutil.ReadFile(srcPath)\r\n\tif err != nil {\r\n\t\tlog.Fatalf(\"%s\", err)\r\n\t}\r\n\tlog.Printf(\"%v bytes loaded\", len(srcBytes))\r\n\r\n\tlines := strings.Split(string(srcBytes), \"\\n\")\r\n\tlog.Printf(\"%v lines found\", len(lines))\r\n\r\n\tsort.Strings(lines)\r\n\tlog.Printf(\"%v lines sorted\", len(lines))\r\n\r\n\tdstBytes := []byte(strings.Join(lines, \"\\n\"))\r\n\tlog.Printf(\"%v bytes joined\", len(dstBytes))\r\n\r\n\terr = ioutil.WriteFile(dstPath, dstBytes, 0666)\r\n\tif err != nil {\r\n\t\tlog.Fatalf(\"%s\", err)\r\n\t}\r\n\tlog.Printf(\"%v bytes saved\", len(dstBytes))\r\n}\r\n```\r\n\r\n4. What did you expect to see?\r\nMy file sorted, but faster\r\n\r\n5. What did you see instead?\r\nMy file sorted, but slower\r\n\r\n# Story\r\n\r\nCurrent implementation of `sort.Sort` is based on QuickSort algorithm. It is a very good choice but as many other have a CPU complexity of `N * log(N)`.\r\n\r\nThis means that if we can split our task into `k` smaller tasks of size `n` with a CPU complexity of `N` we can improve performances:\r\n\r\n```\r\nN + k * (n * log(n)) \u003c N log(N) where k * n = N\r\n```\r\n\r\nMy experiments have proved it quite easily:\r\n- I joined all text lines of the Linux Kernel's sources into a single file (stripped, kept only lines with len \u003e= 2). The outcome was a file of 14 millions of lines of code.\r\n- I sort the file with 2 different strategies:\r\n   * Naive approach: Load into a huge slice. Apply `sort.Sort` on it. Save it back to the file.\r\n   * Faster approach: While loading I split lines into buckets using as a key first two letters (`key := int(line[0]) * 256 + int(line[1])`; then I sort each bucket; I save all basket's lines that are already naturally sorted.\r\n\r\n```go\r\n\t// Allocates all possible buckets:\r\n\tconst KEY_SIZE = 2\r\n\tbuckets = make([][]string, 1 \u003c\u003c (KEY_SIZE * 8))\r\n\r\n\t// Loads text lines from source file and distributes it to their buckets:\r\n\tscan := bufio.NewScanner(fd)\r\n\tfor scan.Scan() {\r\n\t\tline := scan.Text()\r\n\r\n\t\tkey := 0\r\n\t\tmultiplier := len(buckets)\r\n\t\tfor i := 0; i \u003c KEY_SIZE \u0026\u0026 i \u003c len(line); i++ {\r\n\t\t\tmultiplier \u003e\u003e= 8\r\n\t\t\tkey += multiplier * int(line[i])\r\n\t\t}\r\n\r\n\t\tbuckets[key] = append(buckets[key], line)\r\n\t\tnumLines++\r\n\t}\r\n\t...\r\n\tvar j int\r\n\tfor i, bucket := range buckets {\r\n\t\tif len(bucket) == 0 {\r\n\t\t\tcontinue\r\n\t\t}\r\n\r\n\t\tsort.Strings(bucket)\r\n\t\tnumLines += int64(len(bucket))\r\n\r\n\t\tif i != j {\r\n\t\t\tbuckets[j] = bucket\r\n\t\t\tbuckets[i] = nil\r\n\t\t}\r\n\t\tj++\r\n\t}\r\n\r\n\tsortedBuckets = buckets[:j]\r\n```\r\n\r\nThe second algorithm was much faster: from 17 seconds to 10 seconds.\r\n\r\nMoreover other seconds can be easily earned by using go-routines because buckets are independent each other; with my experiments I reached 7 seconds on a 4 cores machine.\r\n\r\n\r\n## Proposal\r\n\r\nMy idea is to provide a set of functions like the following (names and documentation must be improved):\r\n\r\n```go\r\nsort.RadixSortStrings(src []string) \u003c-chan string\r\nsort.RadixSortInts(src []string) \u003c-chan int\r\n...\r\n\r\ntype InterfaceRadix interface {\r\n\r\n        Interface\r\n\r\n        // Generates a key-radix for the element at position i convenient for passed number of buckets.\r\n        // Returned keys need to be naturally sortable in the same way as objects to be sorted.\r\n        // Returned keys must be 0 \u003c= key \u003c= numBuckets\r\n        // Different objects can generate the same value, but is convenient to have an homogeneous distribution of returned values.\r\n        Radix(i, numBuckets int) int\r\n}\r\nsort.RadixSort(src InterfaceRadix) \u003c-chan InterfaceRadix\r\n```\r\n\r\nThese functions:\r\n- would perform sort out of place (a common requirement).\r\n- the sort would make explicit use of go-routines because sorting all the buckets is an embarrassingly parallel algorithm.\r\n- return elements back using a channel so that while sorting the user can already process first elements.\r\n- Interface `InterfaceRadix` would be already implemented for base types.\r\n\r\nHaving this set of functions would give to Go a competitive advantage over other languages on modern applications where sorting millions of elements is quite usual.\r\n\r\nThis functions may prove to be competitive already with just tens/hundreds thousands of elements.\r\n\r\n",
	"user": {
		"login": "rressi",
		"id": 11461454,
		"type": "User",
		"site_admin": false
	},
	"comments": 8,
	"closed_at": "2016-03-09T05:07:44Z",
	"created_at": "2016-03-05T01:28:52Z",
	"updated_at": "2016-03-09T05:07:44Z"
}
