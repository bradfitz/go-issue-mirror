{
	"id": 244575990,
	"body": "@randall77 I believe the performance difference is non trivial.  I made a change that makes `runtime.roundupsize` linked through to `bytes.roundupsize`.  Correct me if I am wrong, but when increasing the size of a slice, it is this function that will choose the actual amount of space granted for a given memory request.\r\n\r\nUsing that, I changed `bytes.makeSlice` (which is used by `Buffer.grow`) to force the capacity to the maximum.   I believe this is equivalent to the effect that append would have in my proposal.  I then wrote a benchmark that writes ~1KB slices to a `bytes.Buffer` a fixed number of times.  Running this benchmark before and after shows fairly large gains when doing lots of writes:\r\n\r\nBefore:\r\n```\r\n//return make([]byte, n)\r\nBenchmarkBufferMultipleWritesExact/1025x1-4         \t 2000000\t       611 ns/op\r\nBenchmarkBufferMultipleWritesExact/1025x2-4         \t 1000000\t      1770 ns/op\r\nBenchmarkBufferMultipleWritesExact/1025x4-4         \t  300000\t      4506 ns/op\r\nBenchmarkBufferMultipleWritesExact/1025x8-4         \t  200000\t      9684 ns/op\r\nBenchmarkBufferMultipleWritesExact/1025x16-4        \t  100000\t     20517 ns/op\r\nBenchmarkBufferMultipleWritesExact/1025x32-4        \t   30000\t     41415 ns/op\r\nBenchmarkBufferMultipleWritesExact/1025x64-4        \t   20000\t     79752 ns/op\r\nBenchmarkBufferMultipleWritesExact/1025x128-4       \t   10000\t    158385 ns/op\r\nBenchmarkBufferMultipleWritesExact/1025x256-4       \t    5000\t    323424 ns/op\r\nBenchmarkBufferMultipleWritesExact/1025x512-4       \t    2000\t    753127 ns/op\r\nBenchmarkBufferMultipleWritesExact/1025x1024-4      \t    1000\t   1429573 ns/op\r\nBenchmarkBufferMultipleWritesExact/1024x1-4         \t 3000000\t       577 ns/op\r\nBenchmarkBufferMultipleWritesExact/1024x2-4         \t 1000000\t      2146 ns/op\r\nBenchmarkBufferMultipleWritesExact/1024x4-4         \t  300000\t      4701 ns/op\r\nBenchmarkBufferMultipleWritesExact/1024x8-4         \t  200000\t     10250 ns/op\r\nBenchmarkBufferMultipleWritesExact/1024x16-4        \t  100000\t     20055 ns/op\r\nBenchmarkBufferMultipleWritesExact/1024x32-4        \t   30000\t     40386 ns/op\r\nBenchmarkBufferMultipleWritesExact/1024x64-4        \t   20000\t     81937 ns/op\r\nBenchmarkBufferMultipleWritesExact/1024x128-4       \t   10000\t    151521 ns/op\r\nBenchmarkBufferMultipleWritesExact/1024x256-4       \t    5000\t    317768 ns/op\r\nBenchmarkBufferMultipleWritesExact/1024x512-4       \t    2000\t    724780 ns/op\r\nBenchmarkBufferMultipleWritesExact/1024x1024-4      \t    1000\t   1417779 ns/op\r\nBenchmarkBufferMultipleWritesExact/1023x1-4         \t 3000000\t       567 ns/op\r\nBenchmarkBufferMultipleWritesExact/1023x2-4         \t 1000000\t      1655 ns/op\r\nBenchmarkBufferMultipleWritesExact/1023x4-4         \t  300000\t      4384 ns/op\r\nBenchmarkBufferMultipleWritesExact/1023x8-4         \t  200000\t      9584 ns/op\r\nBenchmarkBufferMultipleWritesExact/1023x16-4        \t  100000\t     19736 ns/op\r\nBenchmarkBufferMultipleWritesExact/1023x32-4        \t   30000\t     40424 ns/op\r\nBenchmarkBufferMultipleWritesExact/1023x64-4        \t   20000\t     80950 ns/op\r\nBenchmarkBufferMultipleWritesExact/1023x128-4       \t   10000\t    155928 ns/op\r\nBenchmarkBufferMultipleWritesExact/1023x256-4       \t    5000\t    315266 ns/op\r\nBenchmarkBufferMultipleWritesExact/1023x512-4       \t    2000\t    733745 ns/op\r\nBenchmarkBufferMultipleWritesExact/1023x1024-4      \t    1000\t   1381548 ns/op\r\n```\r\n\r\nAfter:\r\n```\r\n//return make([]byte, n, int(roundupsize(uintptr(n))))\r\nBenchmarkBufferMultipleWritesExact/1025x1-4         \t 2000000\t       631 ns/op\r\nBenchmarkBufferMultipleWritesExact/1025x2-4         \t  500000\t      2091 ns/op\r\nBenchmarkBufferMultipleWritesExact/1025x4-4         \t  200000\t      5367 ns/op\r\nBenchmarkBufferMultipleWritesExact/1025x8-4         \t  200000\t      5578 ns/op\r\nBenchmarkBufferMultipleWritesExact/1025x16-4        \t  100000\t     12246 ns/op\r\nBenchmarkBufferMultipleWritesExact/1025x32-4        \t   50000\t     28473 ns/op\r\nBenchmarkBufferMultipleWritesExact/1025x64-4        \t   20000\t     64402 ns/op\r\nBenchmarkBufferMultipleWritesExact/1025x128-4       \t   10000\t    124943 ns/op\r\nBenchmarkBufferMultipleWritesExact/1025x256-4       \t    5000\t    299186 ns/op\r\nBenchmarkBufferMultipleWritesExact/1025x512-4       \t    2000\t    591824 ns/op\r\nBenchmarkBufferMultipleWritesExact/1025x1024-4      \t    1000\t   1202655 ns/op\r\nBenchmarkBufferMultipleWritesExact/1024x1-4         \t 2000000\t       586 ns/op\r\nBenchmarkBufferMultipleWritesExact/1024x2-4         \t 1000000\t      1748 ns/op\r\nBenchmarkBufferMultipleWritesExact/1024x4-4         \t  300000\t      4497 ns/op\r\nBenchmarkBufferMultipleWritesExact/1024x8-4         \t  300000\t      4678 ns/op\r\nBenchmarkBufferMultipleWritesExact/1024x16-4        \t  100000\t     10812 ns/op\r\nBenchmarkBufferMultipleWritesExact/1024x32-4        \t   50000\t     24608 ns/op\r\nBenchmarkBufferMultipleWritesExact/1024x64-4        \t   30000\t     52313 ns/op\r\nBenchmarkBufferMultipleWritesExact/1024x128-4       \t   10000\t    118654 ns/op\r\nBenchmarkBufferMultipleWritesExact/1024x256-4       \t   10000\t    231403 ns/op\r\nBenchmarkBufferMultipleWritesExact/1024x512-4       \t    2000\t    519558 ns/op\r\nBenchmarkBufferMultipleWritesExact/1024x1024-4      \t    1000\t   1051704 ns/op\r\nBenchmarkBufferMultipleWritesExact/1023x1-4         \t 2000000\t       588 ns/op\r\nBenchmarkBufferMultipleWritesExact/1023x2-4         \t 1000000\t      1742 ns/op\r\nBenchmarkBufferMultipleWritesExact/1023x4-4         \t  300000\t      4520 ns/op\r\nBenchmarkBufferMultipleWritesExact/1023x8-4         \t  300000\t      4870 ns/op\r\nBenchmarkBufferMultipleWritesExact/1023x16-4        \t  100000\t     20188 ns/op\r\nBenchmarkBufferMultipleWritesExact/1023x32-4        \t   30000\t     40593 ns/op\r\nBenchmarkBufferMultipleWritesExact/1023x64-4        \t   10000\t    104389 ns/op\r\nBenchmarkBufferMultipleWritesExact/1023x128-4       \t   10000\t    130177 ns/op\r\nBenchmarkBufferMultipleWritesExact/1023x256-4       \t    3000\t    359676 ns/op\r\nBenchmarkBufferMultipleWritesExact/1023x512-4       \t    2000\t    518353 ns/op\r\nBenchmarkBufferMultipleWritesExact/1023x1024-4      \t    1000\t   1054462 ns/op\r\n```\r\n\r\n\r\nThe first number in the name is the size of the slice, in order to try and cause degenerate allocation patterns.  The second number is how many times in the loop said buffer was written to the `bytes.Buffer`.  I can upload the code and benchmark if necessary.  \r\n\r\nOnce there are 8 writes or more, using the entire capacity of the underlying span becomes important.  A common use case would be reading data off the network, which typically comes in small chunks, but may been to be compacted.  ",
	"user": {
		"login": "carl-mastrangelo",
		"id": 8943572,
		"type": "User",
		"site_admin": false
	},
	"created_at": "2016-09-03T23:43:27Z",
	"updated_at": "2016-09-03T23:48:27Z"
}
