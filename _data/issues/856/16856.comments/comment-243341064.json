{
	"id": 243341064,
	"body": "To summarize the last few comments:\r\n\r\n* it's fair to say no library encodes `true` as anything but `1`\r\n* it's fair to say most (if not all) deserializers assume `byte != 0` is `true` (probably because it's less code to implement)\r\n* assuming `byte == 1` is `true` would be more correct and potentially catch out-of-sync errors\r\n* adding an `error` for boolean values that are not `0` or `1` would add a new type of error behavior to `binary.Read` (one that is based on the decoded value and not the state of the `io.Reader` or the provided data structure, which means it would discard a byte from the input stream)\r\n\r\nPractically, this is mostly minutiae (due to the first point), but if we want to defer the decision between the two (in the interest of implementing this proposal), then going for correctness first and making it more lenient later could be an acceptable compromise.\r\n\r\nHowever, if no one has a strong opinion on one over the other, I would suggest the less correct one (`byte != 0`) simply to avoid the additional complexity of the error state, leaving out-of-sync detection up to the user (as is currently the case with all the other value types).",
	"user": {
		"login": "blixt",
		"id": 158591,
		"type": "User",
		"site_admin": false
	},
	"created_at": "2016-08-30T06:00:04Z",
	"updated_at": "2016-08-30T06:00:50Z"
}
