{
	"id": 110988764,
	"body": "\"I don't have a sense of what, in terms\r\nof allocation rate, pointer density, sharing between Goroutines, and\r\npointer mutation, would fill a Terabyte of Go memory.\"\r\n\r\nThe National Center for Biotechnology Information (NCBI) maintains a database of \"non-redundant\" (unique) protein sequences. That database is now over 60 million sequences, and for the past several years has been doubling every two years.\r\n\r\nPubChem (a public database of chemical compounds) is over 300GB in size on disk. It has also been growing, though not as quickly.\r\n\r\nIf Go is going to be useful for big-data science, it needs to be able to use as much RAM as we can throw into a system. Terabyte-plus compute servers are common now. Now, I don't need a terabyte today, but I need more than 128GB. The right solution is for this not to be hardcoded.\r\n\r\nIn my case, what consumes all the RAM is a large lookup table of kmers (very short sequences) indexed into every sequence in the database I'm trying to compress. Lots of pointers, lots of string data, and we don't want to write things out to disk until we're done.",
	"user": {
		"login": "ndaniels",
		"id": 797135,
		"type": "User",
		"site_admin": false
	},
	"created_at": "2015-06-11T03:37:20Z",
	"updated_at": "2015-06-11T03:37:20Z"
}
