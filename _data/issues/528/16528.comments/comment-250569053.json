{
	"id": 250569053,
	"body": "I explored this problem more today. The [rpc benchmark](https://github.com/aclements/go-gcbench/blob/66f6dbdd66ed6f775b0477d0c6a7ad5f4adf03ca/gcbench/progs/rpc.go) from gcbench shows this off particularly well. This benchmark fires up trivial client and server processes, which communicate over a set of TCP sockets. The client uses open-loop control to send requests according to a Poisson process, which models the behavior of real RPC systems.\r\n\r\nThis is what the execution trace looks like during a GC:\r\n\r\n![screenshot from 2016-09-29 14 59 07](https://cloud.githubusercontent.com/assets/2688315/18968011/50722cd4-8655-11e6-90d8-25ec9811934d.png)\r\n\r\nG18 and G19 are the background workers. You can see that they always run for a full quantum, and often run simultaneously, blocking both Ps. Also, even though there's only a little overall idle time, together they're clearly taking up much more than 25% of the total CPU. In the goroutines row, you can see the system regularly falling behind throughout the GC cycle, and in the network row, you can see it mostly falls back to the periodic sysmon network poll.\r\n\r\n[CL 30031](https://go-review.googlesource.com/30031) is an experiment that changes the idle worker to constantly poll for either run queue work or network wake-ups. With this CL, the trace looks very different ([trace file](https://storage.googleapis.com/golang-dash/tmp/rpc-cl30031.trace)):\r\n\r\n![screenshot from 2016-09-29 15 06 42](https://cloud.githubusercontent.com/assets/2688315/18968279/5b4ef564-8656-11e6-85f6-41e1f67daf67.png)\r\n\r\nIn this trace, G8 and G9 are the background workers. There are still large regions where they're running the fractional worker, but there's only one at a time, and they add up to almost exactly 25% of the CPU. In the goroutines and network rows, you can see that it still falls behind at the beginning of GC and switches to the sysmon network poll. However, about 80ms in to the cycle, it catches up and things run quite smoothly for the rest of the cycle: the goroutine queue stays short, and network events are coming in rapidly.\r\n\r\nThere are a few extra colors in this trace. In the lower half of each proc row, red means \"syscall\" (which means user code in this case), teal means \"assist\", and blue means \"idle worker\". At the beginning of GC, when the system is falling behind, there are a lot of assists. In fact, this phase lasts about 70ms, which is right around the P99.9 and max latencies of this run. Eventually, however, the background workers get ahead, the assists stop, the runnable goroutine queue drains, we start having idle time again, and we start seeing the idle worker kicking in for very brief periods.",
	"user": {
		"login": "aclements",
		"id": 2688315,
		"type": "User",
		"site_admin": false
	},
	"created_at": "2016-09-29T19:37:27Z",
	"updated_at": "2016-09-29T19:37:27Z"
}
