{
	"id": 234723681,
	"body": "\u003e Are there any circumstances under which a fix for this would be part of go1.7?\r\n\r\nIt's not impossible. Code-wise it's pretty low risk. It clearly satisfies the invariants there. The only risk in my mind is that touching anything related to the pacer can have surprising consequences, but even there I think this is pretty low risk. I'd like to take a long stare at it and have at least one other person do so, too.\r\n\r\n\u003e Allocation during a GC cycle is several orders of magnitude more expensive than allocation between GC cycles. I'd expect the cost difference to be less than 10x.\r\n\r\nIs that true of all allocations? Given what's currently on master, I would expect *some* allocations to be very expensive, but for the mean cost of allocation to be only slightly higher when GC is running.\r\n\r\n\u003e At the very end of the concurrent GC phase, between 564ms and 566ms in the trace, the goroutine queue drains. The goroutines are each scheduled for more time than they are when the GC isn't running (compare with 568â€“570ms), but they each run quickly enough that the run queue drains. I'd expect the whole concurrent GC cycle to look like this, at least because it looks like confusion (1) isn't in play there.\r\n\r\nThere's a fairness issue in the way assists currently work that's probably causing this. If an assist isn't able to fully satisfy itself, it goes on to the assist queue. But an assist that starts *later* may find more mark work and satisfy itself right away, even though there are older unsatisfied assists on the queue. Since only the background mark worker can satisfy queued assists, if it's not getting enough time because every P is busy doing assists, those queued assists just sit there. This is a lot like very bad lock barging. The result is that you get a pile of blocked assists, and there's a huge spike in runnable goroutines when those get unblocked at GC termination.\r\n\r\nThis is on my list of things to fix for 1.8. My plan is to effectively queue *all* assists and do all mark work in the background mark worker. This will make the queuing fair, centralize all mark work in one place in the code, and make it possible to satisfy an assist even in the middle of an indivisible scanning block (which will, in turn, make it more practical to count root scanning work towards assists, which will help satisfy assists at the beginning of the GC cycle).\r\n\r\n\u003e Is (4) what you mean by \"the pacer pulling back\"? Does the pacer divide the GC into two (or more) phases, during which it imposes different allocation taxes?\r\n\r\nNo, actually. What I meant is that the high CPU utilization over the whole GC cycle should cause the pacer to start the *next* cycle sooner. But you mentioned later that it's also finishing 5--10x sooner than expected, which is probably why this isn't happening.\r\n\r\n\u003e Would a change like that break other invariants or goals of the GC?\r\n\r\nYour proposed change is fine invariant-wise. The only requirement is that a goroutine is not allowed to return from the assist until gcAssistBytes is non-negative. And using the min(over-assist, required work) should accomplish the desired amortization just as well as (if not better than) the current approach of always adding the over-assist.",
	"user": {
		"login": "aclements",
		"id": 2688315,
		"type": "User",
		"site_admin": false
	},
	"created_at": "2016-07-23T15:20:40Z",
	"updated_at": "2016-07-23T15:20:40Z"
}
