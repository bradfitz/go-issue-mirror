{
	"id": 235400997,
	"body": "\u003e Would you mind giving CL 24704 a try on your workload, perhaps both with the 512k over-assist and with the 64k over-assist? I want to make sure we really understand what's going on and what the effect of these changes is.\r\n\r\nI cherry-picked CL 24704 PS 1 onto go1.7rc3, and it helps my application. Objectively, the delay between inbound network events and the execution of the goroutines they unblock is reduced to only a few microseconds, down from several milliseconds. Subjectively, the frequency of inbound network traffic is still lower than when the GC isn't running, but it's not as much of a decrease as with go1.7rc3.\r\n\r\nWith go1.7rc3 plus cherry-pick of CL 24704 PS 1 plus reducing the over-assist to 64k, the inbound network activity continues at just about the same rate whether or not the GC is running. This is somewhat subjective, so we might be in the realm of tuning here.\r\n\r\n\u003e Is that true of all allocations? Given what's currently on master, I would expect some allocations to be very expensive, but for the mean cost of allocation to be only slightly higher when GC is running.\r\n\r\nAs far as I can tell, it's not true of all allocations—just the allocations that happen. Specifically, it looks with go1.7rc3 like allocations near the beginning of concurrent mark are very expensive. There are some goroutines that usually run for a few hundred microseconds, but if they're scheduled at the beginning of concurrent mark then they'll end up taking 10ms to do their work. I don't know what would change for those goroutines other than the per-byte cost of allocation.\r\n\r\nBecause the allocations early in the cycle cost so much, there isn't much time left for less expensive ones. It's possible that these average out to something reasonable, but the 10ms penalty for some goroutines early in the cycle is surprising, and doesn't seem to be in the spirit of a low-latency garbage collector. It seems in practice like each garbage collection ends up with three STW phases, with the new and unfortunate \"gcAssistAlloc\" phase being the longest by far (around 10ms, vs around 1–2ms for mark termination and 100–200µs for sweep termination). Each P will run goroutines (quickly) until it runs a goroutine that allocates, so it's easy for all Ps to end up busy like this, with no progress on interactive user code.\r\n\r\n---\r\n\r\nIn preparing CL 25155, I tried an over-assist of 1\u003c\u003c20 (as in master) and 64\u003c\u003c10 (as in PS2). I picked it because I remember you saying that the GC can scan about 1MB per second, and some old recommendations (from Dmitry Vyukov?) about splitting work into pieces that can be done in about 100µs. Should the new over-assist figure be calculated more scientifically?\r\n\r\n---\r\n\r\nHere's what the \"Network\" row looks like during GCs for a few flavors of the Go runtime:\r\n\r\ngo1.7rc3, GC is around t=70–100ms. Note how network activity nearly stops during this time.\r\n\r\n\u003cimg width=\"1124\" alt=\"screen shot 2016-07-26 at 1 39 58 pm\" src=\"https://cloud.githubusercontent.com/assets/230685/17154593/85ff874a-5336-11e6-9d20-cd5b2012b6b1.png\"\u003e\r\n\r\ngo1.7rc3 with cherry-pick of cl/24704/1, GC is around t=270–310ms\r\n\r\n\u003cimg width=\"1116\" alt=\"screen shot 2016-07-26 at 1 40 20 pm\" src=\"https://cloud.githubusercontent.com/assets/230685/17154595/898aa32c-5336-11e6-8167-3a46384ab67c.png\"\u003e\r\n\r\ngo1.7rc3 with cherry-pick of cl/24704/1 plus reducing the over-assist to 64k, GC is around t=340–370ms\r\n\r\n\u003cimg width=\"1125\" alt=\"screen shot 2016-07-26 at 1 40 33 pm\" src=\"https://cloud.githubusercontent.com/assets/230685/17154597/8cbd9ba8-5336-11e6-84c6-93971eea8228.png\"\u003e\r\n",
	"user": {
		"login": "rhysh",
		"id": 230685,
		"type": "User",
		"site_admin": false
	},
	"created_at": "2016-07-26T20:50:15Z",
	"updated_at": "2016-07-26T20:50:15Z"
}
