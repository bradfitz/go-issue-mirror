{
	"id": 134794340,
	"number": 14406,
	"state": "closed",
	"title": "runtime: GC's mark termination STW time extended by Linux NUMA migration",
	"body": "I have a process that uses a large number of goroutines and sees large mark termination STW times (this is the same program described in #12967). After disabling stack shrinking, it still has large (and variable) mark termination times. The program runs on linux/amd64, with linux version 3.13.\r\n\r\nI've profiled with `perf` after enabling GOEXPERIMENT=framepointer, so I can see full call stacks of the GC and kernel. This profiling indicates that 1) stack shrinking significantly increases pause time, #12967, and 2) with stack shrinking disabled via GODEBUG=gcshrinkstackoff=1 the mark termination phase includes many calls to the kernel `page_fault` function, which six frames deeper leads to `do_numa_page` and then to `migrate_misplaced_page`.\r\n\r\nIt appears that the garbage collector makes sufficient accesses to memory to trick Linux's NUMA logic into moving physical memory pages to be closer to the GC workers that are accessing particular pages. This expensive migration increases GC pause times when it happens during mark termination. I suspect that the affinity that Gs have to Ps and that Ps have to CPU cores means that the mutator and GC fight back and forth on where the pages should be placed.\r\n\r\nSetting the process's memory policy to MPOL_BIND via either mbind(2) or get_mempolicy(2) shows a significant reduction in—and improvement in consistency of—mark termination time, and effectively eliminates time spent on page faults during mark termination.\r\n\r\nI have a small reproducer for this, included below (it's based on the reproducer for #12967 but doesn't have buggy use of WaitGroups, includes a large number of mostly-idle goroutines, and disables go1.6's improved inlining). It shows very consistent GC timings when the process's memory has been marked as MPOL_BIND via `numactl`, even when it's bound to all available nodes.\r\n\r\n```\r\n$ go version\r\ngo version go1.6 darwin/amd64\r\n$ # I'm cross-compiling - building on darwin/amd64, running on linux/amd64\r\n$ go tool compile -V\r\ncompile version go1.6 X:framepointer\r\n```\r\n```\r\n$ uname -r\r\n3.13.0-62-generic\r\n```\r\n\r\nThe included test output was recorded on a c4.8xlarge EC2 instance.\r\n\r\nI'd expect the Go GC to be able to indicate to the kernel that the memory accesses it's making are temporary and somewhat random, to advise the kernel to not migrate memory to match the access patterns.\r\n\r\n---\r\n\r\nHere 100 goroutines grow and shrink their stack requirements while 1,000,000 others read/write small pieces of their stacks without making function calls. Stack shrinking is disabled. Note the mark termination times of between 121ms and 1010ms, 22 million page faults, and 1316 seconds of system time over the 120-second test run.\r\n\r\n```\r\n$ /usr/bin/time env GODEBUG=gctrace=1,gcshrinkstackoff=1 ./issue12967 -d 2m -growers 100 -livers 1000000\r\ngc 1 @0.005s 5%: 0.17+1.7+0.87 ms clock, 2.4+0.13/2.6/1.1+12 ms cpu, 100-\u003e100-\u003e100 MB, 101 MB goal, 36 P\r\ngc 2 @0.023s 5%: 0.38+3.1+0.94 ms clock, 5.0+0.012/19/2.4+12 ms cpu, 101-\u003e101-\u003e101 MB, 200 MB goal, 36 P\r\ngc 3 @0.353s 2%: 1.6+19+5.4 ms clock, 32+4.4/156/198+109 ms cpu, 151-\u003e152-\u003e129 MB, 202 MB goal, 36 P\r\ngc 4 @0.909s 2%: 4.7+28+12 ms clock, 86+0.55/244/573+233 ms cpu, 231-\u003e232-\u003e176 MB, 257 MB goal, 36 P\r\ngc 5 @1.772s 3%: 8.1+57+27 ms clock, 228+0/482/925+761 ms cpu, 324-\u003e337-\u003e239 MB, 352 MB goal, 36 P\r\ngc 6 @2.925s 4%: 13+102+61 ms clock, 339+33/713/1831+1533 ms cpu, 437-\u003e455-\u003e335 MB, 453 MB goal, 36 P\r\ngc 7 @4.649s 6%: 18+122+125 ms clock, 599+38/1015/2690+4014 ms cpu, 620-\u003e637-\u003e438 MB, 637 MB goal, 36 P\r\ngc 8 @7.523s 20%: 117+15452+1010 ms clock, 3174+321/138568/413980+27277 ms cpu, 832-\u003e848-\u003e581 MB, 844 MB goal, 36 P\r\ngc 9 @28.610s 18%: 48+2893+484 ms clock, 1153+0.099/21720/37561+11638 ms cpu, 1111-\u003e1131-\u003e666 MB, 1130 MB goal, 36 P\r\ngc 10 @38.246s 15%: 32+300+121 ms clock, 1024+13/2499/7467+3900 ms cpu, 1266-\u003e1286-\u003e666 MB, 1293 MB goal, 36 P\r\ngc 11 @45.041s 14%: 45+1464+222 ms clock, 1456+20/12898/32722+7106 ms cpu, 1267-\u003e1287-\u003e666 MB, 1294 MB goal, 36 P\r\ngc 12 @52.895s 12%: 34+465+221 ms clock, 384+21/4017/8625+2435 ms cpu, 1266-\u003e1296-\u003e676 MB, 1293 MB goal, 36 P\r\ngc 13 @59.619s 12%: 46+595+426 ms clock, 1500+0.10/5118/14312+13652 ms cpu, 1267-\u003e1297-\u003e676 MB, 1294 MB goal, 36 P\r\ngc 14 @66.699s 11%: 64+712+179 ms clock, 2073+0.21/6241/18584+5733 ms cpu, 1266-\u003e1296-\u003e676 MB, 1293 MB goal, 36 P\r\ngc 15 @73.656s 10%: 37+215+121 ms clock, 1203+0.006/1774/5229+3902 ms cpu, 1266-\u003e1277-\u003e656 MB, 1293 MB goal, 36 P\r\ngc 16 @80.291s 10%: 63+572+157 ms clock, 1653+0.30/4999/10443+4099 ms cpu, 1266-\u003e1296-\u003e676 MB, 1293 MB goal, 36 P\r\ngc 17 @87.130s 10%: 89+925+247 ms clock, 2873+0.017/7985/21239+7935 ms cpu, 1266-\u003e1296-\u003e676 MB, 1293 MB goal, 36 P\r\ngc 18 @94.465s 9%: 36+487+220 ms clock, 1152+0.24/4188/12270+7071 ms cpu, 1266-\u003e1296-\u003e676 MB, 1293 MB goal, 36 P\r\ngc 19 @101.246s 9%: 32+267+121 ms clock, 754+0/2281/4436+2799 ms cpu, 1266-\u003e1286-\u003e666 MB, 1293 MB goal, 36 P\r\ngc 20 @107.862s 8%: 27+293+305 ms clock, 628+0.005/2452/5118+7015 ms cpu, 1266-\u003e1286-\u003e666 MB, 1293 MB goal, 36 P\r\ngc 21 @114.597s 8%: 54+426+181 ms clock, 1750+6.0/3620/10683+5796 ms cpu, 1266-\u003e1296-\u003e676 MB, 1293 MB goal, 36 P\r\n251.54user 1316.06system 2:00.37elapsed 1302%CPU (0avgtext+0avgdata 21715744maxresident)k\r\n0inputs+0outputs (0major+22501185minor)pagefaults 0swaps\r\n```\r\n\r\nHere's the same test, but run with `numactl --membind 0,1`. Note the mark termination times tightly grouped between 124ms and 138ms, 1 million page faults, and only 37 seconds of system time. (I'm interested in the time spent on page faults, not their number, but this is what `time` provides.)\r\n\r\n```\r\n$ /usr/bin/time numactl --membind 0,1 -- env GODEBUG=gctrace=1,gcshrinkstackoff=1 ./issue12967 -d 2m -growers 100 -livers 1000000\r\ngc 1 @0.005s 6%: 0.24+0.83+0.89 ms clock, 3.4+0.11/2.2/0.049+12 ms cpu, 100-\u003e100-\u003e100 MB, 101 MB goal, 36 P\r\ngc 2 @0.025s 5%: 0.18+2.7+1.4 ms clock, 2.3+0.16/17/2.8+18 ms cpu, 101-\u003e101-\u003e101 MB, 200 MB goal, 36 P\r\ngc 3 @0.365s 2%: 1.2+15+5.5 ms clock, 33+0.005/123/174+143 ms cpu, 151-\u003e152-\u003e129 MB, 202 MB goal, 36 P\r\ngc 4 @0.832s 2%: 3.7+32+14 ms clock, 59+1.2/279/517+226 ms cpu, 224-\u003e226-\u003e170 MB, 257 MB goal, 36 P\r\ngc 5 @1.631s 4%: 7.4+55+31 ms clock, 209+23/435/1120+895 ms cpu, 315-\u003e317-\u003e228 MB, 338 MB goal, 36 P\r\ngc 6 @2.837s 4%: 12+100+49 ms clock, 259+24/853/1579+1035 ms cpu, 445-\u003e448-\u003e313 MB, 456 MB goal, 36 P\r\ngc 7 @4.718s 4%: 20+152+77 ms clock, 550+19/1188/2633+2090 ms cpu, 606-\u003e625-\u003e436 MB, 621 MB goal, 36 P\r\ngc 8 @7.194s 4%: 27+190+110 ms clock, 641+20/1588/3803+2533 ms cpu, 822-\u003e838-\u003e585 MB, 837 MB goal, 36 P\r\ngc 9 @11.868s 4%: 38+214+132 ms clock, 1230+0/1769/5194+4235 ms cpu, 1121-\u003e1141-\u003e666 MB, 1141 MB goal, 36 P\r\ngc 10 @18.384s 4%: 30+203+127 ms clock, 983+0/1642/4815+4084 ms cpu, 1266-\u003e1276-\u003e656 MB, 1293 MB goal, 36 P\r\ngc 11 @24.959s 3%: 44+204+124 ms clock, 1422+0/1681/4933+3980 ms cpu, 1266-\u003e1276-\u003e656 MB, 1293 MB goal, 36 P\r\ngc 12 @31.505s 3%: 30+214+128 ms clock, 580+0/1796/4596+2439 ms cpu, 1266-\u003e1276-\u003e656 MB, 1292 MB goal, 36 P\r\ngc 13 @38.057s 3%: 46+212+129 ms clock, 1481+0/1747/5116+4147 ms cpu, 1266-\u003e1276-\u003e656 MB, 1293 MB goal, 36 P\r\ngc 14 @44.618s 3%: 41+200+134 ms clock, 1341+0/1650/4775+4312 ms cpu, 1266-\u003e1276-\u003e656 MB, 1293 MB goal, 36 P\r\ngc 15 @51.171s 3%: 34+252+138 ms clock, 656+0/2097/4271+2624 ms cpu, 1266-\u003e1286-\u003e666 MB, 1293 MB goal, 36 P\r\ngc 16 @57.653s 3%: 40+211+131 ms clock, 1298+0/1743/4642+4211 ms cpu, 1266-\u003e1276-\u003e656 MB, 1293 MB goal, 36 P\r\ngc 17 @64.204s 3%: 33+200+135 ms clock, 1087+0/1654/4892+4328 ms cpu, 1266-\u003e1276-\u003e656 MB, 1293 MB goal, 36 P\r\ngc 18 @70.745s 3%: 46+210+125 ms clock, 1478+0/1737/4750+4029 ms cpu, 1266-\u003e1276-\u003e656 MB, 1293 MB goal, 36 P\r\ngc 19 @77.308s 3%: 38+201+131 ms clock, 1238+0/1661/4877+4200 ms cpu, 1266-\u003e1276-\u003e656 MB, 1293 MB goal, 36 P\r\ngc 20 @83.855s 3%: 29+254+131 ms clock, 503+0/2093/4239+2227 ms cpu, 1266-\u003e1286-\u003e666 MB, 1293 MB goal, 36 P\r\ngc 21 @90.335s 3%: 45+204+131 ms clock, 1460+0/1682/4774+4201 ms cpu, 1266-\u003e1276-\u003e656 MB, 1293 MB goal, 36 P\r\ngc 22 @96.880s 3%: 38+203+129 ms clock, 1220+0/1681/4936+4139 ms cpu, 1266-\u003e1276-\u003e656 MB, 1293 MB goal, 36 P\r\ngc 23 @103.423s 3%: 44+218+136 ms clock, 1418+0/1764/4832+4369 ms cpu, 1266-\u003e1286-\u003e666 MB, 1293 MB goal, 36 P\r\ngc 24 @109.898s 3%: 43+204+127 ms clock, 1406+0/1684/4902+4083 ms cpu, 1266-\u003e1276-\u003e656 MB, 1292 MB goal, 36 P\r\ngc 25 @116.454s 3%: 30+228+130 ms clock, 602+0/1922/4525+2602 ms cpu, 1266-\u003e1286-\u003e676 MB, 1293 MB goal, 36 P\r\n458.58user 37.48system 2:00.36elapsed 412%CPU (0avgtext+0avgdata 21675600maxresident)k\r\n0inputs+0outputs (0major+1055953minor)pagefaults 0swaps\r\n```\r\n\r\nThe test program follows:\r\n\r\n```\r\npackage main\r\n\r\nimport (\r\n\t\"flag\"\r\n\t\"os\"\r\n\t\"time\"\r\n)\r\n\r\nconst (\r\n\tballastSize   = 100 \u003c\u003c 20\r\n\tgarbageSize   = 10 \u003c\u003c 20\r\n\tgarbagePeriod = 100 * time.Millisecond\r\n\r\n\tstackSize   = 10 \u003c\u003c 10\r\n\tstackPeriod = 5000 * time.Millisecond\r\n\tliveSize    = 1 \u003c\u003c 10\r\n)\r\n\r\nvar (\r\n\tballast []byte\r\n\tgarbage []byte\r\n)\r\n\r\nfunc churn() {\r\n\tballast = make([]byte, ballastSize)\r\n\r\n\tfor {\r\n\t\ttime.Sleep(garbagePeriod)\r\n\t\tgarbage = make([]byte, garbageSize)\r\n\t}\r\n}\r\n\r\nfunc run(ch chan struct{}) {\r\n\tfor {\r\n\t\tgrow(ch)\r\n\t\t\u003c-ch\r\n\t}\r\n}\r\n\r\n//go:noinline\r\nfunc grow(ch chan struct{}) *byte {\r\n\tvar s [stackSize / 8]*byte\r\n\t\u003c-ch\r\n\treturn s[0]\r\n}\r\n\r\nfunc live(ch chan struct{}) {\r\n\tvar s [liveSize]byte\r\n\tfor {\r\n\t\t\u003c-ch\r\n\t\t// read/write the stack, so it doesn't go unused\r\n\t\tfor i := 0; i \u003c len(s); i += 256 {\r\n\t\t\ts[i]++\r\n\t\t}\r\n\t}\r\n}\r\n\r\nfunc main() {\r\n\td := flag.Duration(\"d\", 10*time.Second, \"Run duration\")\r\n\tgrowers := flag.Int(\"growers\", 1e4, \"Number of goroutines with growing/shrinking stacks\")\r\n\tlivers := flag.Int(\"livers\", 1e5, \"Number of goroutines with permanent live stacks\")\r\n\tflag.Parse()\r\n\r\n\ttime.AfterFunc(*d, func() { os.Exit(0) })\r\n\r\n\tgo churn()\r\n\r\n\tvar chs []chan struct{}\r\n\r\n\t// some goroutines will periodically grow their stacks\r\n\tfor i := 0; i \u003c *growers; i++ {\r\n\t\tchs = append(chs, make(chan struct{}))\r\n\t\tgo run(chs[len(chs)-1])\r\n\t}\r\n\r\n\t// some goroutines will have large live stacks, which they periodically access\r\n\tfor i := 0; i \u003c *livers; i++ {\r\n\t\tchs = append(chs, make(chan struct{}))\r\n\t\tgo live(chs[len(chs)-1])\r\n\t}\r\n\r\n\tfor range time.Tick(stackPeriod / 2) {\r\n\t\tfor _, ch := range chs {\r\n\t\t\tch \u003c- struct{}{}\r\n\t\t}\r\n\t}\r\n}\r\n```\r\n",
	"user": {
		"login": "rhysh",
		"id": 230685,
		"type": "User",
		"site_admin": false
	},
	"assignee": {
		"login": "aclements",
		"id": 2688315,
		"type": "User",
		"site_admin": false
	},
	"comments": 9,
	"closed_at": "2016-05-25T17:34:11Z",
	"created_at": "2016-02-19T07:16:08Z",
	"updated_at": "2016-05-25T17:34:11Z",
	"milestone": {
		"id": 1414133,
		"number": 31,
		"title": "Go1.7"
	}
}
