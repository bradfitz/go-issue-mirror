{
	"id": 198527889,
	"body": "I sent pull request kubernetes/kubernetes#23210 to clean up one use of time.After that occurs on a particularly highly executed path during the e2e test. See the commit description for details.\r\n\r\nTimes for the kubemark e2e test using Go 1.4.2 and Go 1.6.0, both before and after the timer fix:\r\n\r\n```\r\np50: 148±7   p90: 328±19  p99: 513±29  n: 10  Go 1.4.2\r\np50: 141±9   p90: 383±32  p99: 604±44  n: 11  Go 1.6.0\r\n\r\np50: 151±8   p90: 339±19  p99: 479±20  n: 9   Go 1.4.2 after timer fix\r\np50: 140±14  p90: 360±31  p99: 483±39  n: 10  Go 1.6.0 after timer fix\r\n```\r\n\r\nI believe that with the timer fix applied, Go 1.6.0 is safe for Kubernetes to merge. As mentioned in the PR commit message, I also intend to make Go 1.7 not require that kind of fix, so that all the other uses of time.After will become asymptotically more efficient automatically. This is #8898. Once #8898 is submitted, then I am OK with recognizing `case \u003c-time.After(d)` in the compiler, #8895, which will make things a constant factor more efficient.\r\n\r\n@dvyukov has also been investigating some other possible causes of high 99th percentile latency as part of investigating #14790. He has a number of CLs pending as I write this, and they're linked from that issue.\r\n\r\nOne final note, for @wojtek-t and @gmarek, is that I think Kubernetes may be significantly mismeasuring its server latency in this benchmark, at least with Go 1.4. A 'LIST nodes' RPC comes in about every 5 seconds, and a GC in Go 1.4 takes about 100ms. That means there's about a 1 in 50 chance of the RPC arriving during the GC. If that happens, the RPC sits in a kernel receive buffer while the GC finishes; only once the GC is done does the apiserver read the request and start its timer. The reported latency therefore ignores the time spent waiting for the GC that was going on. Still using very rough numbers, this will affect about 2% of requests, and it will add half a GC pause to each on average. Without knowing the precise shape of the underlying distribution it's hard to estimate the effect on 99th percentile latency, but certainly there is an effect. The reported 99th percentile latency will be lower, perhaps significantly lower, than the actual 99th percentile latency.\r\n\r\nGo 1.5 introduced a concurrent garbage collector, so that the actual stop-the-world pauses, during which the apiserver ignores incoming network traffic, are much smaller. The apiserver should be able to read the request and start the timer earlier in the cases just described, although since a GC is still in progress taking up some of the CPU, the overall time spent on the request will be longer. So part of the reported latency increase from Go 1.4 to Go 1.5 may well be real problems introduced by Go 1.5, but part of the reported latency increase is from reporting something closer to the actual latency.\r\n\r\nI think it's encouraging that, after the timer fix, moving from Go 1.4 to Go 1.6 reports basically no latency increase. Since you'd expect the reported latencies to have gone up due to the more accurate reporting but the reported latencies are level, that suggests that actual latencies went down from Go 1.4 to Go 1.6, a win for users if not for the SLO/SLA measurement.\r\n\r\nI'm hopeful that the combination of Dmitry's scheduler fixes, the timer channel fixes, and @RLH and @aclements's overall GC throughput fixes will make the kubemark latency even lower in Go 1.7. Now that we can run the benchmark, I intend to make sure we test that theory.\r\n\r\nOnce these fixes are all in and I've tested with current Go 1.7 master (on my own cluster, not woktek's), I will close this issue.\r\n\r\nP.S. The ± above are standard deviation, and all the times are 'LIST nodes' apiserver-reported latency in ms.",
	"user": {
		"login": "rsc",
		"id": 104030,
		"type": "User",
		"site_admin": false
	},
	"reactions": {
		"total_count": 1,
		"+1": 1
	},
	"created_at": "2016-03-18T20:23:34Z",
	"updated_at": "2016-03-18T20:29:26Z"
}
