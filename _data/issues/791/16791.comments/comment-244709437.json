{
	"id": 244709437,
	"body": "I took the approach I mentioned earlier (operating on bytes instead of runes) and extended it a bit. What I'm outlining here is an effort to achieve feature parity (and thus a drop-in replacement for the current reader) with improved performance.\r\n\r\nThe main idea is as follows: instead of constructing each field rune by rune (or even byte by byte), why not just check each byte and when an action is required (e.g. a newline/separator/carriage return/quotation mark is hit), slice out everything buffered so far, starting at the end of the previous field. This way we eliminate one allocation per field (a costly one at that). (We may even call `string()` directly on this underlying slice, since `string` copies it out.)\r\n\r\nThis can be done quite easily for unquoted fields, because they are always contiguous in memory, so we just slice them out. There is a bit more work for quoted fields with non-lazy quotes within - e.g. \"this is \"\"a field\"\" with quotes\" would need to be sliced three times to cut out the double quotes - still less work then byte by byte appending.\r\n\r\nIn order to use this slicing of loaded data, I opted for a fixed-sized buffer that I read into and then slice that (or `string` to be precise, so we're good, since `string()` copies the data) once the end of a field is hit. At the end of a CSV row, we take the remaining buffer and move it to the beginning of the buffer, so that the next `Read()` call can reuse this. *I'm not quite sure it's okay we advance the reader more than necessary* - that is, if we just want to read one line that's shorter than 256 bytes (my arbitrary choice), we will have advanced the reader by that much. If we're in a field at the end of our buffer, we save the remainder to a separate slice and read new data (and stitch together afterwards).\r\n\r\nThere are still some issues, obviously:\r\n- saving intermediate data between reads is quite wonky and does not work at times (bloody off by ones)\r\n- at the end of a `Read()`, do we copy the remainder of the buffer to the beginning or do we just save the position we're at for the next `Read()` to pick it up? The former is implemented now, the latter might be a bit quicker for narrow columns\r\n- it currently supports just the basics - no quotes within fields (be it proper quotes or lazy ones), carriage returns are folded into newlines only at the end of a CSV lines, not within quoted fields, no support for comments etc.\r\n\r\nI did some simple benchmarking, but it's a bit premature, since the output is not always correct. The speedup is quite nice thought, around 2-3x, but let's wait for a bit for more sophisticated testing.\r\n\r\nhttps://gist.github.com/kokes/3924255b9629b0bef87f8127d473cd56",
	"user": {
		"login": "kokes",
		"id": 8451755,
		"type": "User",
		"site_admin": false
	},
	"created_at": "2016-09-05T10:03:53Z",
	"updated_at": "2016-09-05T10:03:53Z"
}
