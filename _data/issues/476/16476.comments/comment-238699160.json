{
	"id": 238699160,
	"body": "On Fri, Jul 22, 2016 at 5:16 PM, Bishop Brock \u003cnotifications@github.com\u003e\nwrote:\n\n\u003e Please answer these questions before submitting your issue. Thanks!\n\u003e\n\u003e    1.\n\u003e\n\u003e    What version of Go are you using (go version)?\n\u003e    Go 1.7beta2\n\u003e    2.\n\u003e\n\u003e    What operating system and processor architecture are you using (go env\n\u003e    )?\n\u003e    X86 (Ivy Bridge) - Ubuntu 16.04\n\u003e    POWER8 - Ubuntu 15.10\n\u003e\n\u003e This issue/proposal is a bit long, and likely only of interest to those\n\u003e interested in goroutine scheduling.\n\u003e\n\u003e I work on the Hyperledger fabric (https://github.com/hyperledger/fabric),\n\u003e a large Go application implementing a permissioned blockchain. As part of\n\u003e this work I have observed what I would call \"excessive\" amounts of\n\u003e cumulative time spent in runtime.findrunnable when running on large\n\u003e systems with GOMAXPROCS defaulting to the number of available CPUs. In the\n\u003e following I assume the reader is familiar with the findrunnable routine\n\u003e in proc.go.\n\u003e\n\u003e Drilling down into a findrunnable profile, the obvious culprit is seen to\n\u003e be the work-stealing loop. This loop is inefficient on large systems for\n\u003e several reasons:\n\u003e\n\u003e 1) \"Spinners\" poll the system 4 times while holding a P, and all threads\n\u003e poll once again after releasing their P.\n\u003e\n\u003e 2) The stealing loop checks for stealable work from all Ps, including Ps\n\u003e that have no possibility of having any work to steal. The atomic operations\n\u003e used to load the queue pointers in runqgrab require synchronization\n\u003e primitives on some architectures, and a subroutine call overhead on all\n\u003e architectures. This global polling is disruptive in an SMP-coherence sense,\n\u003e since the poller must pull cache lines from around the system in order to\n\u003e examine only a few fields of each line. The randomized polling order also\n\u003e defeats the hardware's prefetching heuristics.\n\u003e\nRandomized polling order gives you provable guarantees about the time\nrequired to find work, though.\n\n\u003e Regarding 1): I understand why it is good to poll at least twice - First\n\u003e for ez-pickin's from the local run queues, and a second pass for the\n\u003e longer-latency runnext stealing. It occurred to me that perhaps 4 loops\n\u003e were made in Go 1.6 because the randomization used there was not guaranteed\n\u003e to visit every P, so polling 4X increased the odds of looking at every\n\u003e local queue. Now that this has been fixed in Go 1.7, polling more than\n\u003e twice is arguably not necessary. The polling with runnext grabs included\n\u003e is so thourough that once this loop is finished there is no *a priori*\n\u003e reason to expect that another pass will bear fruit.\n\u003e\nThat sounds reasonable to me.  If you can demonstrate that only 2 passes\ninstead of 4 improves performance, we could do that.  I don't remember why\nthose extra iterations are in there, maybe Dmitry knows.  There's always a\ntradeoff in looking for more work vs. sleeping, I just want to make sure we\ndon't improve cases you're worried about and make lots of other cases worse.\n\n\u003e Regarding 2): Note that the answer to the question: \"Could this P possibly\n\u003e have any work to steal?\" can be efficiently centralized since the answer is\n\u003e relatively rarely modified but relatively often observed. I've created a\n\u003e modified scheduler that includes a global array called mayhavework that\n\u003e is indexed by the id of a P. Currently, mayhavework[i] is false whenever\n\u003e a P is queued in the list of idle Ps, and true otherwise. More aggressive\n\u003e update protocols are also possible, but this simple protocol is sufficient\n\u003e to illustrate the benefit.\n\u003e\n\u003e Setting/clearing mayhavework[i] adds a small overhead to queue management\n\u003e of idle Ps, as well as a test during polling loops. Note that the polling\n\u003e loop in the \"delicate dance\" already includes what appears to be a\n\u003e redundant guard of allp[i] != nil which is not made by the work-stealing\n\u003e loop.\n\u003e\nThat sounds reasonable also.  The only thing I worry about is false sharing\nbetween mayhavework[i] and mayhavework[i+1], we'd need padding to ensure\nthat doesn't happen. (Ps with work to do would otherwise never have to\nwrite-share a cache line with any other P.)  Q: Could this be a field of P\ninstead?\n\n\u003e Here are some results for an example Hyperledger fabric benchmark running\n\u003e on a 4-socket X86 Ivy Bridge server with 120 hardware threads. These\n\u003e examples are for illustration only and are not claimed to be exhaustive;\n\u003e The arguments for the proposal should be valid based on first principles.\n\u003e Performance (throuhgput) of the server is measured in transactions per\n\u003e second (TPS). Cumulative profile percentages were reported by the Golang\n\u003e net/http/pprof profiling service running in the application. Results for\n\u003e GOMAXPROCS eqaul to 12 and 120 (the default) are presented.\n\u003e\n\u003e GOMAXPROCS = 12\n\u003e -------------------------------------------------------------------------\n\u003e                         Baseline   2 Stealing Loops Only   Full Proposal\n\u003e -------------------------------------------------------------------------\n\u003e Throughput               996 TPS          987 TPS              997 TPS\n\u003e runtime.findrunnable      14.0%            13.5%                14.1%\n\u003e -------------------------------------------------------------------------\n\u003e\n\u003e GOMAXPROCS = 120\n\u003e -------------------------------------------------------------------------\n\u003e                         Baseline   2 Stealing Loops Only   Full Proposal\n\u003e -------------------------------------------------------------------------\n\u003e Throughput               991 TPS          963 TPS              997 TPS\n\u003e runtime.findrunnable      28.2%            21.9%                16.5%\n\u003e -------------------------------------------------------------------------\n\u003e\n\u003e This full proposal has no effect on findrunnable overhead or performance\n\u003e on this system with GOMAXPROCS=12. However I have also run the experiment\n\u003e on a POWER8 server and observed a reduction from 14.5% to 9.4% of\n\u003e findrunnable overhead on that system with GOMAXPROCS=12. This may be due\n\u003e to the fact that atomic.Load includes a synchronization instruction on\n\u003e POWER.\n\u003e\n\u003e For the full system there is a significant reduction in scheduling\n\u003e overhead. It is not clear whether the slight performance drop with 2\n\u003e stealing loops only is real, or due to experimental variation. In a number\n\u003e of experiments (on POWER8) I have seen what I believe are small, real\n\u003e performance increases *and* decreases from these modified heuristics,\n\u003e which vary based on the particular benchmark.\n\u003e\n\u003e To summarize the proposal:\n\u003e\n\u003e 1) Only poll twice in the work stealing loop;\n\u003e\n\u003e 2) Implement an efficient centralized data structure that records which Ps\n\u003e might possibly have any work to steal.\n\u003e\n\u003e Bishop Brock\n\u003e\n\u003e â€”\n\u003e You are receiving this because you are subscribed to this thread.\n\u003e Reply to this email directly, view it on GitHub\n\u003e \u003chttps://github.com/golang/go/issues/16476\u003e, or mute the thread\n\u003e \u003chttps://github.com/notifications/unsubscribe-auth/AGkgIJ2zqc6paFu3EIpWzMIS2J0N_UT4ks5qYV16gaJpZM4JTOyN\u003e\n\u003e .\n\u003e\n",
	"user": {
		"login": "randall77",
		"id": 6889504,
		"type": "User",
		"site_admin": false
	},
	"created_at": "2016-08-09T21:34:37Z",
	"updated_at": "2016-08-09T21:34:37Z"
}
