{
	"id": 84101174,
	"number": 11035,
	"state": "closed",
	"title": "runtime: Heap fragmentation causing memory leaks",
	"body": "1. What version of Go are you using (`go version`)?\r\n\r\n        $ go version\r\n        go version go1.4.2 linux/amd64\r\n        $ go version\r\n        go version devel +70cf735 Tue Jun 2 13:55:40 2015 +0000 linux/amd64\r\n\r\n2. What operating system and processor architecture are you using?\r\n\r\n        $ uname -a\r\n        Linux nitric 3.13.0-53-generic #89-Ubuntu SMP Wed May 20 10:34:39 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux\r\n        $ cat /proc/cpuinfo \r\n        processor       : 0\r\n        vendor_id       : GenuineIntel\r\n        cpu family      : 6\r\n        model           : 58\r\n        model name      : Intel(R) Core(TM) i7-3537U CPU @ 2.00GHz\r\n        stepping        : 9\r\n        microcode       : 0x17\r\n        cpu MHz         : 2000.000\r\n        cache size      : 4096 KB\r\n        physical id     : 0\r\n        siblings        : 4\r\n        core id         : 0\r\n        cpu cores       : 2\r\n        apicid          : 0\r\n        initial apicid  : 0\r\n        fpu             : yes\r\n        fpu_exception   : yes\r\n        cpuid level     : 13\r\n        wp              : yes\r\n        flags           : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic popcnt tsc_deadline_timer xsave avx f16c rdrand lahf_lm ida arat epb xsaveopt pln pts dtherm tpr_shadow vnmi flexpriority ept vpid fsgsbase smep erms\r\n        bogomips        : 4988.67\r\n        clflush size    : 64\r\n        cache_alignment : 64\r\n        address sizes   : 36 bits physical, 48 bits virtual\r\n        power management:\r\n        $ cat /proc/meminfo \r\n        MemTotal:        7730856 kB\r\n\r\n3. What did you do?\r\n\r\n    run https://github.com/timtadh/fsm on a large graph (see explanation below)\r\n\r\n4. What did you expect to see?\r\n\r\n    memory usage remain relatively stable\r\n\r\n5. What did you see instead?\r\n\r\n    Growth of memory throughout run of program, sometimes extremely rapid growth.\r\n\r\n## Explanation\r\n\r\nI have been working on a frequent subgraph mining algorithm in Go. In\r\ncase you are not familiar with frequent subgraph mining, it involves\r\nlooking in a very large graph for repeated subgraphs. This is an\r\nexponential problem and can require a lot of memory.\r\n\r\nTo solve the memory issue, I wrote a memory mapped B+Tree\r\n[fs2/bptree](https://github.com/timtadh/fs2) which stores the candidate\r\nsubgraphs. The B+Tree works fine and generally has very little overhead\r\nin terms of using memory.\r\n\r\nWhat does have overhead is generating the candidate graphs. Basically\r\nthe process goes something like this:\r\n\r\n1. A partition of subgraphs is loaded from the B+Tree. Each graph in the\r\n   partition is isomorphic to every other graph in the partition. These\r\n   are known as the embeddings.\r\n2. If there are not enough subgraphs in the partition the partition is\r\n   discarded (and therefore eventually garbage collected).\r\n3. Otherwise, each embedding is extended by one edge to create\r\n   candidates for the next round of mining. The embeddings are extended\r\n   multiple times, once for each edge in the surround context.\r\n4. The extensions are then stored in the B+Tree.\r\n5. The extensions are garbaged collected, as is the partition.\r\n\r\nIn theory, the overall memory usage (outside of the B+Trees) should not\r\ngrow in this process. However, it does grow. I believe it is due to the\r\nheap becoming fragmented. I am willing to do considerable legwork to\r\nboth show this conclusively and to help fix the problem.\r\n\r\n#### This may not be fixable\r\n\r\nBefore, we go into my reasons for believing this is a fragmentation\r\nissue, let me first not that this may not be fixable. The solutions I\r\nhave outlined at the bottom of the post may be impractical. However, I\r\nthink it would be worth trying to fix it.\r\n\r\n## In Depth\r\n\r\nI have been profiling my program extensively for several months to try\r\nand get rid of memory allocations as much as possible. However, there\r\nare some limits to my ability to completely get rid of allocations\r\nduring the mining process. It also becomes difficult to reason about\r\nownership of particular bits of memory (like the subgraph objects) when\r\nthey are flying around between different goroutines.\r\n\r\nOne piece of evidence I have gathered for this being a fragmentation\r\nproblem is the statistics reported by `GODEBUG=gctrace=1`. Here is a\r\nsnapshot after the program has been running for a while:\r\n\r\n\r\n    gc7993(4): 8+9+1060+6 us, 2 -\u003e 4 MB, 77327 (431357683-431280356) objects, 33 goroutines, 779/676/0 sweeps, 17(1319) handoff, 6(35) steal, 69/0/0 yields\r\n    gc7994(4): 7+9+1556+21 us, 2 -\u003e 4 MB, 76847 (431409057-431332210) objects, 33 goroutines, 779/679/0 sweeps, 19(891) handoff, 10(67) steal, 118/60/8 yields\r\n    gc7995(4): 17+6+1151+12 us, 2 -\u003e 4 MB, 76512 (431459886-431383374) objects, 33 goroutines, 779/633/0 sweeps, 22(1467) handoff, 9(70) steal, 103/32/3 yields\r\n    gc7996(4): 6+7+1082+122 us, 2 -\u003e 4 MB, 76689 (431510921-431434232) objects, 33 goroutines, 779/634/0 sweeps, 30(1316) handoff, 6(49) steal, 114/37/4 yields\r\n    gc7997(4): 7+9+1155+112 us, 2 -\u003e 4 MB, 76474 (431561707-431485233) objects, 33 goroutines, 779/627/0 sweeps, 13(704) handoff, 11(76) steal, 89/12/1 yields\r\n    gc7998(4): 8+10+1341+5 us, 2 -\u003e 4 MB, 77134 (431613156-431536022) objects, 33 goroutines, 779/644/0 sweeps, 23(1502) handoff, 9(65) steal, 70/0/0 yields\r\n    gc7999(4): 7+13+1188+5 us, 2 -\u003e 4 MB, 76252 (431663721-431587469) objects, 33 goroutines, 779/642/0 sweeps, 19(1183) handoff, 7(43) steal, 124/40/3 yields\r\n    gc8000(4): 16+6+1055+4 us, 2 -\u003e 4 MB, 77080 (431715341-431638261) objects, 33 goroutines, 779/667/0 sweeps, 27(2229) handoff, 13(94) steal, 110/21/0 yields\r\n    gc8001(4): 8+9+1099+4 us, 2 -\u003e 4 MB, 76757 (431766506-431689749) objects, 33 goroutines, 779/615/0 sweeps, 37(2424) handoff, 13(80) steal, 112/11/0 yields\r\n    gc8002(4): 6+6+1046+10 us, 2 -\u003e 4 MB, 76458 (431817363-431740905) objects, 33 goroutines, 779/673/0 sweeps, 24(1090) handoff, 8(54) steal, 99/30/3 yields\r\n    gc8003(4): 7+9+981+4 us, 2 -\u003e 4 MB, 77446 (431869376-431791930) objects, 33 goroutines, 779/657/0 sweeps, 18(1567) handoff, 12(84) steal, 71/0/0 yields\r\n    gc8004(4): 9+11+956+4 us, 2 -\u003e 4 MB, 76508 (431920315-431843807) objects, 33 goroutines, 779/675/0 sweeps, 35(2390) handoff, 7(35) steal, 122/11/0 yields\r\n    gc8005(4): 7+1530+270+3 us, 2 -\u003e 4 MB, 76762 (431971504-431894742) objects, 33 goroutines, 779/648/0 sweeps, 18(1058) handoff, 11(76) steal, 72/30/3 yields\r\n    gc8006(4): 7+7+1151+5 us, 2 -\u003e 4 MB, 77295 (432023032-431945737) objects, 33 goroutines, 779/549/0 sweeps, 21(1517) handoff, 11(78) steal, 96/30/3 yields\r\n    gc8007(4): 9+13+1447+5 us, 2 -\u003e 4 MB, 75554 (432072776-431997222) objects, 33 goroutines, 779/692/0 sweeps, 22(1054) handoff, 6(42) steal, 132/35/3 yields\r\n    gc8008(4): 17+26+1594+21 us, 2 -\u003e 4 MB, 76957 (432124020-432047063) objects, 33 goroutines, 779/421/0 sweeps, 18(1065) handoff, 12(82) steal, 127/43/4 yields\r\n    gc8009(4): 7+7+1206+5 us, 2 -\u003e 4 MB, 77113 (432175501-432098388) objects, 33 goroutines, 779/639/0 sweeps, 42(2590) handoff, 10(66) steal, 153/31/3 yields\r\n\r\n\r\nOne, thing to notice, the number of live objects stays relatively\r\nconstant throughout the execution of the program. It stays in the\r\nneighborhood of 70k-80k on a 4 core machine. Inspecting the memory (as\r\nreported in /proc/\u003cpid\u003e/maps) shows that the Go heap does continue to\r\ngrow. Allowing the program to run for a very long time essentially\r\ncauses the B+Trees memory maps to be evicted from memory to make space\r\nfor the ever growing heap.\r\n\r\n## Sources of Memory Allocations\r\n\r\n\u003e `go tool pprof -png bin/fsm mem.pprof \u003e mem.png`\r\n\r\n![mem](https://cloud.githubusercontent.com/assets/38620/7940281/e2b32e38-091d-11e5-9800-e3f8e677fd71.png)\r\n\r\n\u003e `go tool pprof -png -alloc_space bin/fsm mem.pprof \u003e mem.png`\r\n\r\n![mem](https://cloud.githubusercontent.com/assets/38620/7940227/a7c6eef4-091d-11e5-8d56-f7f864abb06f.png)\r\n\r\nAs you can see, the sources of allocations are what you would expect:\r\n\r\n1. De-serializing the subgraphs\r\n2. Creating new ones (the `canonSubGraph`)\r\n3. Creating the cursor objects for iterating through the B+Trees\r\n\r\nI am obviously experimenting with ways to reduce the allocations\r\nperformed by these operations. However, at some point I may just reach\r\nthe limits with what is easily achievable in Go and I will have to\r\nswitch to either C++ or Rust (which did not really exist when I started\r\nthis project in 2012).\r\n\r\n## Does Go 1.5 Help?\r\n\r\nI compiled and ran master yesterday. Unfortunately, my program does not\r\ncurrently execute on master. I get lots of weird errors. Maybe this is\r\nbecause I do a lot of unsafe things in my B+Tree implementation? I don't\r\nknow. I can post those errors if you want to see them but I think they\r\nare separate issue.\r\n\r\n## Possible Solutions\r\n\r\nThe best idea I can think of would be a `runtime` routine which would\r\nstop the world and do a heap compaction. This would never be triggered\r\nautomatically (that would be catastrophic in say an http server).\r\nHowever, a program could call `runtime.HeapCompact()` which would cause\r\nthe mutator to stop and a full garbage collect + compaction to run.\r\n\r\nThe way this could work is to have a second garbage collector which is\r\neither a *copying-collector* or a *mark-and-compact* collector. The\r\nbasic idea of both is to find all of the current objects active at the\r\ntime of collection. Then, to one by one move each object to a new\r\nlocation and rewrite all pointers into the object. This is obviously an\r\nexpensive operations.\r\n\r\nDrawbacks\r\n\r\n1. We can only find pointers from Go structures. This means any pointers\r\n   passed to native code now point somewhere else.\r\n\r\n2. If we believe (conservatively) an object is being pointed to by a\r\n   native structure we cannot move that object.\r\n\r\nBenefits\r\n\r\n1. This operation only happens when the programmer asks it to happen. So\r\n   they can potentially control negative side effects from pointers\r\n   involving non Go code.\r\n\r\n2. It can potentially perfectly compact the heap.\r\n\r\n3. Some algorithms can improve memory locality by moving frequently\r\n   accessed objects close together.\r\n\r\n### Other Solutions\r\n\r\n#### Incremental\r\n\r\nIncremental compaction during GC or allocations. This would mean that\r\nthe heap would never be fully compacted however, it would limit the\r\namount of work that is done and make the process fully automatic:\r\n\r\nDrawbacks\r\n\r\n1. Everything from a programmer triggered compaction\r\n\r\n2. Breaking expectation of objects staying in one place\r\n\r\n3. Imperfect compaction\r\n\r\nBenefits\r\n\r\n1. Fully automatic\r\n\r\n2. No new runtime methods which need to be supported in the future\r\n\r\n#### Improve the Allocator\r\n\r\nThe other idea is to simply improve the allocator to reduce the\r\nfragmentation in the first place. Programs, like mine, which cause\r\nfragmentation can be collected and their behavior can be studied to\r\nfigure out if there are ways to reduce the fragmentation. \r\n\r\nDrawbacks\r\n\r\n1. No compaction, so the heap will be larger than the idea\r\n\r\nBenefits\r\n\r\n1. Fully automatic\r\n\r\n2. No changes in semantics of the Go memory system\r\n\r\n3. No new runtime methods\r\n\r\n## Conclusion\r\n\r\nI believe Go currently has a heap fragmentation problem that impacts\r\nlong running programs which do lots of small and medium sized\r\nallocations. A potential solution is to add a function\r\n`runtime.HeapCompact()` which would trigger a stop the world compaction.\r\n\r\n\r\n",
	"user": {
		"login": "timtadh",
		"id": 38620,
		"type": "User",
		"site_admin": false
	},
	"labels": [
		{
			"name": "FrozenDueToAge"
		}
	],
	"comments": 24,
	"closed_at": "2015-06-05T21:24:09Z",
	"created_at": "2015-06-02T16:45:13Z",
	"updated_at": "2016-06-25T02:10:05Z",
	"milestone": {
		"id": 905105,
		"number": 1,
		"title": "Go1.5"
	}
}
