{
	"id": 108300334,
	"body": "The Go GCed heap uses a segregated size heap so if the sizes of the objects\nbeing freed are similar to those been allocated fragmentation should not be\na problem.\n\nOn Tue, Jun 2, 2015 at 5:31 PM, Tim Henderson \u003cnotifications@github.com\u003e\nwrote:\n\n\u003e $ go version\n\u003e go version devel +70cf735 Tue Jun 2 13:55:40 2015 +0000 linux/amd64\n\u003e\n\u003e @bradfitz \u003chttps://github.com/bradfitz\u003e / @ianlancetaylor\n\u003e \u003chttps://github.com/ianlancetaylor\u003e Something funky is definitely going\n\u003e on. The\n\u003e pointer that was marked at 0xdead comes from a channel. Basically the\n\u003e code looks like this:\n\u003e\n\u003e type labelGraph struct {\n\u003e     label []byte\n\u003e     sg    *goiso.SubGraph\n\u003e }\n\u003e type Collector struct {\n\u003e     ch   chan *labelGraph\n\u003e     tree store.SubGraphs\n\u003e }\n\u003e func (c *Collector) send(sg *SubGraph) {\n\u003e     label := sg.Label()\n\u003e     c.ch\u003c-\u0026labelGraph{label, sg}\n\u003e }\n\u003e func (c *Collector) collect() {\n\u003e     for lg := range c.ch {\n\u003e         tree.Add(lg.label, lg.sg)\n\u003e     }\n\u003e }\n\u003e func (c *Collector) close() {\n\u003e     close(c.ch)\n\u003e }\n\u003e func otherThread() {\n\u003e     go c.collect()\n\u003e     for sg := range candidates {\n\u003e         c.send(sg)\n\u003e     }\n\u003e     c.close()\n\u003e }\n\u003e\n\u003e The error was coming from deep inside the implementation of\n\u003e tree.Add().\n\u003e\n\u003e In send(sg) I added a check to make sure the values I was putting in\n\u003e were sane. They always were. I added the same check inside of Add().\n\u003e There the check would fail. I pushed the check up one level to be inside\n\u003e of collect. It also failed there.\n\u003e\n\u003e So basically, I was putting good values into the channel but getting\n\u003e dead values out. Super weird. If I turn off using the mmap'ed B+Tree and\n\u003e use a regular datastructure the errors go away.\n\u003e\n\u003e I tried doing something which should have had no effect, I added\n\u003e buffering to the channels.\n\u003e\n\u003e     ch: make(chan *labelGraph, 1)\n\u003e\n\u003e This made the error stop. Which is great. However, now I get a new even\n\u003e more mysterious error:\n\u003e\n\u003e unexpected fault address 0xc208246c90\n\u003e fatal error: fault\n\u003e [signal 0xb code=0x2 addr=0xc208246c90 pc=0xc208246c90]\n\u003e\n\u003e goroutine 200 [running]:\n\u003e runtime.throw(0x602db0, 0x5)\n\u003e         /home/hendersont/srcs/go/src/runtime/panic.go:527 +0x96 fp=0xc208ef3dd8 sp=0xc208ef3dc0\n\u003e runtime.sigpanic()\n\u003e         /home/hendersont/srcs/go/src/runtime/sigpanic_unix.go:27 +0x2ae fp=0xc208ef3e28 sp=0xc208ef3dd8\n\u003e created by github.com/timtadh/fsm/mine.(*Miner).filterAndExtend\n\u003e         /home/hendersont/stuff/research/fsm/src/github.com/timtadh/fsm/mine/mine.go:235 +0x8e\n\u003e\n\u003e Which looks like it might be a segmentation fault? These do not happen\n\u003e every\n\u003e run. The other error was happening consistently. This error seems to only\n\u003e happen when I use anonomous memory maps. If they are backed by files it\n\u003e does\n\u003e not occur (well has not yet occurred).\n\u003e\n\u003e Whatever my program's problems are, the fragmentation problem in the\n\u003e runtime\n\u003e also exists in 1.5. For instance after a successful run I get this with\n\u003e GODEBUG=gctrace=1\n\u003e\n\u003e gc #13181 @266.838s 18%: 0+0+0+10+0 ms clock, 0+0+0+0/10/0+1 ms cpu, 4-\u003e4-\u003e3 MB, 4 MB goal, 4 P\n\u003e gc #13182 @266.869s 18%: 0+0+12+8+0 ms clock, 0+0+0+4/8/0+1 ms cpu, 4-\u003e6-\u003e4 MB, 4 MB goal, 4 P\n\u003e gc #13183 @266.885s 18%: 0+0+0+13+0 ms clock, 0+0+0+0/13/0+2 ms cpu, 4-\u003e4-\u003e3 MB, 7 MB goal, 4 P\n\u003e gc #13184 @266.905s 18%: 0+0+0+11+0 ms clock, 0+0+0+0/11/0+1 ms cpu, 4-\u003e4-\u003e3 MB, 5 MB goal, 4 P\n\u003e gc #13185 @266.929s 18%: 0+0+5+9+0 ms clock, 0+0+0+2/9/0+1 ms cpu, 4-\u003e5-\u003e3 MB, 5 MB goal, 4 P\n\u003e gc #13186 @266.949s 18%: 0+0+0+7+5 ms clock, 0+0+0+0/7/0+23 ms cpu, 4-\u003e4-\u003e2 MB, 5 MB goal, 4 P\n\u003e gc #13187 @266.988s 18%: 0+0+0+32+0 ms clock, 0+0+0+11/12/0+2 ms cpu, 4-\u003e7-\u003e4 MB, 4 MB goal, 4 P\n\u003e gc #13188 @267.012s 18%: 0+0+3+14+0 ms clock, 0+0+0+0/14/0+2 ms cpu, 4-\u003e5-\u003e3 MB, 8 MB goal, 4 P\n\u003e gc #13189 @267.033s 18%: 0+0+0+13+0 ms clock, 0+0+0+0/13/0+1 ms cpu, 4-\u003e5-\u003e3 MB, 5 MB goal, 4 P\n\u003e gc #13190 @267.050s 18%: 0+0+0+10+0 ms clock, 0+0+0+0/10/0+2 ms cpu, 4-\u003e4-\u003e3 MB, 5 MB goal, 4 P\n\u003e gc #13191 @267.082s 18%: 0+0+3+16+0 ms clock, 3+0+0+8/16/0+0 ms cpu, 4-\u003e5-\u003e3 MB, 5 MB goal, 4 P\n\u003e gc #13192 @267.095s 18%: 0+0+0+7+0 ms clock, 0+0+0+0/7/0+2 ms cpu, 4-\u003e4-\u003e2 MB, 6 MB goal, 4 P\n\u003e gc #13193 @267.127s 18%: 4+1+4+12+0 ms clock, 18+1+0+0/12/0+1 ms cpu, 4-\u003e6-\u003e3 MB, 5 MB goal, 4 P\n\u003e gc #13194 @267.145s 18%: 0+0+0+10+0 ms clock, 0+0+0+0/10/0+0 ms cpu, 4-\u003e4-\u003e3 MB, 5 MB goal, 4 P\n\u003e gc #13195 @267.170s 18%: 0+0+1+11+0 ms clock, 2+0+0+0/11/0+2 ms cpu, 4-\u003e5-\u003e3 MB, 5 MB goal, 4 P\n\u003e gc #13196 @267.192s 18%: 0+0+1+10+0 ms clock, 0+0+0+0/10/0+2 ms cpu, 4-\u003e5-\u003e3 MB, 5 MB goal, 4 P\n\u003e gc #13197 @267.215s 18%: 0+0+0+14+0 ms clock, 0+0+0+0/14/0+1 ms cpu, 4-\u003e4-\u003e3 MB, 5 MB goal, 4 P\n\u003e gc #13198 @267.240s 18%: 0+0+3+10+0 ms clock, 3+0+0+0/10/0+2 ms cpu, 4-\u003e5-\u003e3 MB, 4 MB goal, 4 P\n\u003e gc #13199 @267.260s 18%: 0+0+0+14+0 ms clock, 0+0+0+1/14/0+2 ms cpu, 4-\u003e4-\u003e3 MB, 4 MB goal, 4 P\n\u003e gc #13200 @267.287s 18%: 0+0+6+9+0 ms clock, 0+0+0+0/9/0+1 ms cpu, 4-\u003e5-\u003e3 MB, 4 MB goal, 4 P\n\u003e gc #13201 @267.318s 18%: 0+0+10+13+0 ms clock, 0+0+0+0/13/0+1 ms cpu, 4-\u003e5-\u003e4 MB, 5 MB goal, 4 P\n\u003e gc #13202 @267.331s 18%: 0+0+0+9+0 ms clock, 0+0+0+0/9/0+2 ms cpu, 4-\u003e4-\u003e3 MB, 5 MB goal, 4 P\n\u003e gc #13203 @267.367s 18%: 2+0+7+13+0 ms clock, 11+0+0+0/13/0+0 ms cpu, 4-\u003e5-\u003e3 MB, 5 MB goal, 4 P\n\u003e gc #13204 @267.384s 18%: 0+0+0+9+0 ms clock, 0+0+0+0/9/0+1 ms cpu, 4-\u003e5-\u003e3 MB, 5 MB goal, 4 P\n\u003e gc #13205 @267.416s 18%: 0+0+7+12+0 ms clock, 0+0+0+3/12/0+1 ms cpu, 4-\u003e6-\u003e3 MB, 5 MB goal, 4 P\n\u003e\n\u003e But the resident size was continuing to grow at a healthy clip!\n\u003e\n\u003e [image: resource-usage]\n\u003e \u003chttps://cloud.githubusercontent.com/assets/38620/7947587/0cbd18dc-094c-11e5-923e-03baaf5e7e8c.png\u003e\n\u003e\n\u003e This image shows the size of the memory mapped files (as files rather than\n\u003e anonmous mappings) versus the size of the heap which is growing (and\n\u003e continuing\n\u003e to grow actually out pacing the growth rate of the memory maps).\n\u003e Conclusion\n\u003e\n\u003e It looks like\n\u003e\n\u003e    1.\n\u003e\n\u003e    There might be a bug with respect to the unbuffered channels.\n\u003e    Difficult for me to tell if it is your bug or mine. Let me know if\n\u003e    you have any suggestions for sorting that out.\n\u003e    2.\n\u003e\n\u003e    The fragmentation is not fixed by the new collector. The target heap\n\u003e    size is fairly constantly around 4-8 MB but the heap grows at an\n\u003e    unbounded rate.\n\u003e\n\u003e I am interested in helping solve both problems. Let me know how I can\n\u003e help.\n\u003e\n\u003e â€”\n\u003e Reply to this email directly or view it on GitHub\n\u003e \u003chttps://github.com/golang/go/issues/11035#issuecomment-108104091\u003e.\n\u003e\n",
	"user": {
		"login": "RLH",
		"id": 972447,
		"type": "User",
		"site_admin": false
	},
	"created_at": "2015-06-03T11:10:18Z",
	"updated_at": "2015-06-03T11:10:18Z"
}
