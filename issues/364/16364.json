{
	"id": 165440911,
	"number": 16364,
	"state": "open",
	"title": "runtime: Add loosely ordered channels?",
	"body": "**What version of Go are you using (`go version`)?**\r\n`go version go1.6 darwin/amd64`\r\n\r\n**What operating system and processor architecture are you using (`go env`)?**\r\n```\r\nGOARCH=\"amd64\"\r\nGOBIN=\"\"\r\nGOEXE=\"\"\r\nGOHOSTARCH=\"amd64\"\r\nGOHOSTOS=\"darwin\"\r\nGOOS=\"darwin\"\r\nGOPATH=\"/Users/yuriy/gopath\"\r\nGORACE=\"\"\r\nGOROOT=\"/usr/local/go\"\r\nGOTOOLDIR=\"/usr/local/go/pkg/tool/darwin_amd64\"\r\nGO15VENDOREXPERIMENT=\"1\"\r\nCC=\"clang\"\r\nGOGCCFLAGS=\"-fPIC -m64 -pthread -fno-caret-diagnostics -Qunused-arguments -fmessage-length=0 -fno-common\"\r\nCXX=\"clang++\"\r\nCGO_ENABLED=\"1\"\r\n```\r\n**What did you do?**\r\n\r\nI ran a benchmark to see how much time it is needed to process N elements using multiple cores. So what benchmark below does is it runs \"myvalue += 1\" N times in each of 8 goroutines for both consumer and producer threads and checks the results.\r\n\r\nGenerally in some cases it would be great to use a single channel for distributing load (in this case, adding \"1\") among workers and actually get job done faster when you use more cores if the operations themselves do not take much time.\r\n\r\nIt is not achievable with current channels because they imply ordering constraints for events and sometimes you don't need that. So I suggest to consider (maybe?) adding loosely-ordered channels that would allow one to both reduce channel send and receive cost as well as allowing them to scale.\r\n\r\nIn this \"+1\" example the only solution that actually benefits from adding more cores is the sharded channel one.\r\n\r\n```\r\npackage main\r\n\r\nimport (\r\n\t\"sync\"\r\n\t\"sync/atomic\"\r\n\t\"testing\"\r\n)\r\n\r\nconst CORES = 8\r\n\r\ntype BigStruct struct {\r\n\tvalue               int64\r\n\tpreventFalseSharing [1024]byte\r\n}\r\n\r\nvar (\r\n\tmych         = make(chan int64, 10)\r\n\tmychs        = make([]chan int64, CORES)\r\n\tmyvalues     = make([]BigStruct, CORES)\r\n\tmyvalue      = int64(0)\r\n\tmyvalueMutex sync.Mutex\r\n)\r\n\r\nfunc rcvChan(shard int) {\r\n\tmyvalues[shard].value += \u003c-mych\r\n}\r\n\r\nfunc rcvChanSharded(shard int) {\r\n\tmyvalues[shard].value += \u003c-mychs[shard]\r\n}\r\n\r\nfunc sendChan(shard int) {\r\n\tmych \u003c- 1\r\n}\r\n\r\nfunc sendChanSharded(shard int) {\r\n\tmychs[shard] \u003c- 1\r\n}\r\n\r\nfunc sendMutex(shard int) {\r\n\tmyvalueMutex.Lock()\r\n\tmyvalue++\r\n\tmyvalueMutex.Unlock()\r\n}\r\n\r\nfunc sendAtomic(shard int) {\r\n\tatomic.AddInt64(\u0026myvalue, 1)\r\n}\r\n\r\nfunc megaBench(b *testing.B, numproc int, sendfunc, rcvfunc func(int)) {\r\n\tmyvalue = 0\r\n\tmyvalues = make([]BigStruct, CORES)\r\n\twaitCh := make(chan bool)\r\n\tmych = make(chan int64, 10)\r\n\r\n\tfor j := 0; j \u003c numproc; j++ {\r\n\t\tmychs[j] = make(chan int64, 10)\r\n\t}\r\n\r\n\tfor j := 0; j \u003c numproc; j++ {\r\n\t\tgo func(j int) {\r\n\t\t\tfor i := 0; i \u003c b.N; i++ {\r\n\t\t\t\tsendfunc(j)\r\n\t\t\t}\r\n\t\t\twaitCh \u003c- true\r\n\t\t}(j)\r\n\t\tgo func(j int) {\r\n\t\t\tfor i := 0; i \u003c b.N; i++ {\r\n\t\t\t\trcvfunc(j)\r\n\t\t\t}\r\n\t\t\twaitCh \u003c- true\r\n\t\t}(j)\r\n\t}\r\n\r\n\tfor j := 0; j \u003c numproc; j++ {\r\n\t\t\u003c-waitCh\r\n\t\t\u003c-waitCh\r\n\t}\r\n\r\n\tfor j := 0; j \u003c numproc; j++ {\r\n\t\tmyvalue += myvalues[j].value\r\n\t}\r\n\r\n\tif myvalue != int64(b.N*numproc) {\r\n\t\tb.Errorf(\"Wrong number of iterations: got %d, expected %d\", myvalue, b.N*numproc)\r\n\t}\r\n}\r\n\r\nfunc BenchmarkChan(b *testing.B)        { megaBench(b, CORES, rcvChan, sendChan) }\r\nfunc BenchmarkChanSharded(b *testing.B) { megaBench(b, CORES, rcvChanSharded, sendChanSharded) }\r\nfunc BenchmarkAtomic(b *testing.B)      { megaBench(b, CORES, func(shard int) {}, sendAtomic) }\r\nfunc BenchmarkMutex(b *testing.B)       { megaBench(b, CORES, func(shard int) {}, sendMutex) }\r\n```\r\n\r\n**What did you expect to see?**\r\n\r\nI would really like for channels to scale when more cores are used instead of them slowing down. I do not believe it is possible with current channel constraints so option to allow creation of loosely-ordered channels would be nice instead.\r\n\r\n**What did you see instead?**\r\n\r\n```\r\nBenchmarkChan         \t 2000000\t       840 ns/op\r\nBenchmarkChan-2       \t 1000000\t      1208 ns/op\r\nBenchmarkChan-4       \t 1000000\t      1684 ns/op\r\nBenchmarkChan-8       \t  500000\t      2592 ns/op\r\nBenchmarkChanSharded  \t 2000000\t       833 ns/op\r\nBenchmarkChanSharded-2\t 3000000\t       445 ns/op\r\nBenchmarkChanSharded-4\t 5000000\t       265 ns/op\r\nBenchmarkChanSharded-8\t 5000000\t       237 ns/op\r\nBenchmarkAtomic       \t20000000\t        80.8 ns/op\r\nBenchmarkAtomic-2     \t10000000\t       184 ns/op\r\nBenchmarkAtomic-4     \t10000000\t       215 ns/op\r\nBenchmarkAtomic-8     \t10000000\t       182 ns/op\r\nBenchmarkMutex        \t 5000000\t       241 ns/op\r\nBenchmarkMutex-2      \t 2000000\t       814 ns/op\r\nBenchmarkMutex-4      \t 2000000\t      1021 ns/op\r\nBenchmarkMutex-8      \t 1000000\t      1059 ns/op\r\n```\r\n\r\nYou can see that the only solution that scales (ns/op decreases when you add more cores) is sharded channel one. I have 4 physical cores and 8 logical ones so do not pay too much attention to results of 8 threads.",
	"user": {
		"login": "YuriyNasretdinov",
		"id": 575929,
		"type": "User",
		"site_admin": false
	},
	"labels": [
		{
			"name": "Proposal"
		}
	],
	"comments": 9,
	"created_at": "2016-07-13T23:01:25Z",
	"updated_at": "2016-08-11T01:07:54Z",
	"milestone": {
		"id": 1373555,
		"number": 30,
		"title": "Proposal"
	}
}
