{
	"id": 166457364,
	"number": 16432,
	"state": "closed",
	"title": "runtime: concurrent GC starves mutator and netpoller",
	"body": "Please answer these questions before submitting your issue. Thanks!\r\n\r\n1. What version of Go are you using (`go version`)?\r\n\r\n    go1.7rc2\r\n\r\n2. What operating system and processor architecture are you using (`go env`)?\r\n\r\n    linux/amd64\r\n\r\n3. What did you do?\r\n\r\n    I've got a program that accepts data over 200–300 active network connections, unmarshals the data, reformats and remarshals the data, and sends it over a small number of http connections. It usually has around 1300 goroutines and 200MB of live heap, and triggers a garbage collection about twice a second (the apis for encoding/json and others force a lot of allocations). It runs on a 36-core machine with GOMAXPROCS=36.\r\n\r\n    I took an execution trace from /debug/pprof/trace to try to track down a different GC issue.\r\n\r\n4. What did you expect to see?\r\n\r\n    I expected the program's mutator goroutines continue to be scheduled as usual during the concurrent mark phase. I'd expect most goroutines to be scheduled for close to the 50–2000µs they need to do their normal work. I expected the \"Goroutines\" row to indicate that no goroutines were getting stuck in the run queue for long periods of time.\r\n\r\n    I'd expect no more than 25% of threads to be executing runtime.gcBgMarkWorker goroutines.\r\n\r\n    I expected the \"Network\" row to show incoming network events continue to occur during GC, and for the goroutines that are unblocked by network events to be scheduled immediately afterwards.\r\n\r\n5. What did you see instead?\r\n\r\n    Goroutines that are scheduled during garbage collection run for around 10ms. When runtime.gcBgMarkWorker goroutines run, they go for longer than 10ms. The \"Goroutines\" row grows and then stays steady, only draining when the GC is nearly complete.\r\n\r\n    Occasionally, as shown in the execution trace screenshot below, nearly all threads are running a runtime.gcBgMarkWorker goroutine. This one has 35 out of 36 threads running their copy of that goroutine—20 of the 21 pictured there, plus the 15 others offscreen.\r\n\r\n    The \"Network\" row shows less inbound network traffic during the GC, which seems to indicate that the netpoller is also getting starved—I don't know a lot yet about the netpoller's operation, but I don't think that there'd be any less inbound network traffic to this process during garbage collections. Additionally, when a network even is detected it takes a long time to run the goroutine that was unblocked by that network event. The red arrow in the execution tracer screenshot shows a delay of around 10ms between data coming in over the network and the correct goroutine running.\r\n\r\n---\r\n\r\nI've included some output from running the program with `GODEBUG=gctrace=1,gcpacertrace=1`. I'm not yet very familiar with the pacer, but it looks like \"assist ratio=+1.306319e+001\" means that when a goroutine goes into allocation debt that it will end up scanning 13,000,000 scan-units before it's allowed to run again per [gcAssistAlloc](https://github.com/golang/go/blob/go1.7rc2/src/runtime/mgcmark.go#L395-L399) and [gcOverAssistBytes](https://github.com/golang/go/blob/go1.7rc2/src/runtime/mgc.go#L744-L748).\r\n\r\n![screen shot 2016-07-19 at 1 42 23 pm redacted](https://cloud.githubusercontent.com/assets/230685/16968603/c5baa04e-4dc4-11e6-8113-a5d3cda94c30.png)\r\n\r\n\r\n```\r\npacer: sweep done at heap size 270MB; allocated 28MB of spans; swept 61108 pages at +2.548577e-004 pages/byte\r\npacer: assist ratio=+1.306319e+001 (scan 156 MB in 471-\u003e483 MB) workers=9+0\r\npacer: H_m_prev=253496440 h_t=+9.500000e-001 H_T=494318058 h_a=+9.582826e-001 H_a=496417664 h_g=+1.000000e+000 H_g=506992880 u_a=+6.990956e-001 u_g=+2.500000e-001 W_a=85708136 goalΔ=+5.000000e-002 actualΔ=+8.282586e-003 u_a/u_g=+2.796383e+000\r\ngc 2106 @1021.618s 4%: 0.27+30+1.4 ms clock, 8.5+494/256/0+46 ms cpu, 471-\u003e473-\u003e237 MB, 483 MB goal, 36 P\r\npacer: sweep done at heap size 268MB; allocated 31MB of spans; swept 61104 pages at +2.595642e-004 pages/byte\r\npacer: assist ratio=+1.269421e+001 (scan 150 MB in 462-\u003e474 MB) workers=9+0\r\npacer: H_m_prev=248903752 h_t=+9.500000e-001 H_T=485362316 h_a=+9.559496e-001 H_a=486843184 h_g=+1.000000e+000 H_g=497807504 u_a=+5.780640e-001 u_g=+2.500000e-001 W_a=85396448 goalΔ=+5.000000e-002 actualΔ=+5.949559e-003 u_a/u_g=+2.312256e+000\r\ngc 2107 @1022.157s 4%: 0.084+30+1.3 ms clock, 2.7+368/272/246+43 ms cpu, 462-\u003e464-\u003e238 MB, 474 MB goal, 36 P\r\npacer: sweep done at heap size 263MB; allocated 24MB of spans; swept 60513 pages at +2.558500e-004 pages/byte\r\npacer: assist ratio=+1.294395e+001 (scan 153 MB in 465-\u003e476 MB) workers=9+0\r\npacer: H_m_prev=250069528 h_t=+9.500000e-001 H_T=487635579 h_a=+9.529223e-001 H_a=488366352 h_g=+1.000000e+000 H_g=500139056 u_a=+4.340543e-001 u_g=+2.500000e-001 W_a=83280464 goalΔ=+5.000000e-002 actualΔ=+2.922277e-003 u_a/u_g=+1.736217e+000\r\ngc 2108 @1022.667s 4%: 0.17+27+1.2 ms clock, 5.4+182/239/482+37 ms cpu, 465-\u003e465-\u003e235 MB, 476 MB goal, 36 P\r\npacer: sweep done at heap size 257MB; allocated 21MB of spans; swept 60631 pages at +2.596270e-004 pages/byte\r\npacer: assist ratio=+1.270776e+001 (scan 149 MB in 459-\u003e470 MB) workers=9+0\r\npacer: H_m_prev=246926040 h_t=+9.500000e-001 H_T=481505778 h_a=+9.583699e-001 H_a=483572528 h_g=+1.000000e+000 H_g=493852080 u_a=+3.990433e-001 u_g=+2.500000e-001 W_a=83278560 goalΔ=+5.000000e-002 actualΔ=+8.369915e-003 u_a/u_g=+1.596173e+000\r\ngc 2109 @1023.185s 4%: 0.18+29+1.4 ms clock, 5.9+159/249/500+44 ms cpu, 459-\u003e461-\u003e229 MB, 470 MB goal, 36 P\r\npacer: sweep done at heap size 258MB; allocated 28MB of spans; swept 60538 pages at +2.662955e-004 pages/byte\r\npacer: assist ratio=+1.290206e+001 (scan 147 MB in 447-\u003e458 MB) workers=9+0\r\npacer: H_m_prev=240402624 h_t=+9.500000e-001 H_T=468785116 h_a=+9.540130e-001 H_a=469749856 h_g=+1.000000e+000 H_g=480805248 u_a=+4.026614e-001 u_g=+2.500000e-001 W_a=82234064 goalΔ=+5.000000e-002 actualΔ=+4.013014e-003 u_a/u_g=+1.610646e+000\r\ngc 2110 @1023.709s 4%: 0.18+26+1.1 ms clock, 5.9+146/228/474+35 ms cpu, 447-\u003e447-\u003e234 MB, 458 MB goal, 36 P\r\n```\r\n\r\n---\r\n\r\nThe combination of high assist ratio and all goroutines on all Ps running slowly makes this look similar to #14812, with the added twist that sometimes nearly all Ps are running their gcBgMarkWorker goroutine.\r\n\r\n/cc @aclements \r\n",
	"user": {
		"login": "rhysh",
		"id": 230685,
		"type": "User",
		"site_admin": false
	},
	"comments": 26,
	"closed_at": "2016-07-27T18:56:41Z",
	"created_at": "2016-07-19T22:59:26Z",
	"updated_at": "2016-07-30T20:34:11Z",
	"milestone": {
		"id": 1414133,
		"number": 31,
		"title": "Go1.7"
	}
}
