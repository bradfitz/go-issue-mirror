{
	"id": 167155491,
	"number": 16476,
	"state": "open",
	"title": "runtime: a proposal to reduce scheduling overhead on large systems",
	"body": "Please answer these questions before submitting your issue. Thanks!\r\n\r\n1. What version of Go are you using (`go version`)?\r\nGo 1.7beta2\r\n\r\n2. What operating system and processor architecture are you using (`go env`)?\r\nX86 (Ivy Bridge) - Ubuntu 16.04\r\nPOWER8 - Ubuntu 15.10\r\n\r\nThis issue/proposal is a bit long, and likely only of interest to those interested in goroutine scheduling.\r\n\r\nI work on the Hyperledger fabric (https://github.com/hyperledger/fabric), a large Go application implementing a permissioned blockchain. As part of this work I have observed what I would call \"excessive\" amounts of cumulative time spent in `runtime.findrunnable` when running on large systems with GOMAXPROCS defaulting to the number of available CPUs. In the following I assume the reader is familiar with the `findrunnable` routine in `proc.go`.\r\n\r\nDrilling down into a `findrunnable` profile, the obvious culprit is seen to be the work-stealing loop. This loop is inefficient on large systems for several reasons:\r\n\r\n1) \"Spinners\" poll the system 4 times while holding a P, and all threads poll once again after releasing their P.\r\n\r\n2) The stealing loop checks for stealable work from all Ps, including Ps that have no possibility of having any work to steal. The atomic operations used to load the queue pointers in runqgrab require synchronization primitives on some architectures, and a subroutine call overhead on all architectures. This global polling is disruptive in an SMP-coherence sense, since the poller must pull cache lines from around the system in order to examine only a few fields of each line. The randomized polling order also defeats the hardware's prefetching heuristics.\r\n\r\nRegarding 1): I understand why it is good to poll at least twice - First for ez-pickin's from the local run queues, and a second pass for the longer-latency `runnext` stealing. It occurred to me that perhaps 4 loops were made in Go 1.6 because the randomization used there was not guaranteed to visit every P, so polling 4X increased the odds of looking at every local queue. Now that this has been fixed in Go 1.7, polling more than twice is arguably not necessary. The polling with `runnext` grabs included is so thourough that once this loop is finished there is no _a priori_ reason to expect that another pass will bear fruit.\r\n\r\nRegarding 2): Note that the answer to the question: \"Could this P possibly have any work to steal?\" can be efficiently centralized since the answer is relatively rarely modified but relatively often observed. I've created a modified scheduler that includes a global array called `mayhavework` that is indexed by the id of a P. Currently, `mayhavework[i]` is `false` whenever a P is queued in the list of idle Ps, and `true` otherwise. More aggressive update protocols are also possible, but this simple protocol is sufficient to illustrate the benefit.\r\n\r\nSetting/clearing `mayhavework[i]` adds a small overhead to queue management of idle Ps, as well as a test during polling loops. Note that the polling loop in the \"delicate dance\" already includes what appears to be a redundant guard of `allp[i] != nil` which is not made by the work-stealing loop.\r\n\r\nHere are some results for an example Hyperledger fabric benchmark running on a 4-socket X86 Ivy Bridge server with 120 hardware threads. These examples are for illustration only and are not claimed to be exhaustive; The arguments for the proposal should be valid based on first principles. Performance (throuhgput) of the server is measured in transactions per second (TPS). Cumulative profile percentages were reported by the Golang `net/http/pprof` profiling service running in the application. Results for GOMAXPROCS eqaul to 12 and 120 (the default) are presented.\r\n\r\n```\r\nGOMAXPROCS = 12\r\n-------------------------------------------------------------------------\r\n                        Baseline   2 Stealing Loops Only   Full Proposal\r\n-------------------------------------------------------------------------\r\nThroughput               996 TPS          987 TPS              997 TPS\r\nruntime.findrunnable      14.0%            13.5%                14.1%\r\n-------------------------------------------------------------------------\r\n\r\nGOMAXPROCS = 120\r\n-------------------------------------------------------------------------\r\n                        Baseline   2 Stealing Loops Only   Full Proposal\r\n-------------------------------------------------------------------------\r\nThroughput               991 TPS          963 TPS              997 TPS\r\nruntime.findrunnable      28.2%            21.9%                16.5%\r\n-------------------------------------------------------------------------\r\n```\r\n\r\nThis full proposal has no effect on `findrunnable` overhead or performance on this system with GOMAXPROCS=12. However I have also run the experiment on a POWER8 server and observed a reduction from 14.5% to 9.4% of `findrunnable` overhead on that system with GOMAXPROCS=12. This may be due to the fact that `atomic.Load` includes a synchronization instruction on POWER.\r\n\r\nFor the full system there is a significant reduction in scheduling overhead. It is not clear whether the slight performance drop with 2 stealing loops only is real, or due to experimental variation. In a number of experiments (on POWER8) I have seen what I believe are small, real performance increases _and_ decreases from these modified heuristics, which vary based on the particular benchmark.\r\n\r\nTo summarize the proposal:\r\n\r\n1) Only poll twice in the work stealing loop;\r\n\r\n2) Implement an efficient centralized data structure that records which Ps might possibly have any work to steal.\r\n\r\nBishop Brock\r\n\r\n",
	"user": {
		"login": "bcbrock",
		"id": 11968347,
		"type": "User",
		"site_admin": false
	},
	"comments": 10,
	"created_at": "2016-07-23T00:16:18Z",
	"updated_at": "2016-08-10T22:43:10Z",
	"milestone": {
		"id": 1709363,
		"number": 38,
		"title": "Go1.8"
	}
}
