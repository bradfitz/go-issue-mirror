{
	"id": 113885473,
	"number": 13086,
	"state": "open",
	"title": "proposal: runtime: fall back to fair locks after repeated sleep-acquire failures",
	"body": "Go's locks make no guarantee of fairness. However, there exists at least one common pattern that exhibits severely unfair behavior, to the extent where it might be considered a serious bug. I propose we look into whether we can do anything in the sync.Mutex implementation that would ameliorate this severe unfairness without hurting the common case. I have one concrete suggestion at the bottom, but I hope to spur thought and discussion about this. I don't claim that this specific solution is the right one.\r\n\r\n### Problem\r\n\r\nI've seen two different reports now of severe lock unfairness under roughly the same conditions, in two independent programs. The general problem occurs when there are two goroutines contending for a lock and one of them holds it for a long time while only briefly releasing it, and the other releases it for a long time and only briefly acquires it.\r\n\r\nThe demo program available by `go get rsc.io/tmp/lockskew` boils down to this:\r\n\r\n\t// shared state\r\n\tdone := make(chan bool, 1)\r\n\tvar mu sync.Mutex\r\n\r\n\t// goroutine 1\r\n\tgo func() {\r\n\t\tfor {\r\n\t\t\tselect {\r\n\t\t\tcase \u003c-done:\r\n\t\t\t\treturn\r\n\t\t\tdefault:\r\n\t\t\t\tmu.Lock()\r\n\t\t\t\ttime.Sleep(100 * time.Microsecond)\r\n\t\t\t\tmu.Unlock()\r\n\t\t\t}\r\n\t\t}\r\n\t}()\r\n\r\n\t// goroutine 2\r\n\tfor i := 0; i \u003c n; i++ {\r\n\t\ttime.Sleep(100 * time.Microsecond)\r\n\t\tmu.Lock()\r\n\t\tmu.Unlock()\r\n\t}\r\n\tdone \u003c- 1\r\n\r\nHere, goroutine 1 basically wants to hold the lock all the time, releasing it briefly, while goroutine 2 only wants to hold the lock for brief amounts of time. Developers naively (but I don't think completely unreasonably) expect that although goroutine 2 may not get the lock quite as much as goroutine 1, it will get the lock once in a while, maybe after a second or two at worst. In fact, on my Linux workstation it's common for goroutine 2 to take _100+ seconds_ to acquire the lock just once.\r\n\r\nWhen goroutine 1 executes mu.Unlock(), it marks the lock unlocked and wakes up goroutine 2, intending that goroutine 2 will run and acquire the lock itself. But goroutine 2 does not run immediately. Goroutine 1 keeps running, goes around the loop again, calls mu.Lock(), and reacquires the lock. By the time goroutine 2 actually runs and tries to acquire the lock, goroutine 1 has taken it back. The basic pattern is:\r\n\r\n\tg1 holds lock, sleeping\r\n\t\tg2 calls mu.Lock\r\n\t\t- finds lock locked\r\n\t\t- adds itself to waiting queue\r\n\t\t- sleeps\r\n\tg1 wakes up from time.Sleep\r\n\tg1 calls mu.Unlock\r\n\t- marks lock unlocked\r\n\t- tells runtime g2 is allowed to run\r\n\t- returns\r\n\tg1 checks channel (never ready)\r\n\tg1 calls mu.Lock\r\n\t- finds lock unlocked\r\n\t- acquires lock\r\n\tg1 calls time.Sleep, sleeps\r\n\t\tg2 scheduled to run\r\n\t\t- finds lock locked\r\n\t\t- adds itself to waiting queue\r\n\t\t- sleeps\r\n\r\nGiven that g2 is woken by g1 only once per 100µs, the fact that g2 takes 100+ seconds to acquire the lock means that this cycle repeats millions of times before g1 is interrupted enough to let g2 run.\r\n\r\nIf you run `lockskew` it will acquire the lock as much as it can and print about acquisitions, at most once per second. The performance is sensitive to the specific length of the sleep, but they're all bad. On my system, here is lockskew with 100µs sleeps, as in the program above:\r\n\r\n\t$ ./lockskew\r\n\t2015/10/28 12:30:24 lock#0 took 180.773982s; average 180.773982s\r\n\t2015/10/28 12:38:28 lock#1 took 484.603155s; average 332.688568s\r\n\t2015/10/28 12:40:16 lock#2 took 107.852508s; average 257.743215s\r\n\t2015/10/28 12:44:00 lock#3 took 223.592777s; average 249.205605s\r\n\t2015/10/28 12:54:00 lock#4 took 600.067715s; average 319.378027s\r\n\t2015/10/28 13:03:40 lock#5 took 579.896472s; average 362.797768s\r\n\t2015/10/28 13:05:11 lock#6 took 91.223369s; average 324.001425s\r\n\t2015/10/28 13:06:10 lock#8 took 59.019365s; average 258.622936s\r\n\t2015/10/28 13:11:14 lock#9 took 303.023746s; average 263.063017s\r\n\t2015/10/28 13:12:22 lock#10 took 68.975316s; average 245.418681s\r\n\t2015/10/28 13:15:08 lock#11 took 165.555347s; average 238.763403s\r\n\t2015/10/28 13:15:32 lock#12 took 23.713157s; average 222.221076s\r\n\t2015/10/28 13:18:58 lock#13 took 205.748355s; average 221.044453s\r\n\t2015/10/28 13:20:39 lock#14 took 101.183532s; average 213.053725s\r\n\t2015/10/28 13:21:36 lock#15 took 56.859756s; average 203.291602s\r\n\t2015/10/28 13:22:59 lock#16 took 83.056960s; average 196.218976s\r\n\t2015/10/28 13:24:05 lock#17 took 66.815677s; average 189.029904s\r\n\r\nAnd here is lockskew with 100ms sleeps:\r\n\r\n\t$ ./lockskew -t 100ms\r\n\t2015/10/28 13:26:01 lock#0 took 22.828276s; average 22.828276s\r\n\t2015/10/28 13:28:04 lock#1 took 123.553891s; average 73.191083s\r\n\t2015/10/28 13:31:17 lock#2 took 192.438909s; average 112.940359s\r\n\t2015/10/28 13:31:24 lock#3 took 6.708422s; average 86.382375s\r\n\t2015/10/28 13:33:04 lock#4 took 100.719121s; average 89.249724s\r\n\t2015/10/28 13:34:06 lock#5 took 61.773499s; average 84.670353s\r\n\t2015/10/28 13:34:11 lock#6 took 5.004274s; average 73.289485s\r\n\t2015/10/28 13:34:14 lock#7 took 2.902557s; average 64.491119s\r\n\t2015/10/28 13:34:17 lock#8 took 2.702412s; average 57.625707s\r\n\t2015/10/28 13:34:34 lock#9 took 16.418097s; average 53.504946s\r\n\t2015/10/28 13:34:39 lock#10 took 4.904286s; average 49.086704s\r\n\t2015/10/28 13:35:31 lock#11 took 52.456904s; average 49.367554s\r\n\t2015/10/28 13:37:03 lock#12 took 91.809099s; average 52.632288s\r\n\t2015/10/28 13:37:17 lock#13 took 13.213271s; average 49.816644s\r\n\t2015/10/28 13:37:23 lock#15 took 5.204634s; average 43.958641s\r\n\r\n### Barging vs Handoff\r\n\r\nThe fundamental problem here is when g1 decides to wake up g2, it leaves the sync.Mutex unlocked and lets g2 deal with reacquiring it. In the time between those two events, any goroutine other than g2 (here g1, but in general any other goroutine) can arrive at mu.Lock and grab the mutex instead of g2. This \"drive-by acquisition\" is sometimes called _barging_.\r\n\r\nThe alternative to barging is for g1 to leave the lock locked when waking g2, explicitly handing off the lock to g2. This coordinated handoff leaves the lock in the \"locked\" state the entire time, eliminating the window in which barging can occur.\r\n\r\nIn his work on java.util.concurrent, Doug Lea found that barging helped throughput [1], and that fact has entered the received wisdom surrounding locking. Following standard Java at the time, the evaluation used operating system threads and an operating system scheduler (specifically, Linux 2.4 NPTL on Pentium3/4 and Solaris 9 on UltraSparc2/3). As I understand it, barging is a win because it makes some bad operating system scheduling decisions impossible. Specifically, if the operating system delays the scheduling of g2, making the handoff delay long, not holding the lock during the handoff lets another thread do useful work in the interim.\r\n\r\nAs the test program and the real programs that inspired it show, however, at least in the Go implementation, barging enables very bad states. Also, the delays that barging helps avoid may not apply to Go, which is primarily using goroutines and a goroutine scheduler.\r\n\r\n### Alternatives\r\n\r\nIt's worth noting that there is a known workaround to this problem. If you use a pair of locks, with mu2 protecting mu.Lock, then that makes barging impossible in the two-goroutine case and at least less of an issue in the case of more goroutines. Specifically, the above code changes to:\r\n\r\n\t// shared state\r\n\tdone := make(chan bool, 1)\r\n\tvar mu sync.Mutex\r\n\tvar mu2 sync.Mutex\r\n\r\n\t// goroutine 1\r\n\tgo func() {\r\n\t\tfor {\r\n\t\t\tselect {\r\n\t\t\tcase \u003c-done:\r\n\t\t\t\treturn\r\n\t\t\tdefault:\r\n\t\t\t\tmu2.Lock()\r\n\t\t\t\tmu.Lock()\r\n\t\t\t\tmu2.Unlock()\r\n\t\t\t\ttime.Sleep(100 * time.Microsecond)\r\n\t\t\t\tmu.Unlock()\r\n\t\t\t}\r\n\t\t}\r\n\t}()\r\n\r\n\t// goroutine 2\r\n\tfor i := 0; i \u003c n; i++ {\r\n\t\ttime.Sleep(100 * time.Microsecond)\r\n\t\tmu2.Lock()\r\n\t\tmu.Lock()\r\n\t\tmu2.Unlock()\r\n\t\tmu.Unlock()\r\n\t}\r\n\tdone \u003c- 1\r\n\r\nNow, because g1 sleeps without holding mu2, goroutine 2 can get mu2 and block in mu.Lock, and then when goroutine 1 goes back around its loop, it blocks on mu2.Lock and cannot barge in on goroutine 2's acquisition of mu.\r\n\r\nIf we take this approach in the lockskew test, it works much better:\r\n\r\n\t$ ./lockskew -2\r\n\t2015/10/28 12:27:17 lock#0 took 0.000163s; average 0.000163s\r\n\t2015/10/28 12:27:18 lock#2910 took 0.000164s; average 0.000164s\r\n\t2015/10/28 12:27:19 lock#5823 took 0.000163s; average 0.000164s\r\n\t2015/10/28 12:27:20 lock#8727 took 0.000174s; average 0.000164s\r\n\t2015/10/28 12:27:21 lock#11651 took 0.000166s; average 0.000164s\r\n\r\nNote that there is only one print per second but the program is averaging over 2500 loop iterations per second, about 500,000X faster than above.\r\n\r\nIdeally developers would not need to know about, much less use, this kind of hack. Another possibility is to wrap this idiom or some other implementation into a (say) sync.FairMutex with guaranteed fairness properties. But again ideally developers would not need to know or think about these awful details. I do not want to see blog posts explaining when to use sync.Mutex and when to use sync.FairMutex.\r\n\r\nIf Go's sync.Mutex can do something better by default without sacrificing performance, it seems it should. \r\n\r\n### Proposal\r\n\r\nThe proposal is that we detect severe unfairness and revert to handoffs in that case. How to do that is an open question, which is why this is not a design doc.\r\n\r\n### Performance\r\n\r\nAs an approximate lower bound on the cost of this in the general case, I added one line to the implementation of sync.Mutex's Unlock:\r\n\r\n\t\t// Grab the right to wake someone.\r\n\t\tnew = (old - 1\u003c\u003cmutexWaiterShift) | mutexWoken\r\n\t\tif atomic.CompareAndSwapInt32(\u0026m.state, old, new) {\r\n\t\t\truntime_Semrelease(\u0026m.sema)\r\n\t\t\truntime.Gosched()                  \u003c\u003c\u003c ADDED\r\n\t\t\treturn\r\n\t\t}\r\n\r\nToday at least, the implementation of Gosched essentially guarantees to run the goroutine just woken up by the Semrelease. This is not strictly speaking a handoff, since barging can still happen between the unlock and the acquisition in g2, but the handoff period is much smaller than it used to be, and in particular g1 is kept from reacquiring the lock. Anyway, the point is to evaluate how costly this might be, not to do a perfect job. A perfect job would require more lines changed in both sync.Mutex and the runtime (although perhaps not many).\r\n\r\nWith this line, lockskew with only a single lock runs at about the same speed as with two locks:\r\n\r\n\t$ ./lockskew.fast \r\n\t2015/10/28 13:49:02 lock#0 took 0.000155s; average 0.000155s\r\n\t2015/10/28 13:49:03 lock#2934 took 0.000165s; average 0.000162s\r\n\t2015/10/28 13:49:04 lock#5861 took 0.000163s; average 0.000162s\r\n\t2015/10/28 13:49:05 lock#8783 took 0.000166s; average 0.000162s\r\n\t2015/10/28 13:49:06 lock#11712 took 0.000162s; average 0.000162s\r\n\r\nThe pathological case is gone, but what about the common case? Barging is supposed to help throughput, so one expects that average throughput must have gone down. Doug Lea's paper uses as its benchmark multiple threads calling a random number generator protected by a lock. Each call acquires a lock, does a tiny amount of work, and releases the lock. The paper found that with 256 threads running on even single-processor systems, implementations allowing barging ran 10-100X faster than handoffs. Again this was with operating system threads, not goroutines, so the result may not carry over directly. A goroutine switch is much cheaper than a thread switch.\r\n\r\nIn fact, the result does not seem to carry over to goroutines, at least not in the random number case. Running `go test -bench . rsc.io/tmp/lockskew` will benchmark Go's rand.Int (with the same mutex around simple math) called from multiple goroutines simultaneously, from 1 to 256. As a worst case, I ran it with GOMAXPROCS=256 even though my system has only 12 cores. Here is the effect of adding the Gosched:\r\n\r\n\tname         old time/op  new time/op  delta\r\n\tRand1-256    33.2ns ± 2%  32.8ns ± 1%   -1.24%   (p=0.008 n=10+9)\r\n\tRand2-256    40.4ns ±60%  33.4ns ±16%     ~     (p=0.203 n=10+11)\r\n\tRand4-256     143ns ± 9%   140ns ± 3%     ~      (p=0.098 n=9+11)\r\n\tRand8-256     230ns ± 4%   208ns ± 3%   -9.46%   (p=0.000 n=9+12)\r\n\tRand12-256    267ns ± 2%   236ns ± 1%  -11.81%  (p=0.000 n=10+11)\r\n\tRand16-256    272ns ± 1%   250ns ± 2%   -8.12%   (p=0.000 n=9+10)\r\n\tRand32-256    283ns ± 1%   269ns ± 2%   -4.92%  (p=0.000 n=10+10)\r\n\tRand64-256    284ns ± 2%   279ns ± 2%   -1.90%  (p=0.000 n=10+11)\r\n\tRand128-256   279ns ± 3%   280ns ± 2%     ~     (p=0.591 n=10+11)\r\n\tRand256-256   276ns ± 1%   288ns ± 2%   +4.23%  (p=0.000 n=10+10)\r\n\r\nFor the most part, the Gosched makes things better, not worse.\r\n\r\nAlthough this microbenchmark doesn't show it, there may be a small degradation in real programs. I ran the `golang.org/x/benchmarks/bench program` with `-bench=http` with and without the change. The change seemed to cause a roughly 5% increase in the reported \"RunTime\" result.\r\n\r\nPerhaps with work to detect and fall back instead of having the \"handoff\" on always, we could keep real programs running as they do today but fix these pathological (but real) behaviors at the same time. For example, one idea is to have the blocked mu.Lock call do the detection. In the pathological behavior, it goes to sleep, is awoken, and then can't acquire the lock and goes back to sleep. If it is woken up to find the lock unavailable (say) three times in a row, it could set a \"now we're in handoff mode\" bit in the lock word. The handoff mode bit could be sticky for the life of the lock, or it could be cleared on a call to mu.Unlock that finds no goroutines waiting.\r\n\r\n[1] http://gee.cs.oswego.edu/dl/papers/aqs.pdf but ignore section 5.2. @RLH says the theoretical analysis there is known to be flawed.\r\n\r\n/cc @randall77 @dvyukov @aclements @RLH @dr2chase ",
	"user": {
		"login": "rsc",
		"id": 104030,
		"type": "User",
		"site_admin": false
	},
	"comments": 8,
	"created_at": "2015-10-28T18:05:31Z",
	"updated_at": "2015-12-02T22:26:32Z",
	"milestone": {
		"id": 1373555,
		"number": 30,
		"title": "Proposal"
	}
}
