{
	"id": 206121062,
	"body": "@aclements I have a different idea to think about how this leak has been created, but I need your opinion on how to debug this. There will be 2 parts which are both related to leak but from quite different perspectives.\r\n\r\nThe first part is about leak itself, i.e. about memory used by the go proxy. It is unrelated to go version (at least 1.5.1, 1.5.3 and 1.6 behave similar but with different volumes of leaked ram).\r\n\r\nContainers and test I crafted above lead to substantial (or plain huge) memory consumption because of quite ugly 10m allocation per request. This design choice is out of the scope of this post. I modified proxy to allocate 10m slice at every call to http handler and read data from small file from disk to that 10m area and then return it back to the http client via `http.ResponseWriter.Write(10m_slice[:size_of_the_small_file_has_been_read])` and I observe the same huge memory *leak* of 4.4Gb when running on fedora system even using go 1.5.1\r\n\r\nIt looks like go 1.6 introduced something new to the web server (or goroutines scheduling or anything else) which started to work more parallel, which ends up calling http handler more times in parallel and each parallel call allocates 10Mb buffer. I highlight that there is no c++ code involved in this new test, only plain go http handler which does the following (error checks omitted for space saving in the post)\r\n```\r\nx := make([]byte, 10 * 1024 * 1024)\r\nf, err := os.Open(key)\r\ndefer f.Close()\r\n\r\nn, err := f.Read(x)\r\n\r\nw.WriteHeader(http.StatusOK)\r\nw.Write(x[:n])\r\n```\r\n\r\nSince amount of memory used by this test is the same as with running my previous c++/elliptics reader test, I suspect that running this new test compiled with go 1.6 will end up eating more memory just like in the previous test.\r\n\r\nHeap dump, pprof top10 and gctrace show that go `returns` memory to the system, and there is no leak. Maybe this is true, but this behaviour is really unfriendly to the client. One virtually can not allocate somewhat big buffer in http handler since this memory will be `leaked` and by `leaked` I mean it will not be returned to the OS kernel, and eventually this proxy will be OOM-killed if number of parallel requests allocating large amount of ram is somewhat big (like thousands requests).\r\nProblem here is not in the memory allocated (1k rps times 10Mb ends up with 10Gb), which is ok, but the fact that it is never returned back to OS kernel.\r\n\r\nWhich brings us to the second problem.\r\nAfter test completes gctrace shows that memory consumptions goes to very small values like 0-1 megabytes, i.e. go thinks it returns memory back. But 1.6 returns memory much slower than 1.5, here are examples:\r\n\r\n1.5\r\n```\r\ngc 553 @65.881s 14%: 3.2+0+0+0+7.4 ms clock, 25+0+0+15/0.002/0+59 ms cpu, 883-\u003e883-\u003e862 MB, 883 MB goal, 8 P (forced)\r\ngc 554 @65.900s 14%: 8.5+72+17+18+9.7 ms clock, 68+72+0+35/0.41/0+78 ms cpu, 944-\u003e1825-\u003e1293 MB, 1294 MB goal, 8 P\r\ngc 555 @66.027s 14%: 1.3+37+5.0+4.6+6.1 ms clock, 11+37+0+31/0.26/0+49 ms cpu, 1213-\u003e1584-\u003e1563 MB, 1804 MB goal, 8 P\r\ngc 556 @66.088s 14%: 3.9+28+18+7.8+7.4 ms clock, 31+28+0+19/0.082/0+59 ms cpu, 1313-\u003e1747-\u003e1622 MB, 1879 MB goal, 8 P\r\ngc 557 @66.162s 14%: 3.4+32+7.0+5.6+10 ms clock, 27+32+0+24/0.12/0.78+81 ms cpu, 523-\u003e913-\u003e580 MB, 723 MB goal, 8 P\r\ngc 558 @66.222s 14%: 1.2+25+11+7.4+7.0 ms clock, 10+25+0+23/0.086/0.078+56 ms cpu, 390-\u003e752-\u003e750 MB, 571 MB goal, 8 P\r\ngc 559 @66.276s 14%: 1.3+31+16+8.6+5.6 ms clock, 11+31+0+37/0.27/0+45 ms cpu, 400-\u003e851-\u003e830 MB, 585 MB goal, 8 P\r\ngc 560 @66.342s 14%: 1.4+17+5.2+7.4+6.0 ms clock, 11+17+0+32/0.12/0+48 ms cpu, 439-\u003e681-\u003e669 MB, 644 MB goal, 8 P\r\ngc 561 @66.381s 14%: 2.5+21+7.0+7.1+6.8 ms clock, 20+21+0+21/0.065/0.099+55 ms cpu, 198-\u003e509-\u003e508 MB, 283 MB goal, 8 P\r\ngc 562 @66.427s 14%: 1.2+7.8+0.15+1.6+3.5 ms clock, 10+7.8+0+10/0.38/0.005+28 ms cpu, 478-\u003e538-\u003e538 MB, 702 MB goal, 8 P\r\ngc 563 @66.448s 14%: 1.3+7.3+1.3+8.1+7.3 ms clock, 11+7.3+0+8.7/0.006/0.044+58 ms cpu, 448-\u003e599-\u003e517 MB, 582 MB goal, 8 P\r\ngc 564 @66.475s 14%: 2.5+22+6.0+8.0+7.1 ms clock, 20+22+0+14/0.082/4.5+57 ms cpu, 257-\u003e558-\u003e557 MB, 356 MB goal, 8 P\r\ngc 565 @66.523s 14%: 1.2+11+6.4+7.3+7.9 ms clock, 9.7+11+0+11/0.080/0.81+63 ms cpu, 317-\u003e537-\u003e537 MB, 460 MB goal, 8 P\r\ngc 566 @66.557s 14%: 1.2+14+9.8+5.1+8.0 ms clock, 9.9+14+0+15/0.39/1.7+64 ms cpu, 417-\u003e667-\u003e666 MB, 610 MB goal, 8 P\r\ngc 567 @66.597s 14%: 1.2+38+10+8.7+7.5 ms clock, 10+38+0+9.9/0.14/4.3+60 ms cpu, 566-\u003e1037-\u003e1026 MB, 834 MB goal, 8 P\r\ngc 568 @66.665s 14%: 1.2+25+13+11+6.4 ms clock, 10+25+0+11/0.035/1.6+51 ms cpu, 796-\u003e1207-\u003e1206 MB, 1179 MB goal, 8 P\r\ngc 569 @66.724s 14%: 1.3+4.9+7.0+4.5+6.2 ms clock, 10+4.9+0+7.9/0.029/0.037+50 ms cpu, 895-\u003e1018-\u003e1005 MB, 1328 MB goal, 8 P\r\ngc 570 @66.754s 14%: 4.7+11+4.9+11+7.5 ms clock, 38+11+0+7.1/0.12/0.026+60 ms cpu, 245-\u003e465-\u003e364 MB, 308 MB goal, 8 P\r\ngc 571 @66.795s 14%: 1.0+20+8.3+13+6.0 ms clock, 8.7+20+0+11/0.028/0.020+48 ms cpu, 314-\u003e655-\u003e644 MB, 457 MB goal, 8 P\r\ngc 572 @66.844s 14%: 1.2+16+7.0+7.7+5.2 ms clock, 10+16+0+7.8/0.019/0.12+42 ms cpu, 584-\u003e805-\u003e794 MB, 861 MB goal, 8 P\r\ngc 573 @66.884s 14%: 2.3+6.5+1.7+4.9+7.1 ms clock, 19+6.5+0+4.7/1.1/0+57 ms cpu, 674-\u003e796-\u003e764 MB, 981 MB goal, 8 P\r\ngc 574 @66.910s 14%: 2.0+0+0+0+4.7 ms clock, 16+0+0+4.7/1.1/0+38 ms cpu, 264-\u003e264-\u003e133 MB, 264 MB goal, 8 P (forced)\r\ngc 575 @66.918s 14%: 2.6+15+5.2+7.4+7.3 ms clock, 21+15+0+7.8/0.37/2.7+59 ms cpu, 163-\u003e413-\u003e322 MB, 199 MB goal, 8 P\r\ngc 576 @66.957s 14%: 1.1+11+1.0+2.0+8.3 ms clock, 9.0+11+0+5.2/0.003/3.6+66 ms cpu, 292-\u003e443-\u003e442 MB, 424 MB goal, 8 P\r\ngc 577 @66.982s 14%: 1.2+6.6+5.3+10+4.0 ms clock, 9.7+6.6+0+13/0.15/0.23+32 ms cpu, 312-\u003e492-\u003e491 MB, 453 MB goal, 8 P\r\ngc 578 @67.010s 14%: 2.3+2.5+0.032+0.17+2.1 ms clock, 18+2.5+0+4.1/0.013/0.11+17 ms cpu, 421-\u003e432-\u003e431 MB, 602 MB goal, 8 P\r\n\r\ntest completed\r\n\r\ngc 579 @67.918s 14%: 0.26+0+0+0+6.8 ms clock, 2.0+0+0+4.1/0.013/0.11+55 ms cpu, 242-\u003e242-\u003e0 MB, 242 MB goal, 8 P (forced)\r\ngc 580 @68.925s 14%: 0.088+0+0+0+2.8 ms clock, 0.70+0+0+4.1/0.013/0.11+22 ms cpu, 0-\u003e0-\u003e0 MB, 0 MB goal, 8 P (forced)\r\ngc 581 @69.928s 14%: 0.071+0+0+0+2.8 ms clock, 0.57+0+0+4.1/0.013/0.11+23 ms cpu, 0-\u003e0-\u003e0 MB, 0 MB goal, 8 P (forced)\r\ngc 582 @70.932s 13%: 0.23+0+0+0+2.7 ms clock, 1.8+0+0+4.1/0.013/0.11+22 ms cpu, 0-\u003e0-\u003e0 MB, 0 MB goal, 8 P (forced)\r\ngc 583 @71.935s 13%: 0.17+0+0+0+2.9 ms clock, 1.3+0+0+4.1/0.013/0.11+23 ms cpu, 1-\u003e1-\u003e0 MB, 1 MB goal, 8 P (forced)\r\ngc 584 @72.938s 13%: 0.13+0+0+0+2.9 ms clock, 1.1+0+0+4.1/0.013/0.11+23 ms cpu, 0-\u003e0-\u003e0 MB, 0 MB goal, 8 P (forced)\r\n```\r\n\r\n\r\n1.6 is quite different - it allocates memory **much** faster and releases it **much** slower, I had to interrupt test since it ate 10+Gb of memory and computer started heavily lagging. I show below the whole test duration, i.e. 36 seconds compared to 72+ seconds above where test completed, while it is far from being completed below. Also, 1.6 timings are much worse, but it should be related to the memory consumption and since it became worse, it takes more time to allocate buffer, which in turn ends up longer handling http request.\r\n```\r\ngc 1 @1.110s 0%: 0.005+0+2.1 ms clock, 0.010+0/0/0+4.2 ms cpu, 0-\u003e0-\u003e0 MB, 0 MB goal, 8 P (forced)\r\ngc 2 @2.113s 0%: 0.11+0+1.7 ms clock, 0.22+0/0/0+3.4 ms cpu, 0-\u003e0-\u003e0 MB, 0 MB goal, 8 P (forced)\r\ngc 3 @3.115s 0%: 0.14+0+1.8 ms clock, 0.43+0/0/0+5.5 ms cpu, 0-\u003e0-\u003e0 MB, 0 MB goal, 8 P (forced)\r\ngc 4 @4.118s 0%: 0.043+0+1.7 ms clock, 0.21+0/0/0+8.7 ms cpu, 0-\u003e0-\u003e0 MB, 0 MB goal, 8 P (forced)\r\ngc 5 @5.120s 0%: 0.017+0+1.5 ms clock, 0.10+0/0/0+9.2 ms cpu, 0-\u003e0-\u003e0 MB, 0 MB goal, 8 P (forced)\r\ngc 6 @6.122s 0%: 0.024+0+1.6 ms clock, 0.16+0/0/0+11 ms cpu, 0-\u003e0-\u003e0 MB, 0 MB goal, 8 P (forced)\r\ngc 7 @7.124s 0%: 0.024+0+1.5 ms clock, 0.17+0/0/0+10 ms cpu, 0-\u003e0-\u003e0 MB, 0 MB goal, 8 P (forced)\r\ngc 8 @8.125s 0%: 0.16+0+1.5 ms clock, 1.1+0/0/0+10 ms cpu, 0-\u003e0-\u003e0 MB, 0 MB goal, 8 P (forced)\r\ngc 9 @9.127s 0%: 0.006+0+1.8 ms clock, 0.055+0/0/0+14 ms cpu, 0-\u003e0-\u003e0 MB, 0 MB goal, 8 P (forced)\r\ngc 10 @10.129s 0%: 0.007+0+1.6 ms clock, 0.057+0/0/0+13 ms cpu, 0-\u003e0-\u003e0 MB, 0 MB goal, 8 P (forced)\r\ngc 11 @10.151s 0%: 0.11+0.91+1.2 ms clock, 0.58+0.60/1.4/0+6.0 ms cpu, 21-\u003e21-\u003e20 MB, 22 MB goal, 8 P\r\ngc 12 @10.154s 0%: 0.37+0.95+1.2 ms clock, 0.37+1.0/1.4/0.080+1.2 ms cpu, 31-\u003e51-\u003e50 MB, 52 MB goal, 8 P\r\ngc 13 @10.157s 0%: 3.1+2.5+3.1 ms clock, 22+0.081/4.6/0.080+22 ms cpu, 60-\u003e141-\u003e140 MB, 76 MB goal, 8 P\r\ngc 14 @10.166s 0%: 0.14+2.6+1.3 ms clock, 0.89+0.51/1.9/0+8.0 ms cpu, 151-\u003e161-\u003e160 MB, 211 MB goal, 8 P\r\ngc 15 @10.171s 0%: 0.21+1.1+1.7 ms clock, 1.2+0.86/1.5/0+10 ms cpu, 191-\u003e281-\u003e281 MB, 241 MB goal, 8 P\r\ngc 16 @10.174s 0%: 0.24+1.8+1.2 ms clock, 1.2+0.95/2.4/0+6.1 ms cpu, 301-\u003e381-\u003e381 MB, 421 MB goal, 8 P\r\ngc 17 @10.188s 0%: 0.077+2.2+3.2 ms clock, 0.62+0.27/3.6/0+25 ms cpu, 401-\u003e422-\u003e421 MB, 571 MB goal, 8 P\r\ngc 18 @10.196s 0%: 0.12+15+1.6 ms clock, 0.97+0.036/4.2/0+12 ms cpu, 521-\u003e582-\u003e581 MB, 632 MB goal, 8 P\r\ngc 19 @10.216s 0%: 0.28+2.1+1.7 ms clock, 2.2+1.6/3.9/0+13 ms cpu, 752-\u003e802-\u003e802 MB, 872 MB goal, 8 P\r\ngc 20 @10.226s 0%: 0.92+3.5+1.7 ms clock, 7.4+2.0/6.3/0+13 ms cpu, 1082-\u003e1162-\u003e1162 MB, 1203 MB goal, 8 P\r\ngc 21 @10.243s 0%: 0.20+3.9+1.7 ms clock, 1.6+0.38/6.1/0+14 ms cpu, 1563-\u003e1643-\u003e1643 MB, 1743 MB goal, 8 P\r\ngc 22 @10.488s 0%: 0.17+3.9+2.6 ms clock, 1.4+0/6.5/0+21 ms cpu, 2284-\u003e2304-\u003e2093 MB, 2464 MB goal, 8 P\r\ngc 23 @11.132s 0%: 0.022+0+5.6 ms clock, 0.18+0/6.5/0+45 ms cpu, 2684-\u003e2684-\u003e2093 MB, 2684 MB goal, 8 P (forced)\r\ngc 24 @11.303s 0%: 0.18+3.3+2.7 ms clock, 1.4+2.0/5.6/0+22 ms cpu, 3005-\u003e3035-\u003e2924 MB, 3140 MB goal, 8 P\r\ngc 25 @11.340s 0%: 0.048+17+3.3 ms clock, 0.38+4.2/7.3/0+26 ms cpu, 4267-\u003e4307-\u003e4296 MB, 4387 MB goal, 8 P\r\ngc 26 @12.147s 0%: 0.031+0+9.0 ms clock, 0.25+4.2/7.3/0+72 ms cpu, 6039-\u003e6039-\u003e5277 MB, 6039 MB goal, 8 P (forced)\r\ngc 27 @13.162s 0%: 0.82+0+11 ms clock, 6.5+4.2/7.3/0+94 ms cpu, 7641-\u003e7641-\u003e6729 MB, 7641 MB goal, 8 P (forced)\r\ngc 28 @14.145s 0%: 0.19+13+8.2 ms clock, 1.5+0/25/0+66 ms cpu, 9894-\u003e9904-\u003e9002 MB, 10094 MB goal, 8 P\r\ngc 29 @14.196s 0%: 0.18+0+8.5 ms clock, 1.4+0/25/0+68 ms cpu, 9032-\u003e9032-\u003e9032 MB, 9032 MB goal, 8 P (forced)\r\ngc 30 @15.222s 0%: 0.19+0+12 ms clock, 1.5+0/25/0+99 ms cpu, 9964-\u003e9964-\u003e8982 MB, 9964 MB goal, 8 P (forced)\r\ngc 31 @16.237s 0%: 0.071+0+12 ms clock, 0.56+0/25/0+101 ms cpu, 9954-\u003e9954-\u003e9022 MB, 9954 MB goal, 8 P (forced)\r\ngc 32 @17.259s 0%: 0.050+0+15 ms clock, 0.40+0/25/0+120 ms cpu, 10925-\u003e10925-\u003e9994 MB, 10925 MB goal, 8 P (forced)\r\ngc 33 @18.295s 0%: 3.6+0+14 ms clock, 29+0/25/0+115 ms cpu, 10925-\u003e10925-\u003e10014 MB, 10925 MB goal, 8 P (forced)\r\ngc 34 @19.315s 1%: 0.041+0+13 ms clock, 0.33+0/25/0+106 ms cpu, 10655-\u003e10655-\u003e9733 MB, 10655 MB goal, 8 P (forced)\r\ngc 35 @20.337s 1%: 0.062+0+12 ms clock, 0.49+0/25/0+102 ms cpu, 9734-\u003e9734-\u003e8762 MB, 9734 MB goal, 8 P (forced)\r\ngc 36 @21.359s 1%: 0.070+0+13 ms clock, 0.56+0/25/0+105 ms cpu, 8763-\u003e8763-\u003e7791 MB, 8763 MB goal, 8 P (forced)\r\ngc 37 @22.373s 1%: 0.16+0+11 ms clock, 1.3+0/25/0+93 ms cpu, 7792-\u003e7792-\u003e6820 MB, 7792 MB goal, 8 P (forced)\r\ngc 38 @23.385s 1%: 0.069+0+10 ms clock, 0.55+0/25/0+86 ms cpu, 6820-\u003e6820-\u003e5848 MB, 6820 MB goal, 8 P (forced)\r\ngc 39 @24.398s 1%: 0.23+0+10 ms clock, 1.8+0/25/0+84 ms cpu, 5849-\u003e5849-\u003e4887 MB, 5849 MB goal, 8 P (forced)\r\ngc 40 @25.417s 1%: 0.060+0+9.7 ms clock, 0.48+0/25/0+77 ms cpu, 4888-\u003e4888-\u003e3896 MB, 4888 MB goal, 8 P (forced)\r\ngc 41 @26.433s 1%: 0.20+0+8.7 ms clock, 1.6+0/25/0+70 ms cpu, 3897-\u003e3897-\u003e2934 MB, 3897 MB goal, 8 P (forced)\r\ngc 42 @27.448s 1%: 0.075+0+8.1 ms clock, 0.60+0/25/0+65 ms cpu, 2935-\u003e2935-\u003e1973 MB, 2935 MB goal, 8 P (forced)\r\ngc 43 @28.457s 1%: 0.21+0+7.6 ms clock, 1.7+0/25/0+60 ms cpu, 1974-\u003e1974-\u003e982 MB, 1974 MB goal, 8 P (forced)\r\ngc 44 @29.465s 1%: 0.12+0+6.4 ms clock, 0.99+0/25/0+51 ms cpu, 983-\u003e983-\u003e1 MB, 983 MB goal, 8 P (forced)\r\ngc 45 @30.471s 1%: 0.039+0+3.9 ms clock, 0.31+0/25/0+31 ms cpu, 1-\u003e1-\u003e1 MB, 1 MB goal, 8 P (forced)\r\ngc 46 @31.476s 1%: 0.043+0+3.6 ms clock, 0.34+0/25/0+29 ms cpu, 1-\u003e1-\u003e1 MB, 1 MB goal, 8 P (forced)\r\ngc 47 @32.480s 1%: 0.030+0+2.9 ms clock, 0.24+0/25/0+23 ms cpu, 1-\u003e1-\u003e1 MB, 1 MB goal, 8 P (forced)\r\n```\r\n\r\nBasically, the second problem is the fact that after some changes in 1.6 web server started to call http handlers much more frequently in parallel **and** memory is released by go **much** slower than 1.5.\r\n\r\nAnd to refresh, the first problem is that after go releases memory, i.e. gc magic tells us in gctrace that memory is no longer used, it is not returned to kernel and go proxy continues to occupy all that memory somewhere. This can be glibc allocator cache or it can be some other cache if go uses custom allocator.\r\n\r\nAnyway, this all ends up with the case when allocating big slice in http handler will lead to OOM kill after a while if number of requests handled in parallel is rather big. 1.6 makes this problem much sharper.\r\n\r\nI will cook up a much smaller test with this allocation strategy.",
	"user": {
		"login": "bioothod",
		"id": 1847575,
		"type": "User",
		"site_admin": false
	},
	"created_at": "2016-04-06T05:13:18Z",
	"updated_at": "2016-04-06T05:28:36Z"
}
