{
	"id": 201038778,
	"body": "Some approaches we've taken or thought about:\r\n\r\n1. Multiply the assist ratio by an \"aggressiveness\" that starts out 0 at the trigger and grows linearly to 1 at the goal. The idea is that this would damp the assists early in the cycle, let the background mark workers get ahead, and hopefully finish closer to the goal. This didn't work well because allocation proceeded so quickly early in the GC cycle without the assists to slow it down that the assist ratio quickly ramped up anyway.\r\n\r\n2. A variation of this was to set the assist ratio to 0 until live heap reached half-way between the trigger and the goal. This had the same problem because allocation was completely unimpeded.\r\n\r\n3. Let GC go in to \"overtime\" if the heap is growing. When live heap is between the trigger and the goal, compute the assist ratio assuming we only have to scan the same amount of heap as the last GC cycle marked. If we pass the goal, set an \"overtime\" goal to something like 2*goal and set the assist ratio assuming we have to scan the whole heap by the time we reach the overtime goal.\r\n\r\n4. Set (or cap) the assist ratio to 2 (scan 2 bytes for every 1 byte allocated). The idea is to eliminate the within-cycle feedback loop, allow GC to pass the goal, and use this feedback to adjust the trigger between cycles. In principle, the cross-cycle feedback loop should adjust in steady state so that the GC stops passing the goal after a few cycles. By setting the assist ratio to \u003e 1, we still bound the total size the heap can grow to during a cycle. Setting it to 2 specifically bounds it to 2*trigger.",
	"user": {
		"login": "aclements",
		"id": 2688315,
		"type": "User",
		"site_admin": false
	},
	"created_at": "2016-03-24T21:44:42Z",
	"updated_at": "2016-03-24T21:44:42Z"
}
