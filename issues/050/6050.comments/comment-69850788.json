{
	"id": 69850788,
	"body": "I'm parsing wikidata dump (http://dumps.wikimedia.org/other/wikidata/) which is in JSON format (~36GB) and is a single giant array containing objects:\r\n```\r\n[\r\n    { \"id\":\"Q1\", \"type\":\"item\", ...},\r\n    { \"id\":\"Q8\", \"type\":\"item\", ...},\r\n\r\n    (millions of such objects)\r\n]\r\n```\r\nWith the current json Decoder interface it seems impossible to parse this JSON without reading in whole file in memory.  So, exposing tokenizer would be really helpful.",
	"user": {
		"login": "nitingupta910",
		"id": 1069854,
		"type": "User",
		"site_admin": false
	},
	"created_at": "2015-01-14T00:47:23Z",
	"updated_at": "2015-01-14T00:47:23Z"
}
