{
	"id": 229518409,
	"body": "tl;dr, leave gzip as is for go1.7, we can address arm performance issues in go1.8\r\n\r\nAlright, I think I've figured out what's causing the slow-down for ARM. While all of the go1.7 optimizations as a whole have helped arm and arm64 (as seen in my post above), there are some cases where performance regresses (as is the case for test/bench/go1/gzip_test.go)\r\n\r\nOne of major changes made to compress/flate was that we started using 4-byte hashing instead of 3-byte hashing. The new hash function is stronger and leads to less collisions and thus fewer searches, which is good for performance. On amd64, we take advantage of the fact that most CPUs can perform [4-byte unaligned loads](https://github.com/golang/go/blob/7cd6cae6a63f09caa88bbcc394053b40cdeeccd1/src/compress/flate/deflate.go#L285) pretty efficiently (#14267). However, this optimization is not possible on arm, so the arm targets have the pay extra cost to utilize the stronger hash function.\r\n\r\nOn Go1.6, the simpler hash function compiled on arm32 to approximately 17 instrs:\r\n```\r\n102638: MOVW 0x44(R5), R1\r\n10263c: MOVW $2, R2\r\n102640: ADD R2, R1, R2\r\n102644: ADD $72, R5, R1\r\n102648: MOVW 0x4(R1), R3\r\n10264c: CMP R3, R2\r\n102650: B.CC 0x10265c\r\n102654: BL runtime.panicindex(SB)\r\n102658: UNDEF\r\n10265c: MOVW (R1), R1\r\n102660: MOVB (R1)(R2), R0\r\n102664: MOVW 0x74(R5), R1\r\n102668: LSL $6, R1, R1\r\n10266c: ADD R1, R0, R0\r\n102670: MOVW 0x46c(R15), R1\r\n102674: AND R1, R0, R0\r\n102678: MOVW R0, 0x74(R5)\r\n```\r\n\r\nOn Go1.7, the stronger hash function compiled on arm32 to approximately 77 instrs:\r\n```\r\ne7d74: MOVW 0x5e8(R15), R11\r\ne7d78: MOVW (R5)(R11), R2\r\ne7d7c: MOVW 0x5e0(R15), R11\r\ne7d80: MOVW (R5)(R11), R0\r\ne7d84: MOVW $4, R1\r\ne7d88: ADD R1, R0, R3\r\ne7d8c: MOVW R2, R1\r\ne7d90: MOVW R3, R2\r\ne7d94: MOVW 0x5d4(R15), R11\r\ne7d98: MOVW (R5)(R11), R3\r\ne7d9c: CMP R3, R2\r\ne7da0: B.HI 0xe8a90\r\ne7da4: CMP R2, R1\r\ne7da8: B.HI 0xe8a90\r\ne7dac: SUB R1, R2, R2\r\ne7db0: SUB R1, R3, R3\r\ne7db4: MOVW 0x5b8(R15), R11\r\ne7db8: MOVW (R5)(R11), R4\r\ne7dbc: CMP $0, R3\r\ne7dc0: B.EQ 0xe7dc8\r\ne7dc4: ADD R1, R4, R4\r\ne7dc8: MOVW R2, R8\r\ne7dcc: MOVW R3, R7\r\ne7dd0: MOVW R4, 0x50(R13)\r\ne7dd4: MOVW R2, 0x54(R13)\r\ne7dd8: MOVW R3, 0x58(R13)\r\ne7ddc: MOVW $0, R0\r\ne7de0: ADD $80, R13, R0\r\ne7de4: MOVW 0x4(R0), R1\r\ne7de8: MOVW $3, R2\r\ne7dec: CMP R2, R1\r\ne7df0: B.HI 0xe7dfc\r\ne7df4: BL runtime.panicindex(SB)\r\ne7df8: UNDEF\r\ne7dfc: MOVW (R0), R0\r\ne7e00: MOVB 0x3(R0), R0\r\ne7e04: ADD $80, R13, R1\r\ne7e08: MOVW 0x4(R1), R2\r\ne7e0c: MOVW $2, R3\r\ne7e10: CMP R3, R2\r\ne7e14: B.HI 0xe7e20\r\ne7e18: BL runtime.panicindex(SB)\r\ne7e1c: UNDEF\r\ne7e20: MOVW (R1), R1\r\ne7e24: MOVB 0x2(R1), R1\r\ne7e28: LSL $8, R1, R1\r\ne7e2c: ORR R1, R0, R0\r\ne7e30: ADD $80, R13, R1\r\ne7e34: MOVW 0x4(R1), R2\r\ne7e38: MOVW $1, R3\r\ne7e3c: CMP R3, R2\r\ne7e40: B.HI 0xe7e4c\r\ne7e44: BL runtime.panicindex(SB)\r\ne7e48: UNDEF\r\ne7e4c: MOVW (R1), R1\r\ne7e50: MOVB 0x1(R1), R1\r\ne7e54: LSL $16, R1, R1\r\ne7e58: ORR R1, R0, R0\r\ne7e5c: ADD $80, R13, R1\r\ne7e60: MOVW 0x4(R1), R2\r\ne7e64: CMP $0, R2\r\ne7e68: B.HI 0xe7e74\r\ne7e6c: BL runtime.panicindex(SB)\r\ne7e70: UNDEF\r\ne7e74: MOVW (R1), R1\r\ne7e78: MOVB (R1), R1\r\ne7e7c: LSL $24, R1, R1\r\ne7e80: ORR R1, R0, R0\r\ne7e84: MOVW 0x4ec(R15), R1\r\ne7e88: MUL R0, R1, R0\r\ne7e8c: LSR $15, R0, R2\r\ne7e90: CMP $0, R5\r\ne7e94: MOVW.EQ R5, (R5)\r\ne7e98: MOVW 0x4dc(R15), R11\r\ne7e9c: ?\r\ne8a90: BL runtime.panicslice(SB)\r\ne8a94: UNDEF\r\n```\r\n\r\nOn Go1.7, the stronger hash function compiles on amd64 to approximately 31 instrs:\r\n```\r\n4bb0d9: MOVQ 0xa0080(AX), CX\r\n4bb0e0: MOVQ 0xa0070(AX), BX\r\n4bb0e7: LEAQ 0x4(DX), SI\r\n4bb0eb: CMPQ SI, DX\r\n4bb0ee: JA 0x4bbc9f\r\n4bb0f4: CMPQ CX, SI\r\n4bb0f7: JA 0x4bbc9f\r\n4bb0fd: SUBQ DX, CX\r\n4bb100: TESTQ CX, CX\r\n4bb103: JE 0x4bbc98\r\n4bb109: MOVZX 0x3(BX)(DX*1), CX\r\n4bb10e: MOVZX 0x2(BX)(DX*1), SI\r\n4bb113: MOVZX 0x1(BX)(DX*1), DI\r\n4bb118: MOVZX 0(BX)(DX*1), DX\r\n4bb11c: SHLL $0x8, SI\r\n4bb11f: ORL SI, CX\r\n4bb121: SHLL $0x10, DI\r\n4bb124: ORL DI, CX\r\n4bb126: SHLL $0x18, DX\r\n4bb129: ORL DX, CX\r\n4bb12b: IMULL $0x1e35a7bd, CX, CX\r\n4bb131: SHRL $0xf, CX\r\n4bb134: MOVL CX, 0xa00c8(AX)\r\n4bb261: XORL SI, SI\r\n4bb5a8: XORL DX, DX\r\n4bb644: XORL BX, BX\r\n4bbab2: XORL CX, CX\r\n4bbc98: XORL DX, DX\r\n4bbc9a: JMP 0x4bb109\r\n4bbc9f: CALL runtime.panicslice(SB)\r\n4bbca4: UD2\r\n```\r\n\r\nAny changes to fix this would probably be too big to do this late in the release cycle. We can think about better approaches in Go 1.8. I don't know enough about ARM to know if it will ever have unaligned reads/writes. So it may be worth investigating using the previous hash function over the new one for non x86 architectures.",
	"user": {
		"login": "dsnet",
		"id": 6354026,
		"type": "User",
		"site_admin": false
	},
	"created_at": "2016-06-29T23:21:54Z",
	"updated_at": "2016-06-29T23:21:54Z"
}
