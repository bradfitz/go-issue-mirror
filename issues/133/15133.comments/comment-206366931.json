{
	"id": 206366931,
	"body": "\u003e What did you see instead? What is performance depending on number of cores?\r\n\r\n`iowait` and `system` CPU shares grow with increasing the number of CPU cores\r\n\r\n\u003e I don't understand relation between iowait and memory ping-pong? iowait if waiting for IO, like hard drive, memory accesses are not IO.\r\n\r\nI'm not an expert in `iowait`, but the fact is that `iowait` completely vanishes from 20% to 0% when deadlines on `net.TCPConn` connections are disabled. See the following image:\r\n![cpuusage](https://cloud.githubusercontent.com/assets/283442/14317687/2c558ed6-fc12-11e5-80b0-61249eeab20d.png).\r\n\r\nConnection deadlines are enabled during 15:43-15:56\r\nConnection deadlines are disabled starting from 15:56\r\n\r\n\u003e How exactly does profile look?\r\n\r\nHere is CPU profile for the process with deadlines enabled:\r\n```\r\n(pprof) top20\r\n9.85mins of 15.65mins total (62.97%)\r\nDropped 623 nodes (cum \u003c= 0.08mins)\r\nShowing top 20 nodes out of 187 (cum \u003e= 4.98mins)\r\n      flat  flat%   sum%        cum   cum%\r\n  3.27mins 20.92% 20.92%   3.27mins 20.92%  runtime.futex\r\n  1.98mins 12.66% 33.57%   2.20mins 14.04%  syscall.Syscall\r\n  0.66mins  4.25% 37.82%   0.66mins  4.25%  runtime.memmove\r\n  0.54mins  3.48% 41.30%   0.54mins  3.48%  runtime.procyield\r\n  0.52mins  3.31% 44.61%   3.18mins 20.29%  runtime.lock\r\n  0.41mins  2.61% 47.22%   0.41mins  2.61%  runtime/internal/atomic.Xchg\r\n  0.34mins  2.18% 49.40%   0.34mins  2.18%  runtime.epollwait\r\n  0.25mins  1.61% 51.02%   0.25mins  1.61%  runtime.epollctl\r\n  0.25mins  1.60% 52.61%   0.25mins  1.60%  runtime.siftdownTimer\r\n  0.24mins  1.55% 54.16%   0.24mins  1.55%  runtime.siftupTimer\r\n  0.24mins  1.54% 55.70%   0.24mins  1.54%  runtime.osyield\r\n  0.19mins  1.19% 56.89%   0.19mins  1.19%  runtime/internal/atomic.Cas\r\n  0.15mins  0.96% 57.85%   3.03mins 19.36%  runtime.deltimer\r\n  0.14mins  0.91% 58.76%  11.56mins 73.90%  github.com/valyala/fasthttp.(*Server).serveConn\r\n  0.12mins  0.76% 59.52%      2mins 12.78%  runtime.unlock\r\n  0.12mins  0.74% 60.26%   0.16mins     1%  runtime.deferreturn\r\n  0.11mins  0.71% 60.97%   0.12mins  0.77%  runtime.gopark\r\n  0.11mins  0.71% 61.68%   0.42mins  2.68%  runtime.gentraceback\r\n  0.10mins  0.66% 62.35%   0.23mins  1.47%  lib/metric.(*sample).Update\r\n  0.10mins  0.62% 62.97%   4.98mins 31.86%  net.runtime_pollSetDeadline\r\n\r\n(pprof) top20 -cum\r\n387.31s of 938.84s total (41.25%)\r\nDropped 623 nodes (cum \u003c= 4.69s)\r\nShowing top 20 nodes out of 187 (cum \u003e= 112.06s)\r\n      flat  flat%   sum%        cum   cum%\r\n         0     0%     0%    864.20s 92.05%  runtime.goexit\r\n     0.01s 0.0011% 0.0011%    733.84s 78.16%  github.com/valyala/fasthttp.(*workerPool).getCh.func1\r\n     0.39s 0.042% 0.043%    733.75s 78.15%  github.com/valyala/fasthttp.(*workerPool).workerFunc\r\n     0.10s 0.011% 0.053%    693.90s 73.91%  github.com/valyala/fasthttp.(*Server).(github.com/valyala/fasthttp.serveConn)-fm\r\n     8.53s  0.91%  0.96%    693.80s 73.90%  github.com/valyala/fasthttp.(*Server).serveConn\r\n     1.28s  0.14%  1.10%    306.32s 32.63%  net.setDeadlineImpl\r\n     5.86s  0.62%  1.72%    299.09s 31.86%  net.runtime_pollSetDeadline\r\n   196.36s 20.92% 22.64%    196.36s 20.92%  runtime.futex\r\n    31.04s  3.31% 25.94%    190.52s 20.29%  runtime.lock\r\n        9s  0.96% 26.90%    181.79s 19.36%  runtime.deltimer\r\n     0.80s 0.085% 26.99%    159.09s 16.95%  lib/httpserver.(*customConn).SetReadDeadline\r\n     0.72s 0.077% 27.06%    158.10s 16.84%  net.(*conn).SetReadDeadline\r\n     0.36s 0.038% 27.10%    157.38s 16.76%  net.(*netFD).setReadDeadline\r\n     2.63s  0.28% 27.38%    154.28s 16.43%  lib/httpserver.(*customConn).SetWriteDeadline\r\n     0.84s 0.089% 27.47%    150.37s 16.02%  net.(*conn).SetWriteDeadline\r\n     0.23s 0.024% 27.50%    149.53s 15.93%  net.(*netFD).setWriteDeadline\r\n     0.80s 0.085% 27.58%    139.24s 14.83%  runtime.addtimer\r\n   118.83s 12.66% 40.24%    131.79s 14.04%  syscall.Syscall\r\n     7.14s  0.76% 41.00%    120.01s 12.78%  runtime.unlock\r\n     2.39s  0.25% 41.25%    112.06s 11.94%  github.com/valyala/fasthttp.acquireByteReader\r\n\r\n(pprof) list addtimer$\r\nTotal: 15.65mins\r\nROUTINE ======================== runtime.addtimer in /home/aliaksandr/work/go1.5/src/runtime/time.go\r\n     800ms   2.32mins (flat, cum) 14.83% of Total\r\n         .          .     80:// Ready the goroutine arg.\r\n         .          .     81:func goroutineReady(arg interface{}, seq uintptr) {\r\n         .          .     82:\tgoready(arg.(*g), 0)\r\n         .          .     83:}\r\n         .          .     84:\r\n     360ms      360ms     85:func addtimer(t *timer) {\r\n      50ms   1.35mins     86:\tlock(\u0026timers.lock)\r\n     320ms      7.42s     87:\taddtimerLocked(t)\r\n      30ms     50.47s     88:\tunlock(\u0026timers.lock)\r\n      40ms       40ms     89:}\r\n         .          .     90:\r\n         .          .     91:// Add a timer to the heap and start or kick the timer proc.\r\n         .          .     92:// If the new timer is earlier than any of the others.\r\n         .          .     93:// Timers are locked.\r\n         .          .     94:func addtimerLocked(t *timer) {\r\n\r\n\r\n(pprof) list deltimer$\r\nTotal: 15.65mins\r\nROUTINE ======================== runtime.deltimer in /home/aliaksandr/work/go1.5/src/runtime/time.go\r\n        9s   3.03mins (flat, cum) 19.36% of Total\r\n         .          .    117:\t}\r\n         .          .    118:}\r\n         .          .    119:\r\n         .          .    120:// Delete timer t from the heap.\r\n         .          .    121:// Do not need to update the timerproc: if it wakes up early, no big deal.\r\n     200ms      200ms    122:func deltimer(t *timer) bool {\r\n         .          .    123:\t// Dereference t so that any panic happens before the lock is held.\r\n         .          .    124:\t// Discard result, because t might be moving in the heap.\r\n      50ms       50ms    125:\t_ = t.i\r\n         .          .    126:\r\n     180ms   1.61mins    127:\tlock(\u0026timers.lock)\r\n         .          .    128:\t// t may not be registered anymore and may have\r\n         .          .    129:\t// a bogus i (typically 0, if generated by Go).\r\n         .          .    130:\t// Verify it before proceeding.\r\n     570ms      570ms    131:\ti := t.i\r\n     1.64s      1.64s    132:\tlast := len(timers.t) - 1\r\n     2.64s      2.64s    133:\tif i \u003c 0 || i \u003e last || timers.t[i] != t {\r\n         .          .    134:\t\tunlock(\u0026timers.lock)\r\n         .          .    135:\t\treturn false\r\n         .          .    136:\t}\r\n         .          .    137:\tif i != last {\r\n     1.47s      1.47s    138:\t\ttimers.t[i] = timers.t[last]\r\n     330ms      330ms    139:\t\ttimers.t[i].i = i\r\n         .          .    140:\t}\r\n     1.71s      1.71s    141:\ttimers.t[last] = nil\r\n      30ms       30ms    142:\ttimers.t = timers.t[:last]\r\n         .          .    143:\tif i != last {\r\n      30ms     11.32s    144:\t\tsiftupTimer(i)\r\n      30ms      6.23s    145:\t\tsiftdownTimer(i)\r\n         .          .    146:\t}\r\n      30ms     58.89s    147:\tunlock(\u0026timers.lock)\r\n      90ms       90ms    148:\treturn true\r\n         .          .    149:}\r\n         .          .    150:\r\n         .          .    151:// Timerproc runs the time-driven events.\r\n         .          .    152:// It sleeps until the next event in the timers heap.\r\n         .          .    153:// If addtimer inserts a new earlier event, addtimer1 wakes timerproc early.\r\n```\r\n\r\nHere is CPU profile for the app with connection deadlines disabled:\r\n\r\n```\r\n(pprof) top20\r\n465.38s of 1002.60s total (46.42%)\r\nDropped 622 nodes (cum \u003c= 5.01s)\r\nShowing top 20 nodes out of 192 (cum \u003e= 15.87s)\r\n      flat  flat%   sum%        cum   cum%\r\n    54.74s  5.46%  5.46%     54.74s  5.46%  runtime.memmove\r\n    54.54s  5.44% 10.90%    150.32s 14.99%  runtime.lock\r\n    48.96s  4.88% 15.78%    330.74s 32.99%  runtime.findrunnable\r\n    34.12s  3.40% 19.19%     34.12s  3.40%  runtime.futex\r\n    32.93s  3.28% 22.47%     32.93s  3.28%  runtime/internal/atomic.Xchg\r\n    27.41s  2.73% 25.20%     27.41s  2.73%  runtime.procyield\r\n    26.82s  2.68% 27.88%     33.59s  3.35%  runtime.runqgrab\r\n    20.20s  2.01% 29.89%     35.12s  3.50%  runtime.selectgoImpl\r\n    17.45s  1.74% 31.63%     17.45s  1.74%  runtime/internal/atomic.Cas\r\n    16.69s  1.66% 33.30%     21.79s  2.17%  github.com/valyala/fasthttp.(*headerScanner).next\r\n    16.56s  1.65% 34.95%     16.56s  1.65%  math/big.addMulVVW\r\n    16.38s  1.63% 36.58%     25.14s  2.51%  github.com/valyala/fasthttp.(*RequestHeader).AppendBytes\r\n    14.24s  1.42% 38.01%    446.04s 44.49%  github.com/valyala/fasthttp.(*Server).serveConn\r\n    13.88s  1.38% 39.39%     22.81s  2.28%  runtime.netpollready\r\n    13.72s  1.37% 40.76%     13.72s  1.37%  runtime/internal/atomic.Load\r\n    12.19s  1.22% 41.97%     32.18s  3.21%  runtime.unlock\r\n    11.54s  1.15% 43.12%    351.66s 35.07%  runtime.schedule\r\n    11.42s  1.14% 44.26%     11.42s  1.14%  runtime.indexbytebody\r\n    11.02s  1.10% 45.36%     11.02s  1.10%  runtime.osyield\r\n    10.57s  1.05% 46.42%     15.87s  1.58%  github.com/valyala/fasthttp.appendArgBytes\r\n\r\n(pprof) top20 -cum\r\n155.46s of 1002.60s total (15.51%)\r\nDropped 622 nodes (cum \u003c= 5.01s)\r\nShowing top 20 nodes out of 192 (cum \u003e= 61.52s)\r\n      flat  flat%   sum%        cum   cum%\r\n         0     0%     0%    636.02s 63.44%  runtime.goexit\r\n         0     0%     0%    466.84s 46.56%  github.com/valyala/fasthttp.(*workerPool).getCh.func1\r\n     2.85s  0.28%  0.28%    466.84s 46.56%  github.com/valyala/fasthttp.(*workerPool).workerFunc\r\n     0.28s 0.028%  0.31%    446.32s 44.52%  github.com/valyala/fasthttp.(*Server).(github.com/valyala/fasthttp.serveConn)-fm\r\n    14.24s  1.42%  1.73%    446.04s 44.49%  github.com/valyala/fasthttp.(*Server).serveConn\r\n     3.38s  0.34%  2.07%    361.68s 36.07%  runtime.mcall\r\n     1.30s  0.13%  2.20%    354.98s 35.41%  runtime.park_m\r\n    11.54s  1.15%  3.35%    351.66s 35.07%  runtime.schedule\r\n    48.96s  4.88%  8.23%    330.74s 32.99%  runtime.findrunnable\r\n     1.74s  0.17%  8.41%    206.95s 20.64%  lib/httpserver.getHTTPHandler.func1\r\n     1.21s  0.12%  8.53%    199.45s 19.89%  lib/httpserver.httpHandler\r\n     0.43s 0.043%  8.57%    161.15s 16.07%  main.requestHandler\r\n     0.14s 0.014%  8.58%    158.64s 15.82%  main.proxyHandler\r\n    54.54s  5.44% 14.02%    150.32s 14.99%  runtime.lock\r\n     2.46s  0.25% 14.27%    149.65s 14.93%  main.proxyRequest\r\n         0     0% 14.27%    112.92s 11.26%  github.com/valyala/fasthttp.(*PipelineClient).worker.func1\r\n     4.15s  0.41% 14.68%    112.92s 11.26%  github.com/valyala/fasthttp.(*PipelineClient).writer\r\n     5.54s  0.55% 15.24%    106.39s 10.61%  github.com/valyala/fasthttp.acquireByteReader\r\n     0.73s 0.073% 15.31%     67.26s  6.71%  github.com/valyala/fasthttp.(*RequestHeader).SetBytesV\r\n     1.97s   0.2% 15.51%     61.52s  6.14%  github.com/valyala/fasthttp.(*RequestHeader).SetCanonical\r\n\r\n(pprof) list addtimer$\r\nTotal: 16.71mins\r\nROUTINE ======================== runtime.addtimer in /home/aliaksandr/work/go1.5/src/runtime/time.go\r\n     150ms      1.60s (flat, cum)  0.16% of Total\r\n         .          .     80:// Ready the goroutine arg.\r\n         .          .     81:func goroutineReady(arg interface{}, seq uintptr) {\r\n         .          .     82:\tgoready(arg.(*g), 0)\r\n         .          .     83:}\r\n         .          .     84:\r\n     100ms      100ms     85:func addtimer(t *timer) {\r\n      40ms      780ms     86:\tlock(\u0026timers.lock)\r\n      10ms      310ms     87:\taddtimerLocked(t)\r\n         .      410ms     88:\tunlock(\u0026timers.lock)\r\n         .          .     89:}\r\n         .          .     90:\r\n         .          .     91:// Add a timer to the heap and start or kick the timer proc.\r\n         .          .     92:// If the new timer is earlier than any of the others.\r\n         .          .     93:// Timers are locked.\r\n\r\n\r\n(pprof) list deltimer$\r\nTotal: 16.71mins\r\nROUTINE ======================== runtime.deltimer in /home/aliaksandr/work/go1.5/src/runtime/time.go\r\n     2.21s      4.53s (flat, cum)  0.45% of Total\r\n         .          .    117:\t}\r\n         .          .    118:}\r\n         .          .    119:\r\n         .          .    120:// Delete timer t from the heap.\r\n         .          .    121:// Do not need to update the timerproc: if it wakes up early, no big deal.\r\n     370ms      370ms    122:func deltimer(t *timer) bool {\r\n         .          .    123:\t// Dereference t so that any panic happens before the lock is held.\r\n         .          .    124:\t// Discard result, because t might be moving in the heap.\r\n      10ms       10ms    125:\t_ = t.i\r\n         .          .    126:\r\n      60ms      1.03s    127:\tlock(\u0026timers.lock)\r\n         .          .    128:\t// t may not be registered anymore and may have\r\n         .          .    129:\t// a bogus i (typically 0, if generated by Go).\r\n         .          .    130:\t// Verify it before proceeding.\r\n      40ms       40ms    131:\ti := t.i\r\n      30ms       30ms    132:\tlast := len(timers.t) - 1\r\n     1.06s      1.06s    133:\tif i \u003c 0 || i \u003e last || timers.t[i] != t {\r\n         .          .    134:\t\tunlock(\u0026timers.lock)\r\n         .          .    135:\t\treturn false\r\n         .          .    136:\t}\r\n         .          .    137:\tif i != last {\r\n     230ms      230ms    138:\t\ttimers.t[i] = timers.t[last]\r\n      70ms       70ms    139:\t\ttimers.t[i].i = i\r\n         .          .    140:\t}\r\n     250ms      250ms    141:\ttimers.t[last] = nil\r\n         .          .    142:\ttimers.t = timers.t[:last]\r\n         .          .    143:\tif i != last {\r\n      20ms      180ms    144:\t\tsiftupTimer(i)\r\n      10ms      110ms    145:\t\tsiftdownTimer(i)\r\n         .          .    146:\t}\r\n      20ms      1.11s    147:\tunlock(\u0026timers.lock)\r\n      40ms       40ms    148:\treturn true\r\n         .          .    149:}\r\n         .          .    150:\r\n         .          .    151:// Timerproc runs the time-driven events.\r\n         .          .    152:// It sleeps until the next event in the timers heap.\r\n         .          .    153:// If addtimer inserts a new earlier event, addtimer1 wakes timerproc early.\r\n```\r\n\r\n\u003e Would it be possible to change it to SetDeadline? It would setup 1 timer instead of 2.\r\n\r\nNo, this won't work, since read and write deadlines are configured independently in our application.",
	"user": {
		"login": "valyala",
		"id": 283442,
		"type": "User",
		"site_admin": false
	},
	"created_at": "2016-04-06T13:18:49Z",
	"updated_at": "2016-04-06T13:19:27Z"
}
