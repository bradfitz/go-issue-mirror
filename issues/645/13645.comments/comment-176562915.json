{
	"id": 176562915,
	"body": "I was able to collect some core dumps from this. I took the core dumps without allowing the freezetheworld and various other things that happen in the panic handler and, interestingly, the story they tell is rather different from the time out tracebacks.\r\n\r\nHere's a representative example:\r\n```\r\nG state:\r\n       1 waiting (GC assist wait), m 0 (locked to thread)\r\n         started at runtime.main\r\n         created by runtime.rt0_go\r\n       2 waiting (force gc (idle))\r\n         started at runtime.forcegchelper\r\n         created by runtime.init.4\r\n      17 syscall, m 1 (locked to thread)\r\n         started at 0x0\r\n         created by 0x0\r\n      18 waiting (GC sweep wait)\r\n         started at runtime.bgsweep\r\n         created by runtime.gcenable\r\n      19 waiting (finalizer wait)\r\n         started at runtime.runfinq\r\n         created by runtime.createfing\r\n      20 syscall, m 11\r\n         started at runtime.timerproc\r\n         created by runtime.addtimerLocked\r\n     112 waiting (GC worker (idle))\r\n         started at runtime.gcBgMarkWorker\r\n         created by runtime.gcBgMarkStartWorkers\r\n  207479 waiting (GC worker (idle))\r\n         started at runtime.gcBgMarkWorker\r\n         created by runtime.gcBgMarkStartWorkers\r\n  207598 waiting (chan receive)\r\n         started at testing.tRunner\r\n         created by testing.RunTests\r\n  207599 waiting (chan receive)\r\n         started at testing.RunTests.func1\r\n         created by testing.RunTests\r\n  207600 waiting (chan receive)\r\n         started at testing.tRunner\r\n         created by testing.RunTests\r\n  207601 waiting (chan receive)\r\n         started at testing.RunTests.func1\r\n         created by testing.RunTests\r\n\r\nM state:\r\n       0 PID 28287, g 1, blocked, locked to thread\r\n       1 PID 0, g 17, locked to thread\r\n       2 PID 28290, blocked\r\n       3 PID 28291, blocked\r\n       4 PID 28292, blocked\r\n       5 PID 28293, blocked\r\n       6 PID 28294, blocked\r\n       7 PID 28295, blocked\r\n       8 PID 28296, epollwait\r\n       9 PID 28297, blocked\r\n      10 PID 28298, blocked\r\n      11 PID 29459, g 20, blocked\r\n      12 PID 29460, blocked\r\n      13 PID 29461, blocked\r\n      14 PID 29462, blocked\r\n\r\nP state:\r\n       0 idle, mark worker g 112\r\n         runq: \u003cempty\u003e\r\n       1 idle, mark worker g 207479\r\n         runq: \u003cempty\u003e\r\n\r\nglobal runq: \u003cempty\u003e\r\n```\r\n\r\nAll of the Gs are waiting and all of the Ms are blocked. Notably, the main G is blocked in an assist *and* no workers are running *and* everything else is waiting, so nothing schedules a new worker and no worker satisfies the assist.\r\n\r\nGC is in mark 2. work.full is empty, but only 2 of 4 mark root jobs have completed, so there is still GC work to be done. In this situation, since there are only two Ps, it's possible that no fractional worker would be scheduled, but I would expect idle workers to keep things going. The lack of idle workers indicates that we somehow failed to satisfy the idle worker start condition on all of the Ps simultaneously just before they all blocked. My best guess is that the workers were detached, but I haven't been able to come up with a sequence that would cause this, and as of the core file all Ps have workers.\r\n\r\nA few things worth trying to see if they amplify the chance of this happening: Add delay between detaching and gcMarkDone to encourage the workers to simultaneously detach. Disable fractional and dedicated workers to force everything through idle workers.\r\n\r\nFor debugging, it may help for the Ps to record some of the state of the world before they go idle, like why they didn't start an idle worker.",
	"user": {
		"login": "aclements",
		"id": 2688315,
		"type": "User",
		"site_admin": false
	},
	"created_at": "2016-01-29T04:01:41Z",
	"updated_at": "2016-01-29T04:01:41Z"
}
