{
	"id": 193447550,
	"body": "OK, both at master and at dff7490, I now get further, but I don't think the test is working. \r\n\r\nI've gotten the command sequence down to:\r\n\r\n```\r\ngit checkout dff7490c57f309\r\nmake clean\r\nmake quick-release\r\ngo run hack/e2e.go -v -down\r\nNUM_NODES=3 NODE_SIZE=n1-standard-4 go run hack/e2e.go -v -up\r\nNUM_NODES=100 MASTER_SIZE=n1-standard-4 ./test/kubemark/start-kubemark.sh\r\ntest/kubemark/run-e2e-tests.sh --delete-namespace=false\r\n```\r\n\r\nand then that last command prints:\r\n\r\n```\r\n$ test/kubemark/run-e2e-tests.sh --delete-namespace=false\r\nKubemark master name: kubernetes-kubemark-master\r\nYour active configuration is: [default]\r\n\r\nProject: kubertest2\r\nZone: us-central1-b\r\nUsing master: kubernetes-kubemark-master (external IP: 104.154.20.128)\r\nSetting up for KUBERNETES_PROVIDER=\"kubemark\".\r\nYour active configuration is: [default]\r\n\r\nProject: kubertest2\r\nZone: us-central1-b\r\nI0307 15:58:56.266669   30705 e2e.go:237] Starting e2e run \"681e73a2-e4a7-11e5-aeec-8cdcd443ecd8\" on Ginkgo node 1\r\nRunning Suite: Kubernetes e2e suite\r\n===================================\r\nRandom Seed: 1457384335 - Will randomize all specs\r\nWill run 153 of 221 specs\r\n\r\nMar  7 15:58:56.283: INFO: \u003e\u003e\u003e testContext.KubeConfig: /usr/local/google/home/rsc/kubernetes/test/kubemark/kubeconfig.loc\r\n\r\nMar  7 15:58:56.287: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready\r\nMar  7 15:58:56.487: INFO: 0 / 0 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)\r\nMar  7 15:58:56.487: INFO: expected 0 pod replicas in namespace 'kube-system', 0 are Running and Ready.\r\nKubectl client Kubectl describe \r\n  should check if kubectl describe prints relevant information for rc and pods [Conformance]\r\n  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:618\r\n[BeforeEach] Kubectl client\r\n  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework.go:69\r\nSTEP: Creating a kubernetes client\r\nMar  7 15:58:56.487: INFO: \u003e\u003e\u003e testContext.KubeConfig: /usr/local/google/home/rsc/kubernetes/test/kubemark/kubeconfig.loc\r\n\r\nSTEP: Building a namespace api object\r\nMar  7 15:58:56.531: INFO: Skipping waiting for service account\r\n[BeforeEach] Kubectl client\r\n  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:86\r\n[It] should check if kubectl describe prints relevant information for rc and pods [Conformance]\r\n  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:618\r\nMar  7 15:58:56.531: INFO: Running '/usr/local/google/home/rsc/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --server=https://104.154.20.128 --kubeconfig=/usr/local/google/home/rsc/kubernetes/test/kubemark/kubeconfig.loc create -f /usr/local/google/home/rsc/kubernetes/examples/guestbook-go/redis-master-controller.json --namespace=e2e-tests-kubectl-jw96o'\r\nMar  7 15:58:56.885: INFO: stdout: \"replicationcontroller \\\"redis-master\\\" created\\n\"\r\nMar  7 15:58:56.885: INFO: stderr: \"\"\r\nMar  7 15:58:56.885: INFO: Running '/usr/local/google/home/rsc/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --server=https://104.154.20.128 --kubeconfig=/usr/local/google/home/rsc/kubernetes/test/kubemark/kubeconfig.loc create -f /usr/local/google/home/rsc/kubernetes/examples/guestbook-go/redis-master-service.json --namespace=e2e-tests-kubectl-jw96o'\r\nMar  7 15:58:57.243: INFO: stdout: \"service \\\"redis-master\\\" created\\n\"\r\nMar  7 15:58:57.243: INFO: stderr: \"\"\r\nMar  7 15:58:57.287: INFO: Waiting up to 5m0s for pod redis-master-ugeq9 status to be running\r\nMar  7 15:58:57.327: INFO: Waiting for pod redis-master-ugeq9 in namespace 'e2e-tests-kubectl-jw96o' status to be 'running'(found phase: \"Pending\", readiness: false) (39.138172ms elapsed)\r\nMar  7 15:58:59.366: INFO: Found pod 'redis-master-ugeq9' on node '10.245.2.28'\r\nMar  7 15:58:59.366: INFO: Running '/usr/local/google/home/rsc/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --server=https://104.154.20.128 --kubeconfig=/usr/local/google/home/rsc/kubernetes/test/kubemark/kubeconfig.loc describe pod redis-master-ugeq9 --namespace=e2e-tests-kubectl-jw96o'\r\nMar  7 15:58:59.755: INFO: stdout: \"Name:\\t\\tredis-master-ugeq9\\nNamespace:\\te2e-tests-kubectl-jw96o\\nImage(s):\\tredis\\nNode:\\t\\t10.245.2.28/10.245.2.28\\nStart Time:\\tMon, 07 Mar 2016 15:58:56 -0500\\nLabels:\\t\\tapp=redis,role=master\\nStatus:\\t\\tRunning\\nReason:\\t\\t\\nMessage:\\t\\nIP:\\t\\t2.3.4.5\\nControllers:\\tReplicationController/redis-master\\nContainers:\\n  redis-master:\\n    Container ID:\\tdocker:///k8s_redis-master.45360489_redis-master-ugeq9_e2e-tests-kubectl-jw96o_6881c31f-e4a7-11e5-84b2-42010a800002_516c42b0\\n    Image:\\t\\tredis\\n    Image ID:\\t\\tdocker://\\n    QoS Tier:\\n      cpu:\\t\\tBestEffort\\n      memory:\\t\\tBestEffort\\n    State:\\t\\tRunning\\n      Started:\\t\\tMon, 07 Mar 2016 15:58:57 -0500\\n    Ready:\\t\\tTrue\\n    Restart Count:\\t0\\n    Environment Variables:\\nConditions:\\n  Type\\t\\tStatus\\n  Ready \\tTrue \\nNo volumes.\\nEvents:\\n  FirstSeen\\tLastSeen\\tCount\\tFrom\\t\\t\\tSubobjectPath\\tType\\t\\tReason\\t\\tMessage\\n  ---------\\t--------\\t-----\\t----\\t\\t\\t-------------\\t--------\\t------\\t\\t-------\\n  3s\\t\\t3s\\t\\t1\\t{default-scheduler }\\t\\t\\tNormal\\t\\tScheduled\\tSuccessfully assigned redis-master-ugeq9 to 10.245.2.28\\n\\n\\n\"\r\nMar  7 15:58:59.755: INFO: stderr: \"\"\r\nMar  7 15:58:59.756: INFO: Running '/usr/local/google/home/rsc/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --server=https://104.154.20.128 --kubeconfig=/usr/local/google/home/rsc/kubernetes/test/kubemark/kubeconfig.loc describe rc redis-master --namespace=e2e-tests-kubectl-jw96o'\r\nMar  7 15:59:00.129: INFO: stdout: \"Name:\\t\\tredis-master\\nNamespace:\\te2e-tests-kubectl-jw96o\\nImage(s):\\tredis\\nSelector:\\tapp=redis,role=master\\nLabels:\\t\\tapp=redis,role=master\\nReplicas:\\t1 current / 1 desired\\nPods Status:\\t1 Running / 0 Waiting / 0 Succeeded / 0 Failed\\nNo volumes.\\nEvents:\\n  FirstSeen\\tLastSeen\\tCount\\tFrom\\t\\t\\t\\tSubobjectPath\\tType\\t\\tReason\\t\\t\\tMessage\\n  ---------\\t--------\\t-----\\t----\\t\\t\\t\\t-------------\\t--------\\t------\\t\\t\\t-------\\n  4s\\t\\t4s\\t\\t1\\t{replication-controller }\\t\\t\\tNormal\\t\\tSuccessfulCreate\\tCreated pod: redis-master-ugeq9\\n\\n\\n\"\r\nMar  7 15:59:00.129: INFO: stderr: \"\"\r\nMar  7 15:59:00.129: INFO: Running '/usr/local/google/home/rsc/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --server=https://104.154.20.128 --kubeconfig=/usr/local/google/home/rsc/kubernetes/test/kubemark/kubeconfig.loc describe service redis-master --namespace=e2e-tests-kubectl-jw96o'\r\nMar  7 15:59:00.516: INFO: stdout: \"Name:\\t\\t\\tredis-master\\nNamespace:\\t\\te2e-tests-kubectl-jw96o\\nLabels:\\t\\t\\tapp=redis,role=master\\nSelector:\\t\\tapp=redis,role=master\\nType:\\t\\t\\tClusterIP\\nIP:\\t\\t\\t10.0.0.212\\nPort:\\t\\t\\t\u003cunnamed\u003e\\t6379/TCP\\nEndpoints:\\t\\t2.3.4.5:6379\\nSession Affinity:\\tNone\\nNo events.\\n\\n\"\r\nMar  7 15:59:00.516: INFO: stderr: \"\"\r\nMar  7 15:59:00.697: INFO: Running '/usr/local/google/home/rsc/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --server=https://104.154.20.128 --kubeconfig=/usr/local/google/home/rsc/kubernetes/test/kubemark/kubeconfig.loc describe node 10.245.0.10'\r\nMar  7 15:59:01.088: INFO: stdout: \"Name:\\t\\t\\t10.245.0.10\\nLabels:\\t\\t\\tkubernetes.io/hostname=10.245.0.10\\nCreationTimestamp:\\tMon, 07 Mar 2016 15:57:55 -0500\\nPhase:\\t\\t\\t\\nConditions:\\n  Type\\t\\tStatus\\tLastHeartbeatTime\\t\\t\\tLastTransitionTime\\t\\t\\tReason\\t\\t\\t\\tMessage\\n  ----\\t\\t------\\t-----------------\\t\\t\\t------------------\\t\\t\\t------\\t\\t\\t\\t-------\\n  OutOfDisk \\tFalse \\tMon, 07 Mar 2016 15:58:55 -0500 \\tMon, 07 Mar 2016 15:57:55 -0500 \\tKubeletHasSufficientDisk \\tkubelet has sufficient disk space available\\n  Ready \\tTrue \\tMon, 07 Mar 2016 15:58:55 -0500 \\tMon, 07 Mar 2016 15:57:55 -0500 \\tKubeletReady \\t\\t\\tkubelet is posting ready status\\nAddresses:\\t10.245.0.10,10.245.0.10\\nCapacity:\\n memory:\\t0\\n pods:\\t\\t40\\n cpu:\\t\\t0\\nSystem Info:\\n Machine ID:\\t\\t\\t\\n System UUID:\\t\\t\\t\\n Boot ID:\\t\\t\\t\\n Kernel Version:\\t\\t\\n OS Image:\\t\\t\\t\\n Container Runtime Version:\\tdocker://1.1.3\\n Kubelet Version:\\t\\tv1.2.0-alpha.6.909+dff7490c57f309\\n Kube-Proxy Version:\\t\\tv1.2.0-alpha.6.909+dff7490c57f309\\nExternalID:\\t\\t\\t10.245.0.10\\nNon-terminated Pods:\\t\\t(0 in total)\\n  Namespace\\t\\t\\tName\\t\\tCPU Requests\\tCPU Limits\\tMemory Requests\\tMemory Limits\\n  ---------\\t\\t\\t----\\t\\t------------\\t----------\\t---------------\\t-------------\\nAllocated resources:\\n  (Total limits may be over 100%, i.e., overcommitted. More info: http://releases.k8s.io/HEAD/docs/user-guide/compute-resources.md)\\n  CPU Requests\\t\\t\\tCPU Limits\\t\\t\\tMemory Requests\\t\\t\\tMemory Limits\\n  ------------\\t\\t\\t----------\\t\\t\\t---------------\\t\\t\\t-------------\\n  0 (-9223372036854775808%)\\t0 (-9223372036854775808%)\\t0 (-9223372036854775808%)\\t0 (-9223372036854775808%)\\nEvents:\\n  FirstSeen\\tLastSeen\\tCount\\tFrom\\t\\t\\t\\tSubobjectPath\\tType\\t\\tReason\\t\\tMessage\\n  ---------\\t--------\\t-----\\t----\\t\\t\\t\\t-------------\\t--------\\t------\\t\\t-------\\n  1m\\t\\t1m\\t\\t1\\t{kube-proxy 10.245.0.10}\\t\\t\\tNormal\\t\\tStarting\\tStarting kube-proxy.\\n  1m\\t\\t1m\\t\\t1\\t{controllermanager }\\t\\t\\t\\tNormal\\t\\tRegisteredNode\\tNode 10.245.0.10 event: Registered Node 10.245.0.10 in NodeController\\n\\n\\n\"\r\nMar  7 15:59:01.088: INFO: stderr: \"\"\r\nMar  7 15:59:01.088: INFO: Running '/usr/local/google/home/rsc/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --server=https://104.154.20.128 --kubeconfig=/usr/local/google/home/rsc/kubernetes/test/kubemark/kubeconfig.loc describe namespace e2e-tests-kubectl-jw96o'\r\nMar  7 15:59:01.462: INFO: stdout: \"Name:\\te2e-tests-kubectl-jw96o\\nLabels:\\te2e-framework=kubectl,e2e-run=681e73a2-e4a7-11e5-aeec-8cdcd443ecd8\\nStatus:\\tActive\\n\\nNo resource quota.\\n\\nNo resource limits.\\n\\n\\n\"\r\nMar  7 15:59:01.462: INFO: stderr: \"\"\r\n[AfterEach] Kubectl client\r\n  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework.go:70\r\nMar  7 15:59:01.462: INFO: Waiting up to 1m0s for all nodes to be ready\r\nMar  7 15:59:01.648: INFO: Found DeleteNamespace=false, skipping namespace deletion!\r\n\r\n• [SLOW TEST:5.161 seconds]\r\nKubectl client\r\n/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:995\r\n  Kubectl describe\r\n  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:619\r\n    should check if kubectl describe prints relevant information for rc and pods [Conformance]\r\n    /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:618\r\n------------------------------\r\nAddon update \r\n  should propagate add-on file changes\r\n  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/addon_update.go:322\r\n[BeforeEach] Addon update\r\n  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework.go:69\r\nSTEP: Creating a kubernetes client\r\nMar  7 15:59:01.648: INFO: \u003e\u003e\u003e testContext.KubeConfig: /usr/local/google/home/rsc/kubernetes/test/kubemark/kubeconfig.loc\r\n\r\nSTEP: Building a namespace api object\r\nMar  7 15:59:01.695: INFO: Skipping waiting for service account\r\n[BeforeEach] Addon update\r\n  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/addon_update.go:214\r\n[It] should propagate add-on file changes\r\n  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/addon_update.go:322\r\n[AfterEach] Addon update\r\n  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework.go:70\r\nMar  7 15:59:01.696: INFO: Waiting up to 1m0s for all nodes to be ready\r\nMar  7 15:59:01.803: INFO: Found DeleteNamespace=false, skipping namespace deletion!\r\n[AfterEach] Addon update\r\n  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/addon_update.go:222\r\n\r\nS [SKIPPING] [0.155 seconds]\r\nAddon update\r\n/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/addon_update.go:323\r\n  should propagate add-on file changes [It]\r\n  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/addon_update.go:322\r\n\r\n  Mar  7 15:59:01.695: Only supported for providers [gce] (not kubemark)\r\n\r\n  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/util.go:263\r\n------------------------------\r\nSSS\r\n------------------------------\r\nKubectl client Kubectl apply \r\n  should apply a new configuration to an existing RC\r\n  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:505\r\n[BeforeEach] Kubectl client\r\n  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework.go:69\r\nSTEP: Creating a kubernetes client\r\nMar  7 15:59:01.803: INFO: \u003e\u003e\u003e testContext.KubeConfig: /usr/local/google/home/rsc/kubernetes/test/kubemark/kubeconfig.loc\r\n\r\nSTEP: Building a namespace api object\r\nMar  7 15:59:01.843: INFO: Skipping waiting for service account\r\n[BeforeEach] Kubectl client\r\n  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:86\r\n[It] should apply a new configuration to an existing RC\r\n  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:505\r\nSTEP: creating Redis RC\r\nMar  7 15:59:01.843: INFO: Running '/usr/local/google/home/rsc/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --server=https://104.154.20.128 --kubeconfig=/usr/local/google/home/rsc/kubernetes/test/kubemark/kubeconfig.loc create -f /usr/local/google/home/rsc/kubernetes/examples/guestbook-go/redis-master-controller.json --namespace=e2e-tests-kubectl-kjl0q'\r\nMar  7 15:59:02.190: INFO: stdout: \"replicationcontroller \\\"redis-master\\\" created\\n\"\r\nMar  7 15:59:02.190: INFO: stderr: \"\"\r\nSTEP: applying a modified configuration\r\nMar  7 15:59:02.193: INFO: Running '/usr/local/google/home/rsc/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --server=https://104.154.20.128 --kubeconfig=/usr/local/google/home/rsc/kubernetes/test/kubemark/kubeconfig.loc apply -f - --namespace=e2e-tests-kubectl-kjl0q'\r\nMar  7 15:59:02.582: INFO: stdout: \"replicationcontroller \\\"redis-master\\\" configured\\n\"\r\nMar  7 15:59:02.582: INFO: stderr: \"\"\r\nSTEP: checking the result\r\n[AfterEach] Kubectl client\r\n  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework.go:70\r\nMar  7 15:59:02.621: INFO: Waiting up to 1m0s for all nodes to be ready\r\nMar  7 15:59:02.762: INFO: Found DeleteNamespace=false, skipping namespace deletion!\r\n•\r\n------------------------------\r\nKubectl client Proxy server \r\n  should support --unix-socket=/path [Conformance]\r\n  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:993\r\n[BeforeEach] Kubectl client\r\n  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework.go:69\r\nSTEP: Creating a kubernetes client\r\nMar  7 15:59:02.763: INFO: \u003e\u003e\u003e testContext.KubeConfig: /usr/local/google/home/rsc/kubernetes/test/kubemark/kubeconfig.loc\r\n\r\nSTEP: Building a namespace api object\r\nMar  7 15:59:02.803: INFO: Skipping waiting for service account\r\n[BeforeEach] Kubectl client\r\n  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:86\r\n[It] should support --unix-socket=/path [Conformance]\r\n  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:993\r\nSTEP: Starting the proxy\r\nMar  7 15:59:02.803: INFO: Asynchronously running '/usr/local/google/home/rsc/kubernetes/_output/dockerized/bin/linux/amd64/kubectl kubectl --server=https://104.154.20.128 --kubeconfig=/usr/local/google/home/rsc/kubernetes/test/kubemark/kubeconfig.loc proxy --unix-socket=/tmp/kubectl-proxy-unix513838643/test'\r\nSTEP: retrieving proxy /api/ output\r\n[AfterEach] Kubectl client\r\n  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework.go:70\r\nMar  7 15:59:03.027: INFO: Waiting up to 1m0s for all nodes to be ready\r\nMar  7 15:59:03.137: INFO: Found DeleteNamespace=false, skipping namespace deletion!\r\n•\r\n------------------------------\r\nMetricsGrabber \r\n  should grab all metrics from a Kubelet.\r\n  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/metrics_grabber_test.go:112\r\n[BeforeEach] MetricsGrabber\r\n  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework.go:69\r\nSTEP: Creating a kubernetes client\r\nMar  7 15:59:03.137: INFO: \u003e\u003e\u003e testContext.KubeConfig: /usr/local/google/home/rsc/kubernetes/test/kubemark/kubeconfig.loc\r\n\r\nSTEP: Building a namespace api object\r\nMar  7 15:59:03.177: INFO: Skipping waiting for service account\r\n[BeforeEach] MetricsGrabber\r\n  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/metrics_grabber_test.go:90\r\nW0307 15:59:03.251062   30705 metrics_grabber.go:74] Master node is not registered. Grabbing metrics from Scheduler and ControllerManager is disabled.\r\n[It] should grab all metrics from a Kubelet.\r\n  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/metrics_grabber_test.go:112\r\n[AfterEach] MetricsGrabber\r\n  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework.go:70\r\nMar  7 15:59:03.251: INFO: Waiting up to 1m0s for all nodes to be ready\r\nMar  7 15:59:03.321: INFO: Found DeleteNamespace=false, skipping namespace deletion!\r\n•\r\n------------------------------\r\nPort forwarding With a server that expects no client request \r\n  should support a client that connects, sends no data, and disconnects [Conformance]\r\n  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/portforward.go:238\r\n[BeforeEach] Port forwarding\r\n  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework.go:69\r\nSTEP: Creating a kubernetes client\r\nMar  7 15:59:03.321: INFO: \u003e\u003e\u003e testContext.KubeConfig: /usr/local/google/home/rsc/kubernetes/test/kubemark/kubeconfig.loc\r\n\r\nSTEP: Building a namespace api object\r\nMar  7 15:59:03.366: INFO: Skipping waiting for service account\r\n[It] should support a client that connects, sends no data, and disconnects [Conformance]\r\n  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/portforward.go:238\r\nSTEP: creating the target pod\r\nMar  7 15:59:03.431: INFO: Waiting up to 5m0s for pod pfpod status to be running\r\nMar  7 15:59:03.469: INFO: Waiting for pod pfpod in namespace 'e2e-tests-port-forwarding-ad7sb' status to be 'running'(found phase: \"Pending\", readiness: false) (38.627068ms elapsed)\r\nMar  7 15:59:05.508: INFO: Found pod 'pfpod' on node '10.245.2.10'\r\nSTEP: Running 'kubectl port-forward'\r\nMar  7 15:59:05.508: INFO: starting port-forward command and streaming output\r\nMar  7 15:59:05.508: INFO: Asynchronously running '/usr/local/google/home/rsc/kubernetes/_output/dockerized/bin/linux/amd64/kubectl kubectl --server=https://104.154.20.128 --kubeconfig=/usr/local/google/home/rsc/kubernetes/test/kubemark/kubeconfig.loc port-forward --namespace=e2e-tests-port-forwarding-ad7sb pfpod :80'\r\nMar  7 15:59:05.510: INFO: reading from `kubectl port-forward` command's stderr\r\n[AfterEach] Port forwarding\r\n  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework.go:70\r\nSTEP: Collecting events from namespace \"e2e-tests-port-forwarding-ad7sb\".\r\nMar  7 15:59:36.053: INFO: event for pfpod: {default-scheduler } Scheduled: Successfully assigned pfpod to 10.245.2.10\r\nMar  7 15:59:36.093: INFO: POD                 NODE         PHASE    GRACE  CONDITIONS\r\nMar  7 15:59:36.094: INFO: redis-master-ugeq9  10.245.2.28  Running         [{Ready True 0001-01-01 00:00:00 +0000 UTC 2016-03-07 15:58:58 -0500 EST  }]\r\nMar  7 15:59:36.094: INFO: redis-master-p2zsc  10.245.0.29  Running         [{Ready True 0001-01-01 00:00:00 +0000 UTC 2016-03-07 15:59:03 -0500 EST  }]\r\nMar  7 15:59:36.094: INFO: redis-master-tltco  10.245.0.14  Running         [{Ready True 0001-01-01 00:00:00 +0000 UTC 2016-03-07 15:59:03 -0500 EST  }]\r\nMar  7 15:59:36.094: INFO: pfpod               10.245.2.10  Running         [{Ready True 0001-01-01 00:00:00 +0000 UTC 2016-03-07 15:59:04 -0500 EST  }]\r\nMar  7 15:59:36.094: INFO: \r\nMar  7 15:59:36.277: INFO: \r\nLogging node info for node 10.245.0.10\r\nMar  7 15:59:36.316: INFO: Node Info: \u0026{{ } {10.245.0.10   /api/v1/nodes/10.245.0.10 4417323c-e4a7-11e5-84b2-42010a800002 1274 0 2016-03-07 15:57:55 -0500 EST \u003cnil\u003e \u003cnil\u003e map[kubernetes.io/hostname:10.245.0.10] map[]} { 10.245.0.10  false} {map[cpu:{0.000 DecimalSI} memory:{0.000 DecimalSI} pods:{40.000 DecimalSI}] map[memory:{0.000 DecimalSI} pods:{40.000 DecimalSI} cpu:{0.000 DecimalSI}]  [{OutOfDisk False 2016-03-07 15:59:35 -0500 EST 2016-03-07 15:57:55 -0500 EST KubeletHasSufficientDisk kubelet has sufficient disk space available} {Ready True 2016-03-07 15:59:35 -0500 EST 2016-03-07 15:57:55 -0500 EST KubeletReady kubelet is posting ready status}] [{LegacyHostIP 10.245.0.10} {InternalIP 10.245.0.10}] {{10250}} {     docker://1.1.3 v1.2.0-alpha.6.909+dff7490c57f309 v1.2.0-alpha.6.909+dff7490c57f309} []}}\r\nMar  7 15:59:36.316: INFO: \r\nLogging kubelet events for node 10.245.0.10\r\nMar  7 15:59:36.354: INFO: \r\nLogging pods the kubelet thinks is on node 10.245.0.10\r\nMar  7 16:00:06.393: INFO: Unable to retrieve kubelet pods for node 10.245.0.10\r\nMar  7 16:00:06.393: INFO: \r\nLogging node info for node 10.245.0.11\r\nMar  7 16:00:06.432: INFO: Node Info: \u0026{{ } {10.245.0.11   /api/v1/nodes/10.245.0.11 40fa9f2f-e4a7-11e5-84b2-42010a800002 1525 0 2016-03-07 15:57:50 -0500 EST \u003cnil\u003e \u003cnil\u003e map[kubernetes.io/hostname:10.245.0.11] map[]} { 10.245.0.11  false} {map[pods:{40.000 DecimalSI} cpu:{0.000 DecimalSI} memory:{0.000 DecimalSI}] map[cpu:{0.000 DecimalSI} memory:{0.000 DecimalSI} pods:{40.000 DecimalSI}]  [{OutOfDisk False 2016-03-07 16:00:00 -0500 EST 2016-03-07 15:57:50 -0500 EST KubeletHasSufficientDisk kubelet has sufficient disk space available} {Ready True 2016-03-07 16:00:00 -0500 EST 2016-03-07 15:57:50 -0500 EST KubeletReady kubelet is posting ready status}] [{LegacyHostIP 10.245.0.11} {InternalIP 10.245.0.11}] {{10250}} {     docker://1.1.3 v1.2.0-alpha.6.909+dff7490c57f309 v1.2.0-alpha.6.909+dff7490c57f309} []}}\r\nMar  7 16:00:06.432: INFO: \r\nLogging kubelet events for node 10.245.0.11\r\nMar  7 16:00:06.470: INFO: \r\nLogging pods the kubelet thinks is on node 10.245.0.11\r\nMar  7 16:00:36.509: INFO: Unable to retrieve kubelet pods for node 10.245.0.11\r\n...\r\n```\r\n\r\nwhere each stanza sits for a little while and then says \"Unable to retrieve kubelet pods for node XXX\". I am guessing this is not good. I let it run for quite a while once and then interrupted it. I can let it run longer next time if this is actually OK. When I interrupt it I get:\r\n\r\n```\r\nLogging pods the kubelet thinks is on node 10.245.0.14\r\n^C\r\n---------------------------------------------------------\r\nReceived interrupt.  Running AfterSuite...\r\n^C again to terminate immediately\r\n\r\nRan 152 of 221 Specs in 188.326 seconds\r\nFAIL! -- 4 Passed | 1 Failed | 2 Pending | 67 Skipped \r\nGinkgo ran 1 suite in 3m8.672947611s\r\nTest Suite Failed\r\n```\r\n\r\nAm I making progress?",
	"user": {
		"login": "rsc",
		"id": 104030,
		"type": "User",
		"site_admin": false
	},
	"created_at": "2016-03-07T21:04:12Z",
	"updated_at": "2016-03-07T21:04:12Z"
}
