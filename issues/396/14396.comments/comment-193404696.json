{
	"id": 193404696,
	"body": "OK, still following the doc posted earlier. I can run start-kubemark.sh, but the output looks maybe not quite right:\r\n\r\n```\r\n$ NUM_NODES=100 MASTER_SIZE=n1-standard-4 ./test/kubemark/start-kubemark.sh\r\ndocker build -t gcr.io/kubertest2/kubemark .\r\nSending build context to Docker daemon 58.54 MB\r\nStep 1 : FROM debian:jessie\r\n ---\u003e 040bf8e08425\r\nStep 2 : COPY kubemark.sh /kubemark.sh\r\n ---\u003e Using cache\r\n ---\u003e 0f42238a0dce\r\nStep 3 : RUN chmod a+x /kubemark.sh\r\n ---\u003e Using cache\r\n ---\u003e 4db91651de2f\r\nStep 4 : COPY kubemark /kubemark\r\n ---\u003e 14b35b4c46fa\r\nRemoving intermediate container b6a6e29c624f\r\nSuccessfully built 14b35b4c46fa\r\ngcloud docker push gcr.io/kubertest2/kubemark\r\nThe push refers to a repository [gcr.io/kubertest2/kubemark] (len: 1)\r\n14b35b4c46fa: Pushed \r\n4db91651de2f: Image already exists \r\n0f42238a0dce: Image already exists \r\n040bf8e08425: Image already exists \r\n73e8d4f6bf84: Image already exists \r\nlatest: digest: sha256:b67b4eab4b1099bc4da74231e18f9fba56e5f66bcffb424c4179021d986e2155 size: 7441\r\nCreated [https://www.googleapis.com/compute/v1/projects/kubertest2/zones/us-central1-b/disks/kubernetes-kubemark-master-pd].\r\nNAME                          ZONE          SIZE_GB TYPE   STATUS\r\nkubernetes-kubemark-master-pd us-central1-b 20      pd-ssd READY\r\nCreated [https://www.googleapis.com/compute/v1/projects/kubertest2/zones/us-central1-b/instances/kubernetes-kubemark-master].\r\nNAME                       ZONE          MACHINE_TYPE  PREEMPTIBLE INTERNAL_IP EXTERNAL_IP     STATUS\r\nkubernetes-kubemark-master us-central1-b n1-standard-4             10.128.0.2  104.197.228.204 RUNNING\r\nCreated [https://www.googleapis.com/compute/v1/projects/kubertest2/global/firewalls/kubernetes-kubemark-master-https].\r\nNAME                             NETWORK SRC_RANGES RULES   SRC_TAGS TARGET_TAGS\r\nkubernetes-kubemark-master-https default 0.0.0.0/0  tcp:443          kubemark-master\r\nWarning: Permanently added '104.197.228.204' (ECDSA) to the list of known hosts.\r\nWarning: Permanently added '104.197.228.204' (ECDSA) to the list of known hosts.\r\nkubernetes-server-linux-amd64.tar.gz                                                                                                      100%  240MB  48.1MB/s   00:05    \r\nstart-kubemark-master.sh                                                                                                                  100% 2351     2.3KB/s   00:00    \r\nconfigure-kubectl.sh                                                                                                                      100% 1003     1.0KB/s   00:00    \r\nWarning: Permanently added '104.197.228.204' (ECDSA) to the list of known hosts.\r\nUnable to find image 'gcr.io/google_containers/etcd:2.2.1' locally\r\nPulling repository gcr.io/google_containers/etcd\r\nfbea2d67e633: Pulling image (2.2.1) from gcr.io/google_containers/etcd\r\nfbea2d67e633: Pulling image (2.2.1) from gcr.io/google_containers/etcd, endpoint: https://gcr.io/v1/\r\nfbea2d67e633: Pulling dependent layers\r\nbf0f46991aed: Pulling metadata\r\nbf0f46991aed: Pulling fs layer\r\nbf0f46991aed: Download complete\r\n3d5bcd78e074: Pulling metadata\r\n3d5bcd78e074: Pulling fs layer\r\n3d5bcd78e074: Download complete\r\n5673765ccce5: Pulling metadata\r\n5673765ccce5: Pulling fs layer\r\n5673765ccce5: Download complete\r\nd5bf32328e50: Pulling metadata\r\nd5bf32328e50: Pulling fs layer\r\nd5bf32328e50: Download complete\r\nfbea2d67e633: Pulling metadata\r\nfbea2d67e633: Pulling fs layer\r\nfbea2d67e633: Download complete\r\nfbea2d67e633: Download complete\r\nStatus: Downloaded newer image for gcr.io/google_containers/etcd:2.2.1\r\ngcr.io/google_containers/etcd: this image was pulled from a legacy registry.  Important: This registry version will not be supported in future versions of docker.\r\n7c805352cdba641901dbfb7bc21d1efe1fd177bff0c0221fe83b2141d2181277\r\n4464daddf21c06e00ca572212edb287132aa1dcb805d895cfa23d4e23b7d9ae0\r\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\r\n                                 Dload  Upload   Total   Spent    Left  Speed\r\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0curl: (7) couldn't connect to host\r\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\r\n                                 Dload  Upload   Total   Spent    Left  Speed\r\n100     2  100     2    0     0    987      0 --:--:-- --:--:-- --:--:--  2000\r\nThe connection to the server localhost:8080 was refused - did you specify the right host or port?\r\n```\r\n\r\nIs that successful output? Looks like maybe not? I am using `git checkout dff7490c57f309` because that was where the 1.5.3 regression was recorded.\r\n\r\nSince I don't know if that command succeeded, I tried the next one. It definitely says it failed, but I can't quite infer whether it's because the previous command failed or something else. Full output is below but the main complaint seems to be:\r\n\r\n```\r\n/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/density.go:465\r\n  [Performance] should allow starting 30 pods per node [BeforeEach]\r\n  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/density.go:463\r\n\r\n  Expected\r\n      \u003cint\u003e: 0\r\n  not to be zero-valued\r\n\r\n```\r\n\r\nI do agree that 0 is in fact zero-valued, but I am not sure what the point is. Full output:\r\n\r\n```\r\n$ test/kubemark/run-e2e-tests.sh\r\nKubemark master name: kubernetes-kubemark-master\r\nYour active configuration is: [default]\r\n\r\nProject: kubertest2\r\nZone: us-central1-b\r\nUsing master: kubernetes-kubemark-master (external IP: 104.197.228.204)\r\nSetting up for KUBERNETES_PROVIDER=\"kubemark\".\r\nYour active configuration is: [default]\r\n\r\nProject: kubertest2\r\nZone: us-central1-b\r\nI0307 14:12:39.417839   52136 e2e.go:237] Starting e2e run \"8f35cae5-e498-11e5-92f3-8cdcd443ecd8\" on Ginkgo node 1\r\nRunning Suite: Kubernetes e2e suite\r\n===================================\r\nRandom Seed: 1457377959 - Will randomize all specs\r\nWill run 1 of 221 specs\r\n\r\nMar  7 14:12:39.440: INFO: \u003e\u003e\u003e testContext.KubeConfig: /usr/local/google/home/rsc/kubernetes/test/kubemark/kubeconfig.loc\r\n\r\nMar  7 14:12:39.445: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready\r\nMar  7 14:12:39.651: INFO: 0 / 0 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)\r\nMar  7 14:12:39.651: INFO: expected 0 pod replicas in namespace 'kube-system', 0 are Running and Ready.\r\nSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS\r\n------------------------------\r\nDensity [Skipped] \r\n  [Performance] should allow starting 30 pods per node\r\n  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/density.go:463\r\n[BeforeEach] Density [Skipped]\r\n  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework.go:69\r\nSTEP: Creating a kubernetes client\r\nMar  7 14:12:39.651: INFO: \u003e\u003e\u003e testContext.KubeConfig: /usr/local/google/home/rsc/kubernetes/test/kubemark/kubeconfig.loc\r\n\r\nSTEP: Building a namespace api object\r\nMar  7 14:12:39.694: INFO: Skipping waiting for service account\r\n[BeforeEach] Density [Skipped]\r\n  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/density.go:170\r\n[AfterEach] Density [Skipped]\r\n  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/density.go:130\r\nMar  7 14:12:39.825: INFO: Top latency metric: {Resource:serviceaccounts Verb:PUT Latency:{Perc50:14.491ms Perc90:14.491ms Perc99:14.491ms}}\r\nMar  7 14:12:39.825: INFO: Top latency metric: {Resource:secrets Verb:POST Latency:{Perc50:8.098ms Perc90:8.098ms Perc99:8.098ms}}\r\nMar  7 14:12:39.825: INFO: Top latency metric: {Resource:serviceaccounts Verb:POST Latency:{Perc50:1.377ms Perc90:4.631ms Perc99:4.631ms}}\r\nMar  7 14:12:39.825: INFO: Top latency metric: {Resource:namespaces Verb:POST Latency:{Perc50:1.609ms Perc90:1.609ms Perc99:1.609ms}}\r\nMar  7 14:12:39.825: INFO: Top latency metric: {Resource:nodes Verb:LIST Latency:{Perc50:650µs Perc90:854µs Perc99:1.012ms}}\r\nMar  7 14:12:39.825: INFO: API calls latencies: {\r\n  \"apicalls\": [\r\n    {\r\n      \"resource\": \"serviceaccounts\",\r\n      \"verb\": \"PUT\",\r\n      \"latency\": {\r\n        \"Perc50\": 14491000,\r\n        \"Perc90\": 14491000,\r\n        \"Perc99\": 14491000\r\n      }\r\n    },\r\n    {\r\n      \"resource\": \"secrets\",\r\n      \"verb\": \"POST\",\r\n      \"latency\": {\r\n        \"Perc50\": 8098000,\r\n        \"Perc90\": 8098000,\r\n        \"Perc99\": 8098000\r\n      }\r\n    },\r\n    {\r\n      \"resource\": \"serviceaccounts\",\r\n      \"verb\": \"POST\",\r\n      \"latency\": {\r\n        \"Perc50\": 1377000,\r\n        \"Perc90\": 4631000,\r\n        \"Perc99\": 4631000\r\n      }\r\n    },\r\n    {\r\n      \"resource\": \"namespaces\",\r\n      \"verb\": \"POST\",\r\n      \"latency\": {\r\n        \"Perc50\": 1609000,\r\n        \"Perc90\": 1609000,\r\n        \"Perc99\": 1609000\r\n      }\r\n    },\r\n    {\r\n      \"resource\": \"nodes\",\r\n      \"verb\": \"LIST\",\r\n      \"latency\": {\r\n        \"Perc50\": 650000,\r\n        \"Perc90\": 854000,\r\n        \"Perc99\": 1012000\r\n      }\r\n    },\r\n    {\r\n      \"resource\": \"pods\",\r\n      \"verb\": \"LIST\",\r\n      \"latency\": {\r\n        \"Perc50\": 168000,\r\n        \"Perc90\": 860000,\r\n        \"Perc99\": 860000\r\n      }\r\n    },\r\n    {\r\n      \"resource\": \"horizontalpodautoscalers\",\r\n      \"verb\": \"LIST\",\r\n      \"latency\": {\r\n        \"Perc50\": 515000,\r\n        \"Perc90\": 661000,\r\n        \"Perc99\": 661000\r\n      }\r\n    },\r\n    {\r\n      \"resource\": \"serviceaccounts\",\r\n      \"verb\": \"GET\",\r\n      \"latency\": {\r\n        \"Perc50\": 452000,\r\n        \"Perc90\": 452000,\r\n        \"Perc99\": 452000\r\n      }\r\n    },\r\n    {\r\n      \"resource\": \"resourcequotas\",\r\n      \"verb\": \"LIST\",\r\n      \"latency\": {\r\n        \"Perc50\": 410000,\r\n        \"Perc90\": 410000,\r\n        \"Perc99\": 410000\r\n      }\r\n    },\r\n    {\r\n      \"resource\": \"serviceaccounts\",\r\n      \"verb\": \"LIST\",\r\n      \"latency\": {\r\n        \"Perc50\": 162000,\r\n        \"Perc90\": 357000,\r\n        \"Perc99\": 357000\r\n      }\r\n    },\r\n    {\r\n      \"resource\": \"secrets\",\r\n      \"verb\": \"LIST\",\r\n      \"latency\": {\r\n        \"Perc50\": 279000,\r\n        \"Perc90\": 279000,\r\n        \"Perc99\": 279000\r\n      }\r\n    },\r\n    {\r\n      \"resource\": \"replicationcontrollers\",\r\n      \"verb\": \"LIST\",\r\n      \"latency\": {\r\n        \"Perc50\": 226000,\r\n        \"Perc90\": 271000,\r\n        \"Perc99\": 271000\r\n      }\r\n    },\r\n    {\r\n      \"resource\": \"namespaces\",\r\n      \"verb\": \"LIST\",\r\n      \"latency\": {\r\n        \"Perc50\": 270000,\r\n        \"Perc90\": 270000,\r\n        \"Perc99\": 270000\r\n      }\r\n    },\r\n    {\r\n      \"resource\": \"services\",\r\n      \"verb\": \"LIST\",\r\n      \"latency\": {\r\n        \"Perc50\": 208000,\r\n        \"Perc90\": 232000,\r\n        \"Perc99\": 232000\r\n      }\r\n    },\r\n    {\r\n      \"resource\": \"jobs\",\r\n      \"verb\": \"LIST\",\r\n      \"latency\": {\r\n        \"Perc50\": 134000,\r\n        \"Perc90\": 201000,\r\n        \"Perc99\": 201000\r\n      }\r\n    },\r\n    {\r\n      \"resource\": \"persistentvolumeclaims\",\r\n      \"verb\": \"LIST\",\r\n      \"latency\": {\r\n        \"Perc50\": 170000,\r\n        \"Perc90\": 170000,\r\n        \"Perc99\": 170000\r\n      }\r\n    },\r\n    {\r\n      \"resource\": \"persistentvolumes\",\r\n      \"verb\": \"LIST\",\r\n      \"latency\": {\r\n        \"Perc50\": 166000,\r\n        \"Perc90\": 167000,\r\n        \"Perc99\": 167000\r\n      }\r\n    }\r\n  ]\r\n}\r\n\r\nMar  7 14:12:40.305: INFO: Scheduling latency: {\r\n  \"Scheduling\": {\r\n    \"Perc50\": 0,\r\n    \"Perc90\": 0,\r\n    \"Perc99\": 0\r\n  },\r\n  \"binding\": {\r\n    \"Perc50\": 0,\r\n    \"Perc90\": 0,\r\n    \"Perc99\": 0\r\n  },\r\n  \"total\": {\r\n    \"Perc50\": 0,\r\n    \"Perc90\": 0,\r\n    \"Perc99\": 0\r\n  }\r\n}\r\n\r\n[AfterEach] Density [Skipped]\r\n  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework.go:70\r\nSTEP: Collecting events from namespace \"e2e-tests-density-fors9\".\r\nMar  7 14:12:40.386: INFO: POD  NODE  PHASE  GRACE  CONDITIONS\r\nMar  7 14:12:40.386: INFO: \r\nMar  7 14:12:40.424: INFO: Waiting up to 1m0s for all nodes to be ready\r\nSTEP: Destroying namespace \"e2e-tests-density-fors9\" for this suite.\r\n\r\n• Failure in Spec Setup (BeforeEach) [5.968 seconds]\r\nDensity [Skipped]\r\n/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/density.go:465\r\n  [Performance] should allow starting 30 pods per node [BeforeEach]\r\n  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/density.go:463\r\n\r\n  Expected\r\n      \u003cint\u003e: 0\r\n  not to be zero-valued\r\n\r\n  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/density.go:144\r\n------------------------------\r\nSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS\r\n\r\nSummarizing 1 Failure:\r\n\r\n[Fail] Density [Skipped] [BeforeEach] [Performance] should allow starting 30 pods per node \r\n/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/density.go:144\r\n\r\nRan 1 of 221 Specs in 6.183 seconds\r\nFAIL! -- 0 Passed | 1 Failed | 0 Pending | 220 Skipped --- FAIL: TestE2E (6.21s)\r\nFAIL\r\n\r\nGinkgo ran 1 suite in 6.557506861s\r\nTest Suite Failed\r\n!!! Error in test/kubemark/../../cluster/kubemark/../../cluster/gce/../../cluster/../hack/ginkgo-e2e.sh:96\r\n  '\"${ginkgo}\" \"${ginkgo_args[@]:+${ginkgo_args[@]}}\" \"${e2e_test}\" -- \"${auth_config[@]:+${auth_config[@]}}\" --host=\"${KUBE_MASTER_URL}\" --provider=\"${KUBERNETES_PROVIDER}\" --gce-project=\"${PROJECT:-}\" --gce-zone=\"${ZONE:-}\" --gce-service-account=\"${GCE_SERVICE_ACCOUNT:-}\" --gke-cluster=\"${CLUSTER_NAME:-}\" --kube-master=\"${KUBE_MASTER:-}\" --cluster-tag=\"${CLUSTER_ID:-}\" --repo-root=\"${KUBE_VERSION_ROOT}\" --node-instance-group=\"${NODE_INSTANCE_GROUP:-}\" --num-nodes=\"${NUM_NODES:-}\" --prefix=\"${KUBE_GCE_INSTANCE_PREFIX:-e2e}\" ${E2E_CLEAN_START:+\"--clean-start=true\"} ${E2E_MIN_STARTUP_PODS:+\"--minStartupPods=${E2E_MIN_STARTUP_PODS}\"} ${E2E_REPORT_DIR:+\"--report-dir=${E2E_REPORT_DIR}\"} \"${@:-}\"' exited with status 1\r\nCall stack:\r\n  1: test/kubemark/../../cluster/kubemark/../../cluster/gce/../../cluster/../hack/ginkgo-e2e.sh:96 main(...)\r\nExiting with status 1\r\n$\r\n```\r\n\r\nI will try to do the same thing over again and see if I get a different result. \r\n\r\n/cc @wojtek-t @gmarek ",
	"user": {
		"login": "rsc",
		"id": 104030,
		"type": "User",
		"site_admin": false
	},
	"created_at": "2016-03-07T19:18:48Z",
	"updated_at": "2016-03-07T19:18:48Z"
}
