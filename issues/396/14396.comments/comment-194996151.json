{
	"id": 194996151,
	"body": "Hi @wojtek-t. Yes, I've been able to reproduce the observed kubemark latency increases on 1000-node systems. I'm still using your GCE project. Still gathering data.\r\n\r\nTentatively, it looks like the difference in average latency from Go 1.4.2 to Go 1.5.3 was mainly due to Go 1.5.3 doing a better job of keeping heap size at 2 x live data size, but that meant more frequent garbage collection and therefore increased overall garbage collection overhead. Running Go 1.5.3 with GOGC=300 (target 4x instead of 2x) brings things back to roughly Go 1.4 levels. On Kubemark that makes plenty of sense, because the benchmark is using about 2 GB of memory on an otherwise mostly idle 120 GB (n1-standard-32) machine; letting it use 4 GB instead is clearly reasonable for improved latency. It's not clear to me whether kube-apiserver should do that in general, since I don't know whether it usually runs on such idle machines.\r\n\r\nI also tried with Go 1.6.0, and that looks like it is doing a better job at latency even without any GOGC tuning, so my suggestion would be to jump from Go 1.4.2 to Go 1.6.0 and not bother trying to game the GOGC setting. \r\n\r\nThis is what I've seen so far:\r\n\r\n\t                       P50   / P90   / P99\r\n\t\r\n\tGo 1.4.2    GOGC=100   132ms / 248ms / 468ms\r\n\tGo 1.4.2    GOGC=100   129ms / 261ms / 473ms\r\n\t\r\n\tGo 1.4.3    GOGC=100   148ms / 311ms / 468ms\r\n\t\r\n\tGo 1.5.3    GOGC=100   123ms / 410ms / 713ms\r\n\tGo 1.5.3    GOGC=300   141ms / 177ms / 547ms \r\n\tGo 1.5.3    GOGC=900   112ms / 136ms / 412ms\r\n\t\r\n\tGo 1.6.0    GOGC=100   117ms / 252ms / 449ms\r\n\r\nSo Go 1.6.0 is by default about as good as Go 1.4.2 for the 'LIST nodes' metric. \r\n\r\nUnfortunately, the test using Go 1.6.0 is failing by now complaining about pod startup latency. Normally you see measured latencies of 1-2 seconds, and the limit in the test is 5 seconds, but the claim in the failure is that some pod startup latencies are measured in minutes. That would obviously be bad, but I can't tell whether it's true. For the 1000 node test starting 30000 pods, both the passing and the failing tests take about 35 minutes to start all the pods. That total time doesn't change even though the reported startup latency changes dramatically. I haven't worked out how the time is measured or what might influence it. If you have any ideas about that, I'd love to hear them. Right now I think that if we can figure out the pod startup latency issue then the API call latency problems are basically fixed by Go 1.6.\r\n\r\nSeparately, from looking at CPU profiles to try to understand the Go 1.5.3 delta I've identified a number of simple optimizations in the kubernetes code that would likely improve API call latency across the board. I will measure and send those once all the Go-specific stuff is done.",
	"user": {
		"login": "rsc",
		"id": 104030,
		"type": "User",
		"site_admin": false
	},
	"created_at": "2016-03-10T18:42:36Z",
	"updated_at": "2016-03-10T18:42:36Z"
}
