{
	"id": 123732739,
	"body": "\u003e  This can happen simply because it takes us a while to go in to mark termination when we run out of work because we do things like rescanning globals to pick up more work to do in concurrent mark.\r\n\r\nI think for 1.6 we need to make all work equally available to all goroutines. That is, allow mutators to rescan stacks and globals if that's the next piece of work that GC needs to perform.\r\n\r\n\u003e The solution to both probably includes back pressure that forces the\r\nmutator to be more cooperative, for #11677 this probably means putting the\r\nmutator to sleep for a some period proportional to its debt, in effect\r\nslowing allocation by this goroutine down.\r\n\r\nWe need to put them to sleep _before_ the allocation. That is, you already have _credit_ to pay for the allocation, or you are not allowed to do it.\r\n\r\nI am also worried about transition period when heap profile/size changes. If controller parameters are incorrect, can't mutators under-help and lead to overallocation?\r\n\r\n",
	"user": {
		"login": "dvyukov",
		"id": 1095328,
		"type": "User",
		"site_admin": false
	},
	"created_at": "2015-07-22T14:02:46Z",
	"updated_at": "2015-07-22T14:02:46Z"
}
